# Tensor Parallelism æºç æ‰§è¡Œæµç¨‹è¯¦è§£

> æœ¬æ–‡æ¡£è¯¦ç»†è¿½è¸ª Axolotl ä»é…ç½®è¯»å–åˆ°æ¨¡å‹è®­ç»ƒçš„å®Œæ•´ TP å®ç°æµç¨‹

## æ‰§è¡Œæµç¨‹æ¦‚è§ˆ

```
ç”¨æˆ·æ‰§è¡Œ: axolotl train config.yaml
    â†“
1. CLI å…¥å£ (cli/main.py)
    â†“
2. é…ç½®è§£æ (cli/config.py)
    â†“
3. å¹¶è¡Œé…ç½®æ„å»º (utils/distributed.py)
    â†“
4. æ¨¡å‹åŠ è½½å™¨åˆå§‹åŒ– (loaders/model.py)
    â†“
5. ParallelismConfig è®¾ç½® (accelerate)
    â†“
6. DeviceMesh æ„å»º (torch.distributed)
    â†“
7. æ¨¡å‹å®ä¾‹åŒ– (transformers)
    â†“
8. DTensor è½¬æ¢ (torch.distributed.tensor)
    â†“
9. Trainer æ„å»º (core/builders/causal.py)
    â†“
10. è®­ç»ƒå¾ªç¯ (transformers.Trainer)
```

---

## ç¬¬ä¸€é˜¶æ®µï¼šé…ç½®è§£æ

### 1.1 CLI å…¥å£ç‚¹

```python
# æ–‡ä»¶ï¼šsrc/axolotl/cli/main.py (77-100 è¡Œ)

@cli.command()
@click.argument("config", type=click.Path(exists=True, path_type=str))
@click.option("--launcher", default="accelerate")
def train(ctx, config: str, launcher: str, **kwargs):
    """è®­ç»ƒå‘½ä»¤å…¥å£"""

    # è¯»å– YAML é…ç½®æ–‡ä»¶
    with open(config, encoding="utf-8") as file:
        cfg_dict = yaml.safe_load(file)

    # åˆå¹¶å‘½ä»¤è¡Œå‚æ•°
    cfg_dict.update(kwargs)

    # è½¬æ¢ä¸º DictDefault å¯¹è±¡
    from axolotl.utils.dict import DictDefault
    cfg = DictDefault(cfg_dict)

    # å¯åŠ¨è®­ç»ƒ
    if launcher == "accelerate":
        # ä½¿ç”¨ accelerate launch å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
        launch_training(cfg, ...)
    elif launcher == "torchrun":
        # ä½¿ç”¨ torchrun å¯åŠ¨
        ...
```

### 1.2 é…ç½®éªŒè¯

```python
# æ–‡ä»¶ï¼šsrc/axolotl/utils/schemas/validation.py

class AxolotlConfigValidator:
    """é…ç½®éªŒè¯å™¨"""

    def validate_parallelism(self, cfg):
        """éªŒè¯å¹¶è¡Œé…ç½®"""
        world_size = get_world_size()

        # è®¡ç®—æ€»å¹¶è¡Œåº¦
        total_parallel = (
            cfg.get("tensor_parallel_size", 1) *
            cfg.get("context_parallel_size", 1) *
            cfg.get("dp_shard_size", 1) *
            cfg.get("dp_replicate_size", 1)
        )

        if total_parallel != world_size:
            raise ValueError(
                f"å¹¶è¡Œé…ç½® ({total_parallel}) ä¸ GPU æ•°é‡ ({world_size}) ä¸åŒ¹é…"
            )

        # TP éœ€è¦ FSDP2
        if cfg.get("tensor_parallel_size", 1) > 1:
            if cfg.get("fsdp_version") != 2:
                raise ValueError("TP éœ€è¦ FSDP version 2")

        return True
```

---

## ç¬¬äºŒé˜¶æ®µï¼šåˆ†å¸ƒå¼åˆå§‹åŒ–

### 2.1 Accelerate åˆå§‹åŒ–

```python
# æ–‡ä»¶ï¼šaccelerate åº“å†…éƒ¨ï¼ˆAxolotl è°ƒç”¨ï¼‰

from accelerate import Accelerator, PartialState

# åœ¨è®­ç»ƒè„šæœ¬å¯åŠ¨æ—¶è‡ªåŠ¨æ‰§è¡Œ
state = PartialState()

# è¿™ä¼šåˆå§‹åŒ–ï¼š
# - torch.distributed.init_process_group()
# - è®¾ç½®ç¯å¢ƒå˜é‡ï¼šRANK, WORLD_SIZE, LOCAL_RANK
# - åˆ›å»ºé»˜è®¤ process group
```

**ç¯å¢ƒå˜é‡ç¤ºä¾‹**ï¼ˆ8 GPU è®­ç»ƒï¼‰ï¼š
```bash
# GPU 0:
RANK=0
LOCAL_RANK=0
WORLD_SIZE=8

# GPU 1:
RANK=1
LOCAL_RANK=1
WORLD_SIZE=8

# ... ä»¥æ­¤ç±»æ¨
```

### 2.2 æ„å»º ParallelismConfig

```python
# æ–‡ä»¶ï¼šsrc/axolotl/utils/distributed.py (299-316 è¡Œ)

def build_parallelism_config(cfg):
    """
    æ ¹æ®é…ç½®æ„å»º ParallelismConfig å¯¹è±¡
    è¿™ä¸ªå¯¹è±¡ä¼šå‘Šè¯‰ Accelerate å¦‚ä½•ç»„ç»‡ GPU
    """

    # æå–å¹¶è¡Œå‚æ•°
    pc_kwargs = _get_parallel_config_kwargs(
        world_size=get_world_size(),           # 8
        tensor_parallel_size=cfg.tensor_parallel_size,  # 2
        context_parallel_size=cfg.context_parallel_size,  # 1
        dp_shard_size=cfg.dp_shard_size,      # 4
        dp_replicate_size=cfg.dp_replicate_size,  # 1
        is_fsdp=bool(cfg.fsdp or cfg.fsdp_config),  # True
    )

    if pc_kwargs:
        # åˆ›å»º ParallelismConfig
        # pc_kwargs = {"tp_size": 2, "dp_shard_size": 4}
        parallelism_config = ParallelismConfig(**pc_kwargs)

        # æ„å»º DeviceMesh
        device_mesh = parallelism_config.build_device_mesh("cuda")

        return parallelism_config, device_mesh

    return None, None
```

### 2.3 DeviceMesh ç»“æ„è¯¦è§£

```python
# å‡è®¾é…ç½®ï¼š8 GPUs, TP=2, FSDP=4

device_mesh = DeviceMesh(
    "cuda",
    mesh=[
        [0, 1],  # FSDP shard 0, TP group
        [2, 3],  # FSDP shard 1, TP group
        [4, 5],  # FSDP shard 2, TP group
        [6, 7],  # FSDP shard 3, TP group
    ],
    mesh_dim_names=["dp_shard", "tp"]
)

# è®¿é—®ä¸åŒç»´åº¦çš„å­ç½‘æ ¼ï¼š
device_mesh["tp"]        # TP ç»´åº¦çš„ç½‘æ ¼
device_mesh["dp_shard"]  # FSDP ç»´åº¦çš„ç½‘æ ¼

# ç¤ºä¾‹ï¼šGPU 2 çš„è§†è§’
# - å®ƒåœ¨ FSDP shard 1 ä¸­
# - å®ƒçš„ TP ä¼™ä¼´æ˜¯ GPU 3
# - å®ƒçš„ FSDP ä¼™ä¼´æ˜¯ GPU 0, 4, 6 (åŒåˆ—)
```

**å¯è§†åŒ–**ï¼š
```
        TP ç»´åº¦ â†’
FSDP   â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
ç»´     â”‚ 0   â”‚ 1   â”‚  Shard 0
åº¦     â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â†“      â”‚ 2   â”‚ 3   â”‚  Shard 1
       â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
       â”‚ 4   â”‚ 5   â”‚  Shard 2
       â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
       â”‚ 6   â”‚ 7   â”‚  Shard 3
       â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

é€šä¿¡ç»„ï¼š
- TP group 0: [0, 1] - é«˜é¢‘é€šä¿¡ï¼ˆNVLinkï¼‰
- TP group 1: [2, 3]
- TP group 2: [4, 5]
- TP group 3: [6, 7]

- FSDP group 0: [0, 2, 4, 6] - ä¸­é¢‘é€šä¿¡
- FSDP group 1: [1, 3, 5, 7]
```

---

## ç¬¬ä¸‰é˜¶æ®µï¼šæ¨¡å‹åŠ è½½ä¸ TP åº”ç”¨

### 3.1 æ¨¡å‹åŠ è½½å™¨æµç¨‹

```python
# æ–‡ä»¶ï¼šsrc/axolotl/loaders/model.py (161-190 è¡Œ)

class ModelLoader:
    def load(self):
        """å®Œæ•´çš„æ¨¡å‹åŠ è½½æµç¨‹"""

        # === ç¬¬ 1 æ­¥ï¼šé¢„å¤„ç† ===
        self.patch_manager.apply_pre_model_load_patches()
        self._apply_pre_model_load_setup()
        # â†‘ åœ¨è¿™é‡Œè®¾ç½® self.parallelism_config

        # === ç¬¬ 2 æ­¥ï¼šåŠ è½½æ¨¡å‹æƒé‡ ===
        PLUGIN_MANAGER.pre_model_load(self.cfg)
        skip_move_to_device = self._build_model()
        # â†‘ æ ¸å¿ƒï¼æ¨¡å‹åœ¨è¿™é‡Œè¢«åŠ è½½å’Œè½¬æ¢

        PLUGIN_MANAGER.post_model_build(self.cfg, self.model)

        # === ç¬¬ 3 æ­¥ï¼šåå¤„ç† ===
        self._apply_post_model_load_setup()

        # === ç¬¬ 4 æ­¥ï¼šåŠ è½½ LoRA ç­‰é€‚é…å™¨ ===
        lora_config = self._load_adapters()

        return self.model, lora_config
```

### 3.2 è®¾ç½®å¹¶è¡Œé…ç½®

```python
# æ–‡ä»¶ï¼šsrc/axolotl/loaders/model.py (192-216 è¡Œ)

def _apply_pre_model_load_setup(self):
    """æ¨¡å‹åŠ è½½å‰çš„é…ç½®"""

    # æ£€æŸ¥æ˜¯å¦éœ€è¦å¹¶è¡Œé…ç½®
    self.use_parallel_config = (
        self.cfg.fsdp_config or
        (self.cfg.tensor_parallel_size and self.cfg.tensor_parallel_size > 1) or
        (self.cfg.context_parallel_size and self.cfg.context_parallel_size > 1)
    )

    # å¦‚æœä½¿ç”¨ FSDP1ï¼ˆæ—§ç‰ˆï¼‰ï¼Œä¸æ”¯æŒ TP
    if self.cfg.fsdp_config and self.cfg.fsdp_version != 2:
        self.use_parallel_config = False

    # æ„å»º ParallelismConfig
    if self.use_parallel_config:
        self._set_parallel_config()  # â† å…³é”®è°ƒç”¨

    # è®¾ç½®å…¶ä»–é…ç½®...
    self._set_auto_model_loader()
    self._set_device_map_config()
    self._set_quantization_config()
    self._set_attention_config()
```

```python
# æ–‡ä»¶ï¼šsrc/axolotl/loaders/model.py (421-426 è¡Œ)

def _set_parallel_config(self):
    """è®¾ç½®å¹¶è¡Œé…ç½®"""
    parallelism_config, device_mesh = build_parallelism_config(self.cfg)

    if parallelism_config:
        # ä¿å­˜åˆ°å®ä¾‹å˜é‡
        self.parallelism_config = parallelism_config
        self.device_mesh = device_mesh

        # è¿™äº›ä¼šè¢«ä¼ é€’ç»™ Accelerator
        # Accelerator ä¼šåœ¨æ¨¡å‹åŒ…è£…æ—¶ä½¿ç”¨å®ƒä»¬
```

### 3.3 æ¨¡å‹å®ä¾‹åŒ–

```python
# æ–‡ä»¶ï¼šsrc/axolotl/loaders/model.py (_build_model æ–¹æ³•ç®€åŒ–ç‰ˆ)

def _build_model(self):
    """æ„å»ºæ¨¡å‹å®ä¾‹"""

    # è·å–æ¨¡å‹é…ç½®
    model_config = self.model_config

    # æ ¹æ®æ˜¯å¦ä½¿ç”¨ FSDP é€‰æ‹©åŠ è½½æ–¹å¼
    if self.is_fsdp_enabled:
        # FSDP æ¨¡å¼ï¼šåœ¨ meta è®¾å¤‡ä¸Šåˆå§‹åŒ–ï¼ˆä¸å æ˜¾å­˜ï¼‰
        with init_empty_weights():
            model = self.auto_model_loader.from_config(
                model_config,
                torch_dtype=self.cfg.torch_dtype,
                trust_remote_code=self.cfg.trust_remote_code,
            )
        # ç¨ååœ¨ FSDP åŒ…è£…æ—¶åŠ è½½æƒé‡

    else:
        # æ™®é€šæ¨¡å¼ï¼šç›´æ¥åŠ è½½åˆ° GPU
        model = self.auto_model_loader.from_pretrained(
            self.base_model,
            config=model_config,
            torch_dtype=self.cfg.torch_dtype,
            device_map=self.model_kwargs.get("device_map"),
            **self.model_kwargs,
        )

    self.model = model
    return skip_move_to_device
```

### 3.4 FSDP2 + TP åŒ…è£…

è¿™æ˜¯æœ€å…³é”®çš„æ­¥éª¤ï¼æ¨¡å‹åœ¨ Trainer åˆå§‹åŒ–æ—¶è¢« FSDP2 åŒ…è£…ï¼ŒåŒæ—¶åº”ç”¨ TPã€‚

```python
# æ–‡ä»¶ï¼štransformers.Trainer å†…éƒ¨ï¼ˆç®€åŒ–ç‰ˆï¼‰

class Trainer:
    def __init__(self, model, args, ...):
        # åˆ›å»º Accelerator
        self.accelerator = Accelerator(
            fsdp_plugin=args.fsdp_config,  # FSDP é…ç½®
        )

        # å¦‚æœæä¾›äº† device_meshï¼ˆæ¥è‡ª ModelLoaderï¼‰
        if hasattr(model, 'device_mesh') and model.device_mesh:
            self.accelerator.state.device_mesh = model.device_mesh

        # å‡†å¤‡æ¨¡å‹ï¼ˆè¿™é‡Œåº”ç”¨ FSDP å’Œ TPï¼‰
        self.model = self.accelerator.prepare_model(model)
```

**Accelerator.prepare_model å†…éƒ¨æµç¨‹**ï¼š

```python
# accelerate åº“å†…éƒ¨ï¼ˆç®€åŒ–ç‰ˆï¼‰

def prepare_model(self, model):
    """å‡†å¤‡æ¨¡å‹ä»¥ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒ"""

    # å¦‚æœé…ç½®äº† FSDP2
    if self.state.distributed_type == DistributedType.FSDP:
        from torch.distributed.fsdp import FullyShardedDataParallel

        # è·å– device_mesh
        device_mesh = self.state.device_mesh

        # åŒ…è£…æ¯ä¸ª Transformer å±‚
        for layer in model.layers:
            # 1. å…ˆåº”ç”¨ TPï¼ˆè½¬æ¢ä¸º DTensorï¼‰
            if device_mesh and "tp" in device_mesh.mesh_dim_names:
                layer = apply_tensor_parallel(layer, device_mesh["tp"])

            # 2. å†åº”ç”¨ FSDP
            layer = FullyShardedDataParallel(
                layer,
                device_mesh=device_mesh["dp_shard"] if device_mesh else None,
                **fsdp_kwargs
            )

        return model
```

### 3.5 DTensor è½¬æ¢ç»†èŠ‚

```python
# PyTorch å†…éƒ¨ï¼šapply_tensor_parallel (ç®€åŒ–ç‰ˆ)

def apply_tensor_parallel(module, tp_mesh):
    """
    å°†æ¨¡å—çš„æƒé‡è½¬æ¢ä¸º DTensorï¼Œå®ç° TP
    """

    # éå†æ¨¡å—çš„æ‰€æœ‰å­æ¨¡å—
    for name, child in module.named_children():
        if isinstance(child, nn.Linear):
            # åˆ¤æ–­æ˜¯åˆ—åˆ‡åˆ†è¿˜æ˜¯è¡Œåˆ‡åˆ†
            if name in ["q_proj", "k_proj", "v_proj", "gate_proj", "up_proj"]:
                # åˆ—åˆ‡åˆ†
                parallelize_linear(child, tp_mesh, style="colwise")
            elif name in ["o_proj", "down_proj"]:
                # è¡Œåˆ‡åˆ†
                parallelize_linear(child, tp_mesh, style="rowwise")

    return module


def parallelize_linear(linear, tp_mesh, style):
    """å°† Linear å±‚çš„æƒé‡è½¬æ¢ä¸º DTensor"""

    from torch.distributed.tensor import distribute_tensor
    from torch.distributed.tensor.placement_types import Shard, Replicate

    # è·å–å½“å‰æƒé‡
    weight = linear.weight  # [out_features, in_features]

    if style == "colwise":
        # åˆ—åˆ‡åˆ†ï¼šåœ¨ out_features ç»´åº¦åˆ‡åˆ†
        placement = [Shard(0)]  # ç»´åº¦ 0 = out_features
        # GPU 0: weight[:out_features//2, :]
        # GPU 1: weight[out_features//2:, :]

    elif style == "rowwise":
        # è¡Œåˆ‡åˆ†ï¼šåœ¨ in_features ç»´åº¦åˆ‡åˆ†
        placement = [Shard(1)]  # ç»´åº¦ 1 = in_features
        # GPU 0: weight[:, :in_features//2]
        # GPU 1: weight[:, in_features//2:]

    # è½¬æ¢ä¸º DTensor
    linear.weight = nn.Parameter(
        distribute_tensor(weight, tp_mesh, placement)
    )

    # åç½®ï¼ˆå¦‚æœæœ‰ï¼‰é€šå¸¸å¤åˆ¶åˆ°æ‰€æœ‰ GPU
    if linear.bias is not None:
        linear.bias = nn.Parameter(
            distribute_tensor(linear.bias, tp_mesh, [Replicate()])
        )
```

**è½¬æ¢åçš„æƒé‡ç¤ºä¾‹**ï¼š

```python
# åŸå§‹æƒé‡ï¼ˆå• GPUï¼‰ï¼š
linear.weight.shape = [4096, 4096]  # 16MB (fp16)

# TP=2 åˆ—åˆ‡åˆ†åï¼š
# GPU 0:
linear.weight.local_tensor.shape = [2048, 4096]  # 8MB
linear.weight.placements = [Shard(0)]

# GPU 1:
linear.weight.local_tensor.shape = [2048, 4096]  # 8MB
linear.weight.placements = [Shard(0)]

# ä¸¤ä¸ª GPU åˆèµ·æ¥æ‰æ˜¯å®Œæ•´çš„æƒé‡
```

---

## ç¬¬å››é˜¶æ®µï¼šè®­ç»ƒæ‰§è¡Œ

### 4.1 å‰å‘ä¼ æ’­

```python
# ç”¨æˆ·ä»£ç ï¼ˆæ— å˜åŒ–ï¼‰ï¼š
outputs = model(input_ids, attention_mask)

# DTensor è‡ªåŠ¨å¤„ç†çš„å¹•åæ“ä½œï¼š

# 1. è¾“å…¥å¹¿æ’­
# input_ids å¤åˆ¶åˆ°æ‰€æœ‰ TP GPUs
input_ids_replicated = DTensor(
    local_tensor=input_ids,
    placements=[Replicate()]  # å¤åˆ¶åˆ°æ‰€æœ‰ GPU
)

# 2. Embedding å±‚ï¼ˆé€šå¸¸ä¸åˆ‡åˆ†ï¼‰
# hidden_states = embedding(input_ids)
# è¾“å‡ºï¼š[batch, seq_len, hidden_dim]ï¼Œåœ¨æ‰€æœ‰ TP GPUs ä¸Šç›¸åŒ

# 3. ç¬¬ä¸€ä¸ª Transformer å±‚
# 3.1 QKV æŠ•å½± (åˆ—åˆ‡åˆ†)
# GPU 0: Q1 = hidden @ Wq1  (è®¡ç®—å‰ä¸€åŠ heads)
# GPU 1: Q2 = hidden @ Wq2  (è®¡ç®—åä¸€åŠ heads)
# æ— éœ€é€šä¿¡ï¼è¾“å‡ºè‡ªåŠ¨æ˜¯ DTensor[Shard(2)]

# 3.2 Attention è®¡ç®—ï¼ˆå„ GPU ç‹¬ç«‹ï¼‰
# GPU 0: attn_out1 = softmax(Q1 @ K1.T) @ V1
# GPU 1: attn_out2 = softmax(Q2 @ K2.T) @ V2

# 3.3 O æŠ•å½± (è¡Œåˆ‡åˆ†)
# GPU 0: out1 = attn_out1 @ Wo1
# GPU 1: out2 = attn_out2 @ Wo2
# DTensor è‡ªåŠ¨æ’å…¥ All-Reduceï¼š
# out = all_reduce_sum([out1, out2])  â† é€šä¿¡ï¼

# 4. FFN å±‚ï¼ˆç±»ä¼¼æµç¨‹ï¼‰
# Gate/Up åˆ—åˆ‡åˆ† â†’ æ¿€æ´»å‡½æ•° â†’ Down è¡Œåˆ‡åˆ† â†’ All-Reduce

# 5. æ‰€æœ‰å±‚é‡å¤ä¸Šè¿°è¿‡ç¨‹...

# 6. æœ€ç»ˆè¾“å‡º
# logits åœ¨æ‰€æœ‰ TP GPUs ä¸Šç›¸åŒï¼ˆå› ä¸ºæœ€ååšäº† All-Reduceï¼‰
```

**é€šä¿¡å¯è§†åŒ–**ï¼š

```
æ—¶é—´çº¿ (å•ä¸ª Transformer å±‚)ï¼š

GPU 0              GPU 1
  â”‚                  â”‚
  â”œâ”€ QKV æŠ•å½± â”€â”€â”€â”€â”€â”€â”€â”¤  (æ— é€šä¿¡ï¼Œåˆ—åˆ‡åˆ†)
  â”‚                  â”‚
  â”œâ”€ Attention â”€â”€â”€â”€â”€â”€â”¤  (æ— é€šä¿¡ï¼Œå„è‡ªè®¡ç®—)
  â”‚                  â”‚
  â”œâ”€ O æŠ•å½± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                  â”‚
  â””â”€â†’ All-Reduce â†â”€â”€â”€â”˜  (é€šä¿¡ï¼åŒæ­¥ç»“æœ)
  â”‚                  â”‚
  â”œâ”€ Gate/Up æŠ•å½± â”€â”€â”€â”¤  (æ— é€šä¿¡)
  â”‚                  â”‚
  â”œâ”€ SiLU æ¿€æ´» â”€â”€â”€â”€â”€â”€â”¤  (æ— é€šä¿¡)
  â”‚                  â”‚
  â”œâ”€ Down æŠ•å½± â”€â”€â”€â”€â”€â”€â”¤
  â”‚                  â”‚
  â””â”€â†’ All-Reduce â†â”€â”€â”€â”˜  (é€šä¿¡ï¼åŒæ­¥ç»“æœ)
  â”‚                  â”‚
```

### 4.2 åå‘ä¼ æ’­

```python
# ç”¨æˆ·ä»£ç ï¼š
loss = outputs.loss
loss.backward()

# DTensor è‡ªåŠ¨å¤„ç†çš„æ¢¯åº¦è®¡ç®—ï¼š

# å‡è®¾æœ€åä¸€å±‚æ˜¯è¡Œåˆ‡åˆ†çš„ down_proj
# GPU 0: out1 = x1 @ W1
# GPU 1: out2 = x2 @ W2
# out = out1 + out2  (All-Reduce)

# åå‘ä¼ æ’­ï¼š
# d_out æ˜¯æŸå¤±å¯¹ out çš„æ¢¯åº¦ï¼ˆæ‰€æœ‰ GPU ç›¸åŒï¼‰

# 1. æ¢¯åº¦åå‘ä¼ æ’­åˆ° out1, out2
# d_out1 = d_out  (GPU 0)
# d_out2 = d_out  (GPU 1)

# 2. è®¡ç®—æƒé‡æ¢¯åº¦
# GPU 0: d_W1 = x1.T @ d_out1
# GPU 1: d_W2 = x2.T @ d_out2
# æ— éœ€é€šä¿¡ï¼æ¯ä¸ª GPU åªæ›´æ–°è‡ªå·±çš„æƒé‡

# 3. è®¡ç®—è¾“å…¥æ¢¯åº¦
# GPU 0: d_x1 = d_out1 @ W1.T
# GPU 1: d_x2 = d_out2 @ W2.T
# All-Reduce æ±‚å’Œï¼š
# d_x = d_x1 + d_x2  â† é€šä¿¡ï¼

# 4. ç»§ç»­åå‘ä¼ æ’­åˆ°å‰ä¸€å±‚...
```

**æ¢¯åº¦é€šä¿¡è§„åˆ™**ï¼š
```
åˆ—åˆ‡åˆ†å±‚ï¼ˆQKV, Gate, Upï¼‰ï¼š
- å‰å‘ï¼šæ— é€šä¿¡
- åå‘ï¼šéœ€è¦ All-Reduce è¾“å…¥æ¢¯åº¦

è¡Œåˆ‡åˆ†å±‚ï¼ˆO, Downï¼‰ï¼š
- å‰å‘ï¼šéœ€è¦ All-Reduce è¾“å‡º
- åå‘ï¼šæ— é€šä¿¡ï¼ˆæƒé‡æ¢¯åº¦ï¼‰
```

### 4.3 ä¼˜åŒ–å™¨æ›´æ–°

```python
# æ¯ä¸ª TP GPU åªæ›´æ–°è‡ªå·±æŒæœ‰çš„æƒé‡éƒ¨åˆ†

# GPU 0 çš„ä¼˜åŒ–å™¨ï¼š
optimizer.step()
# æ›´æ–°ï¼š
# - q_proj.weight (å‰ä¸€åŠ)
# - k_proj.weight (å‰ä¸€åŠ)
# - o_proj.weight (å·¦åŠéƒ¨åˆ†è¡Œ)
# ...

# GPU 1 çš„ä¼˜åŒ–å™¨ï¼š
optimizer.step()
# æ›´æ–°ï¼š
# - q_proj.weight (åä¸€åŠ)
# - k_proj.weight (åä¸€åŠ)
# - o_proj.weight (å³åŠéƒ¨åˆ†è¡Œ)
# ...

# æ— éœ€åŒæ­¥ï¼æ¯ä¸ª GPU ç®¡ç†è‡ªå·±çš„å‚æ•°
```

---

## ç¬¬äº”é˜¶æ®µï¼šä¿å­˜ä¸åŠ è½½

### 5.1 Checkpoint ä¿å­˜

```python
# æ–‡ä»¶ï¼štransformers.Trainer (save_model æ–¹æ³•)

def save_model(self, output_dir):
    """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""

    # FSDP + TP æ¨¡å¼ä¸‹ï¼š
    # 1. æ”¶é›†åˆ†ç‰‡å‚æ•°åˆ°ä¸»è¿›ç¨‹
    # 2. ä» DTensor è½¬æ¢å›æ™®é€š Tensor
    # 3. ä¿å­˜å®Œæ•´æ¨¡å‹

    if self.args.fsdp:
        # ä½¿ç”¨ FSDP çš„çŠ¶æ€å­—å…¸æ”¶é›†
        from torch.distributed.fsdp import FullStateDictConfig
        from torch.distributed.fsdp import StateDictType

        with FSDP.state_dict_type(
            self.model,
            StateDictType.FULL_STATE_DICT,
            FullStateDictConfig(offload_to_cpu=True, rank0_only=True),
        ):
            state_dict = self.model.state_dict()

        # åªæœ‰ä¸»è¿›ç¨‹ä¿å­˜
        if self.is_world_process_zero():
            # state_dict ç°åœ¨æ˜¯å®Œæ•´çš„ã€æœªåˆ‡åˆ†çš„æƒé‡
            self.model.save_pretrained(output_dir, state_dict=state_dict)
```

**çŠ¶æ€å­—å…¸è½¬æ¢è¿‡ç¨‹**ï¼š

```python
# TP + FSDP åˆ†ç‰‡çŠ¶æ€ï¼š
# GPU 0: layers.0.q_proj.weight = DTensor([2048, 4096])
# GPU 1: layers.0.q_proj.weight = DTensor([2048, 4096])
# GPU 2: layers.0.q_proj.weight = DTensor([2048, 4096])
# GPU 3: layers.0.q_proj.weight = DTensor([2048, 4096])

# FSDP æ”¶é›†åï¼ˆåœ¨ TP group 0 çš„ rank 0 ä¸Šï¼‰ï¼š
# GPU 0: layers.0.q_proj.weight = DTensor([4096, 4096])  â† ä» TP GPUs 0,1 æ”¶é›†

# æœ€ç»ˆå…¨å±€æ”¶é›†ï¼ˆåœ¨å…¨å±€ rank 0 ä¸Šï¼‰ï¼š
# GPU 0: layers.0.q_proj.weight = Tensor([4096, 4096])  â† å®Œæ•´æƒé‡
```

### 5.2 Checkpoint åŠ è½½

```python
# ä»ä¿å­˜çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ

# 1. åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨ç›¸åŒçš„å¹¶è¡Œé…ç½®ï¼‰
model = ModelLoader(cfg, tokenizer).load()

# 2. åŠ è½½æ£€æŸ¥ç‚¹
if cfg.resume_from_checkpoint:
    # FSDP2 ä¼šè‡ªåŠ¨ï¼š
    # - è¯»å–å®Œæ•´çŠ¶æ€å­—å…¸
    # - åˆ†ç‰‡åˆ°å„ä¸ª GPU
    # - è½¬æ¢ä¸º DTensorï¼ˆå¦‚æœä½¿ç”¨ TPï¼‰

    trainer = Trainer(model, ...)
    trainer.train(resume_from_checkpoint=cfg.resume_from_checkpoint)
```

---

## è°ƒè¯•æŠ€å·§

### 1. æ‰“å° DTensor ä¿¡æ¯

```python
# åœ¨æ¨¡å‹åŠ è½½åæ·»åŠ ï¼š
for name, param in model.named_parameters():
    if hasattr(param, 'placements'):
        print(f"{name}:")
        print(f"  - å…¨å±€å½¢çŠ¶: {param.shape}")
        print(f"  - æœ¬åœ°å½¢çŠ¶: {param.local_tensor.shape}")
        print(f"  - åˆ‡åˆ†æ–¹å¼: {param.placements}")
        print(f"  - DeviceMesh: {param.device_mesh}")
        break  # åªæ‰“å°ç¬¬ä¸€ä¸ªå‚æ•°
```

**é¢„æœŸè¾“å‡º**ï¼š
```
layers.0.self_attn.q_proj.weight:
  - å…¨å±€å½¢çŠ¶: torch.Size([4096, 4096])
  - æœ¬åœ°å½¢çŠ¶: torch.Size([2048, 4096])
  - åˆ‡åˆ†æ–¹å¼: [Shard(dim=0)]
  - DeviceMesh: DeviceMesh('cuda', [0, 1])
```

### 2. ç›‘æ§é€šä¿¡

```python
# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥æŸ¥çœ‹ NCCL é€šä¿¡æ—¥å¿—
import os
os.environ['NCCL_DEBUG'] = 'INFO'

# è®­ç»ƒæ—¶ä¼šæ‰“å°ï¼š
# NCCL INFO AllReduce: size 16777216 (16MB), time 1.2ms
# NCCL INFO Broadcast: size 8388608 (8MB), time 0.8ms
```

### 3. éªŒè¯ TP æ­£ç¡®æ€§

```python
# æ¯”è¾ƒ TP å’Œé TP çš„è¾“å‡º

# 1. ä¸ä½¿ç”¨ TP è®­ç»ƒå‡ æ­¥ï¼Œä¿å­˜è¾“å‡º
# tensor_parallel_size: 1

# 2. ä½¿ç”¨ TP è®­ç»ƒï¼Œæ¯”è¾ƒè¾“å‡º
# tensor_parallel_size: 2

# å‰å‡ æ­¥çš„ loss åº”è¯¥åŸºæœ¬ä¸€è‡´ï¼ˆæµ®ç‚¹è¯¯å·®ï¼‰
```

### 4. æ€§èƒ½åˆ†æ

```python
# ä½¿ç”¨ PyTorch Profiler
from torch.profiler import profile, ProfilerActivity

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    with_stack=True
) as prof:
    # è®­ç»ƒå‡ ä¸ª step
    for step in range(10):
        outputs = model(input_ids)
        loss = outputs.loss
        loss.backward()

# æŸ¥çœ‹é€šä¿¡æ—¶é—´
print(prof.key_averages().table(sort_by="cuda_time_total"))
# å¯»æ‰¾ nccl:all_reduce ç­‰é€šä¿¡æ“ä½œ
```

---

## å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜ 1ï¼šæ˜¾å­˜çˆ†ç‚¸

**ç—‡çŠ¶**ï¼š
```
RuntimeError: CUDA out of memory. Tried to allocate 20.00 GiB
```

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ GPU éƒ½å‚ä¸äº† TP
```python
print(f"TP size: {cfg.tensor_parallel_size}")
print(f"World size: {get_world_size()}")
```

2. æ£€æŸ¥ DTensor æ˜¯å¦ç”Ÿæ•ˆ
```python
# åº”è¯¥çœ‹åˆ°æœ¬åœ° tensor æ›´å°
print(f"Local shape: {model.layers[0].q_proj.weight.local_tensor.shape}")
```

3. æ£€æŸ¥ FSDP é…ç½®
```yaml
fsdp_config:
  reshard_after_forward: true  # å¿…é¡»å¼€å¯ä»¥èŠ‚çœæ˜¾å­˜
```

### é—®é¢˜ 2ï¼šè®­ç»ƒé€Ÿåº¦æ…¢

**ç—‡çŠ¶**ï¼š
```
TP=2 æ¯”å•å¡è¿˜æ…¢
```

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥ GPU äº’è¿
```bash
nvidia-smi topo -m

# åº”è¯¥çœ‹åˆ° NVLinkï¼š
#   GPU0    GPU1
# GPU0   X     NV12
# GPU1  NV12    X

# å¦‚æœçœ‹åˆ° PHB (PCIe)ï¼ŒTP ä¼šå¾ˆæ…¢
```

2. æ£€æŸ¥é€šä¿¡å æ¯”
```python
# ä½¿ç”¨ profiler æŸ¥çœ‹ All-Reduce æ—¶é—´
# All-Reduce æ—¶é—´ä¸åº”è¶…è¿‡æ€»æ—¶é—´çš„ 20%
```

### é—®é¢˜ 3ï¼šLoss ä¸æ”¶æ•›æˆ– NaN

**ç—‡çŠ¶**ï¼š
```
Step 10: loss = nan
```

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥æ··åˆç²¾åº¦é…ç½®
```yaml
bf16: true  # TP æ¨è bf16
fp16: false # ä¸æ¨è fp16
```

2. æ£€æŸ¥æ¢¯åº¦è£å‰ª
```yaml
max_grad_norm: 1.0  # é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
```

3. æ£€æŸ¥å­¦ä¹ ç‡
```yaml
learning_rate: 1e-5  # TP å¯èƒ½éœ€è¦æ›´å°çš„å­¦ä¹ ç‡
```

### é—®é¢˜ 4ï¼šCheckpoint åŠ è½½å¤±è´¥

**ç—‡çŠ¶**ï¼š
```
RuntimeError: Error(s) in loading state_dict
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# ç¡®ä¿è®­ç»ƒå’ŒåŠ è½½æ—¶çš„å¹¶è¡Œé…ç½®ä¸€è‡´
tensor_parallel_size: 2  # å¿…é¡»ç›¸åŒ
dp_shard_size: 4         # å¿…é¡»ç›¸åŒ

# æˆ–ä½¿ç”¨ FULL_STATE_DICT æ ¼å¼ï¼ˆå…¼å®¹æ€§æ›´å¥½ï¼‰
fsdp_config:
  state_dict_type: FULL_STATE_DICT
```

---

## æ€§èƒ½ä¼˜åŒ–æ¸…å•

### âœ… å¿…åšä¼˜åŒ–

1. **å¯ç”¨ Flash Attention**
```yaml
flash_attention: true
```

2. **ä½¿ç”¨ bf16**
```yaml
bf16: true
tf32: true
```

3. **å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹**
```yaml
gradient_checkpointing: true
```

4. **åˆç†é…ç½® batch size**
```yaml
micro_batch_size: 1
gradient_accumulation_steps: 32
# æœ‰æ•ˆ batch = 1 Ã— 32 Ã— num_gpus = 256
```

### ğŸ”§ å¯é€‰ä¼˜åŒ–

5. **å¯ç”¨ç¼–è¯‘ï¼ˆPyTorch 2.0+ï¼‰**
```yaml
torch_compile: true
torch_compile_backend: "inductor"
```

6. **ä½¿ç”¨ Fused Optimizer**
```yaml
optimizer: adamw_torch_fused  # æ¯” adamw_torch å¿«
```

7. **å¼€å¯ CCE (Cut Cross Entropy)**
```yaml
plugins:
  - axolotl.integrations.cut_cross_entropy.CutCrossEntropyPlugin
```

8. **è°ƒæ•´ FSDP å‚æ•°**
```yaml
fsdp_config:
  forward_prefetch: true  # æå‰é¢„å–å‚æ•°
  backward_prefetch: backward_pre  # åå‘ä¼ æ’­é¢„å–
```

---

## æ€»ç»“

Axolotl çš„ TP å®ç°æµç¨‹ï¼š

1. **é…ç½®è§£æ** â†’ éªŒè¯å¹¶è¡Œå‚æ•°
2. **ParallelismConfig** â†’ å®šä¹‰ GPU æ‹“æ‰‘
3. **DeviceMesh** â†’ åˆ›å»ºé€»è¾‘ GPU ç½‘æ ¼
4. **DTensor è½¬æ¢** â†’ è‡ªåŠ¨åˆ‡åˆ†æƒé‡
5. **å‰å‘/åå‘ä¼ æ’­** â†’ è‡ªåŠ¨é€šä¿¡
6. **ä¿å­˜/åŠ è½½** â†’ è‡ªåŠ¨æ”¶é›†/åˆ†å‘

å…³é”®ç‚¹ï¼š
- âœ… **è‡ªåŠ¨åŒ–**ï¼šç”¨æˆ·åªéœ€é…ç½®ï¼Œåº•å±‚è‡ªåŠ¨å¤„ç†
- âœ… **é€æ˜æ€§**ï¼šä»£ç æ— éœ€ä¿®æ”¹ï¼ŒDTensor è‡ªåŠ¨å¤„ç†
- âœ… **çµæ´»æ€§**ï¼šå¯ä¸ FSDP/DDP/CP ç»„åˆ

æ ¸å¿ƒä¾èµ–ï¼š
- PyTorch â‰¥ 2.7ï¼ˆDTensor æ”¯æŒï¼‰
- FSDP2ï¼ˆæ–°ç‰ˆ FSDPï¼‰
- Accelerateï¼ˆå¹¶è¡Œé…ç½®ï¼‰
- transformersï¼ˆTrainer é›†æˆï¼‰

---

*æœ¬æ–‡æ¡£è¯¦ç»†è§£æäº† Axolotl çš„ TP æºç æ‰§è¡Œæµç¨‹ï¼Œå¸®åŠ©å¼€å‘è€…æ·±å…¥ç†è§£å®ç°ç»†èŠ‚ã€‚*

# Axolotl æ¡†æ¶ä¸­çš„ Liger Kernel æ·±åº¦è§£æ

> æœ¬æ–‡æ¡£é¢å‘ infra åˆå­¦è€…ï¼Œé€šä¿—æ˜“æ‡‚åœ°è®²è§£ Axolotl å¦‚ä½•é›†æˆ Liger Kernel

## ç›®å½•

1. [ä»€ä¹ˆæ˜¯ Liger Kernelï¼Ÿ](#1-ä»€ä¹ˆæ˜¯-liger-kernel)
2. [ä¸ºä»€ä¹ˆéœ€è¦ Liger Kernelï¼Ÿ](#2-ä¸ºä»€ä¹ˆéœ€è¦-liger-kernel)
3. [Liger Kernel çš„å·¥ä½œåŸç†](#3-liger-kernel-çš„å·¥ä½œåŸç†)
4. [Axolotl ä¸­çš„å®ç°](#4-axolotl-ä¸­çš„å®ç°)
5. [æºç å®ç°åˆ†æ](#5-æºç å®ç°åˆ†æ)
6. [å®æˆ˜ç¤ºä¾‹](#6-å®æˆ˜ç¤ºä¾‹)
7. [å¸¸è§é—®é¢˜ä¸æœ€ä½³å®è·µ](#7-å¸¸è§é—®é¢˜ä¸æœ€ä½³å®è·µ)

---

## 1. ä»€ä¹ˆæ˜¯ Liger Kernelï¼Ÿ

### 1.1 ç”¨ä¸€ä¸ªæ¯”å–»æ¥ç†è§£

å›åˆ°æˆ‘ä»¬çš„"æ¬æ¡Œå­"ä½“ç³»ï¼š

æƒ³è±¡ä½ åœ¨è£…ä¿®æˆ¿å­ï¼Œéœ€è¦ä½¿ç”¨å„ç§å·¥å…·ï¼š
- **æ™®é€šå·¥å…·**ï¼šé”¤å­ã€èºä¸åˆ€ã€æ‰³æ‰‹ï¼Œéƒ½æ˜¯åˆ†å¼€çš„ï¼Œæ¯æ¬¡ç”¨å®Œè¿˜è¦æ¢å·¥å…·
- **å¤šåŠŸèƒ½å·¥å…·**ï¼šç‘å£«å†›åˆ€ï¼Œé›†æˆäº†å¤šç§å·¥å…·ï¼Œè€Œä¸”æ¯ä¸ªå·¥å…·éƒ½æ˜¯ç²¾å¿ƒä¼˜åŒ–è¿‡çš„

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼š
- **åŸå§‹ PyTorch/HuggingFace å®ç°**ï¼šå„ä¸ªç®—å­ï¼ˆå±‚å½’ä¸€åŒ–ã€æ¿€æ´»å‡½æ•°ã€æŸå¤±è®¡ç®—ç­‰ï¼‰éƒ½æ˜¯ç‹¬ç«‹å®ç°
- **Liger Kernel**ï¼šç”¨æ‰‹å·¥ä¼˜åŒ–çš„ Triton å†…æ ¸**æ›¿æ¢**è¿™äº›ç®—å­ï¼Œåƒç‘å£«å†›åˆ€ä¸€æ ·ï¼Œæ¯ä¸ªå·¥å…·éƒ½æ›´å¿«æ›´çœç©ºé—´

**å…³é”®ç‚¹**ï¼š
- ğŸ”§ ä¸æ˜¯æ”¹å˜ç®—æ³•ï¼Œè€Œæ˜¯**æ¢äº†æ›´å¥½çš„å·¥å…·**
- âš¡ è¿™äº›"å·¥å…·"éƒ½æ˜¯ç”¨ **Triton** æ‰‹å†™çš„ GPU å†…æ ¸ï¼Œé’ˆå¯¹è®­ç»ƒåœºæ™¯æ·±åº¦ä¼˜åŒ–
- ğŸ¯ ç›®æ ‡ï¼š**ç›¸åŒçš„ç»“æœï¼Œæ›´å¿«çš„é€Ÿåº¦ï¼Œæ›´å°‘çš„æ˜¾å­˜**

### 1.2 æŠ€æœ¯å®šä¹‰

**Liger Kernel** æ˜¯ LinkedIn å¼€æºçš„é«˜æ€§èƒ½ Triton å†…æ ¸åº“ï¼Œä¸“ä¸º LLM è®­ç»ƒä¼˜åŒ–ã€‚å®ƒé€šè¿‡æ›¿æ¢ PyTorch/HuggingFace ä¸­çš„æ ‡å‡†å®ç°ï¼Œæä¾›ï¼š

- **20% è®­ç»ƒååæå‡**
- **60% æ˜¾å­˜èŠ‚çœ**
- **æ— æŸç²¾åº¦**ï¼ˆæ•°å€¼ä¸Šç­‰ä»·ï¼‰
- **å…¼å®¹ FSDP / DeepSpeed**

**æ ¸å¿ƒæ€æƒ³**ï¼šæ¥è‡ª [Liger Kernel è®ºæ–‡](https://arxiv.org/abs/2410.10989)
- è¯†åˆ« LLM è®­ç»ƒä¸­çš„æ€§èƒ½ç“¶é¢ˆç®—å­
- ç”¨ Triton ç¼–å†™é«˜åº¦ä¼˜åŒ–çš„ GPU å†…æ ¸
- é€šè¿‡ Monkey Patch æ— ç¼æ›¿æ¢åŸå§‹å®ç°

**ä¸å…¶ä»–ä¼˜åŒ–çš„åŒºåˆ«**ï¼š

| æŠ€æœ¯ | ä¼˜åŒ–å¯¹è±¡ | å®ç°æ–¹å¼ | ä¾µå…¥æ€§ |
|------|---------|---------|--------|
| **Liger Kernel** | ç®—å­å±‚ï¼ˆkernelï¼‰ | æ›¿æ¢å®ç° | ä½ï¼ˆé…ç½®å³ç”¨ï¼‰ |
| **Flash Attention** | Attention è®¡ç®— | æ›¿æ¢å®ç° | ä½ |
| **TiledMLP** | æ¿€æ´»å€¼ç®¡ç† | æ”¹å˜è®¡ç®—æµç¨‹ | ä¸­ |
| **Tensor Parallelism** | æ¨¡å‹åˆ†å¸ƒ | æ”¹å˜æ‹“æ‰‘ | é«˜ |
| **torch.compile** | æ•´ä½“ä¼˜åŒ– | ç¼–è¯‘å™¨ä¼˜åŒ– | ä½ |

---

## 2. ä¸ºä»€ä¹ˆéœ€è¦ Liger Kernelï¼Ÿ

### 2.1 æ ‡å‡† PyTorch å®ç°çš„ç“¶é¢ˆ

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼šè®¡ç®— Cross Entropy Lossã€‚

#### é—®é¢˜ï¼šæ ‡å‡†å®ç°çš„æ˜¾å­˜æµªè´¹

```python
# æ ‡å‡† PyTorch/HuggingFace å®ç°
def standard_ce_loss(model, input_ids, labels):
    """
    æ ‡å‡†æµç¨‹ï¼š
    1. æ¨¡å‹å‰å‘ä¼ æ’­ -> hidden_states [batch, seq_len, hidden_dim]
    2. é€šè¿‡ lm_head -> logits [batch, seq_len, vocab_size]  # â† æ˜¾å­˜çˆ†ç‚¸ï¼
    3. è®¡ç®— loss -> nn.CrossEntropyLoss(logits, labels)
    """
    outputs = model(input_ids)
    hidden_states = outputs.last_hidden_state  # [1, 4096, 4096]
    logits = model.lm_head(hidden_states)      # [1, 4096, 128256] â† å…³é”®ï¼
    loss = F.cross_entropy(logits.view(-1, vocab_size), labels.view(-1))
    return loss
```

**æ˜¾å­˜å ç”¨è®¡ç®—**ï¼š

```
å‡è®¾ï¼šLlama-3.1-8B æ¨¡å‹
- Batch size: 1
- Sequence length: 4096
- Vocabulary size: 128,256
- æ•°æ®ç±»å‹: bfloat16 (2 bytes)

logits å¼ é‡å¤§å°ï¼š
1 Ã— 4096 Ã— 128,256 Ã— 2 bytes = 1,050 MB â‰ˆ 1 GB

é—®é¢˜ï¼š
1. è¿™ 1GB åªæ˜¯ä¸ºäº†è®¡ç®— lossï¼
2. è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦ä¿å­˜æ¢¯åº¦ï¼Œå®é™…å ç”¨ Ã— 2 = 2 GB
3. 80 å±‚ Transformerï¼Œå¦‚æœæ¯å±‚éƒ½è¿™æ · = 160 GB âŒ çˆ†æ˜¾å­˜ï¼

ä½†å®é™…ä¸Šï¼š
- æˆ‘ä»¬åªéœ€è¦ lossï¼ˆä¸€ä¸ªæ ‡é‡ï¼‰
- logits åœ¨è®¡ç®—å®Œ loss åå°±æ²¡ç”¨äº†
- å®Œå…¨å¯ä»¥**è¾¹è®¡ç®—è¾¹ä¸¢å¼ƒ**
```

#### é—®é¢˜ï¼šå†…å­˜å¸¦å®½æµªè´¹

```python
# æ ‡å‡† RMSNorm å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
def standard_rms_norm(x, weight):
    """
    å¤šæ¬¡å†…å­˜è®¿é—®ï¼š
    1. è¯»å– x è®¡ç®—æ–¹å·®
    2. å†æ¬¡è¯»å– x è¿›è¡Œå½’ä¸€åŒ–
    3. å†æ¬¡è¯»å– x ä¹˜ä»¥ weight
    """
    variance = x.pow(2).mean(-1, keepdim=True)  # ç¬¬ 1 æ¬¡è¯» x
    x = x / torch.sqrt(variance + eps)          # ç¬¬ 2 æ¬¡è¯» x
    return x * weight                            # ç¬¬ 3 æ¬¡è¯» x

# é—®é¢˜ï¼š
# - æ¯æ¬¡ä» HBM (High Bandwidth Memory) è¯»å–æ•°æ®éƒ½éœ€è¦æ—¶é—´
# - GPU è®¡ç®—é€Ÿåº¦è¿œå¿«äºå†…å­˜è®¿é—®é€Ÿåº¦
# - é‡å¤è¯»å– = æµªè´¹å¸¦å®½ = é™ä½åå
```

**Roofline æ¨¡å‹åˆ†æ**ï¼š

```
ç°ä»£ GPU (A100) çš„ç“¶é¢ˆï¼š

è®¡ç®—èƒ½åŠ›ï¼š312 TFLOPS (FP16)
å†…å­˜å¸¦å®½ï¼š2 TB/s

ç®—æœ¯å¼ºåº¦ï¼ˆArithmetic Intensityï¼‰= FLOPS / Memory Access

æ ‡å‡† RMSNormï¼š
- è®¡ç®—é‡ï¼š~3 Ã— hidden_dim FLOPS (å¹³æ–¹ã€é™¤æ³•ã€ä¹˜æ³•)
- å†…å­˜è®¿é—®ï¼š3 Ã— hidden_dim Ã— sizeof(bf16) bytes (è¯» 3 æ¬¡)
- ç®—æœ¯å¼ºåº¦ï¼š3 / (3 Ã— 2) = 0.5 FLOPS/byte

è¿™æ˜¯å…¸å‹çš„**å†…å­˜å¸¦å®½ç“¶é¢ˆ**ç®—å­ï¼
GPU å¤§éƒ¨åˆ†æ—¶é—´åœ¨ç­‰æ•°æ®ï¼Œè®¡ç®—å•å…ƒé—²ç½®ã€‚
```

### 2.2 Liger Kernel çš„è§£å†³æ–¹æ¡ˆ

#### è§£å†³æ–¹æ¡ˆ 1ï¼šFused Linear Cross Entropy (FLCE)

```python
# Liger çš„ FLCE å®ç°ï¼ˆæ¦‚å¿µï¼‰
def liger_fused_linear_ce(hidden_states, lm_head_weight, labels):
    """
    æ ¸å¿ƒæ€æƒ³ï¼šä¸ç‰©åŒ– logitsï¼

    1. åˆ†å—å¤„ç†ï¼šå°† vocab_size åˆ‡æˆå¤šä¸ª chunk
    2. é€ chunk è®¡ç®—ï¼š
       - è®¡ç®—å½“å‰ chunk çš„ logits
       - ç«‹å³è®¡ç®—å¯¹åº”çš„ loss è´¡çŒ®
       - ä¸¢å¼ƒ logitsï¼ˆä¸ä¿å­˜ï¼‰
    3. ç´¯åŠ æ‰€æœ‰ chunk çš„ loss
    """
    loss = 0.0
    chunk_size = 4096  # æ¯æ¬¡åªå¤„ç† 4096 ä¸ª vocab

    for chunk_idx in range(0, vocab_size, chunk_size):
        # åªè®¡ç®—å½“å‰ chunk çš„ logits
        chunk_logits = hidden_states @ lm_head_weight[chunk_idx:chunk_idx+chunk_size].T
        # [batch, seq_len, chunk_size] â† åªæœ‰ 32 MBï¼

        # ç«‹å³è®¡ç®— loss è´¡çŒ®å¹¶ç´¯åŠ 
        loss += compute_ce_loss_chunk(chunk_logits, labels, chunk_idx)

        # chunk_logits ç¦»å¼€ä½œç”¨åŸŸï¼Œè‡ªåŠ¨é‡Šæ”¾ âœ…

    return loss
```

**æ˜¾å­˜èŠ‚çœ**ï¼š

```
æ ‡å‡† CEï¼š
- logits: 1 Ã— 4096 Ã— 128,256 Ã— 2 = 1,050 MB

Liger FLCE (chunk_size=4096)ï¼š
- chunk_logits: 1 Ã— 4096 Ã— 4096 Ã— 2 = 33.5 MB

èŠ‚çœï¼š1050 / 33.5 = 31 å€ï¼
```

#### è§£å†³æ–¹æ¡ˆ 2ï¼šKernel Fusionï¼ˆç®—å­èåˆï¼‰

```python
# Liger RMSNormï¼šèåˆæ‰€æœ‰æ“ä½œåˆ°ä¸€ä¸ª Triton kernel
@triton.jit
def liger_rms_norm_kernel(
    x_ptr, weight_ptr, output_ptr,
    stride, hidden_size, eps,
    BLOCK_SIZE: tl.constexpr
):
    """
    å•ä¸ª Triton kernel å®Œæˆæ‰€æœ‰æ“ä½œï¼š
    1. ä¸€æ¬¡æ€§è¯»å– x åˆ° SRAM (ç‰‡ä¸Šç¼“å­˜)
    2. åœ¨ SRAM ä¸­å®Œæˆæ‰€æœ‰è®¡ç®—
    3. å†™å›ç»“æœåˆ° HBM

    å†…å­˜è®¿é—®ï¼š
    - è¯»å– xï¼š1 æ¬¡
    - è¯»å– weightï¼š1 æ¬¡
    - å†™å› outputï¼š1 æ¬¡
    æ€»è®¡ï¼š3 æ¬¡è®¿é—®ï¼ˆvs æ ‡å‡†å®ç°çš„ 3 æ¬¡è¯» x + 1 æ¬¡è¯» weight + 1 æ¬¡å†™ = 5 æ¬¡ï¼‰
    """
    # åŠ è½½æ•°æ®åˆ° SRAM
    x = tl.load(x_ptr + offsets)
    weight = tl.load(weight_ptr + offsets)

    # åœ¨ SRAM ä¸­å®Œæˆæ‰€æœ‰è®¡ç®—ï¼ˆæ— å†…å­˜è®¿é—®ï¼‰
    variance = tl.sum(x * x, axis=0) / hidden_size
    normalized = x / tl.sqrt(variance + eps)
    output = normalized * weight

    # å†™å›ç»“æœ
    tl.store(output_ptr + offsets, output)
```

**æ€§èƒ½æå‡**ï¼š

```
A100 GPU å‚æ•°ï¼š
- HBM å¸¦å®½ï¼š2 TB/s
- SRAM å¸¦å®½ï¼š19 TB/s (èŠ¯ç‰‡å†…éƒ¨ï¼Œå¿« 9.5 å€ï¼)

æ ‡å‡†å®ç°ï¼š
- 5 æ¬¡ HBM è®¿é—®
- å‡è®¾æ¯æ¬¡ 100 GB æ•°æ®
- æ—¶é—´ï¼š(5 Ã— 100 GB) / 2 TB/s = 250 ms

Liger å®ç°ï¼š
- 3 æ¬¡ HBM è®¿é—®
- æ—¶é—´ï¼š(3 Ã— 100 GB) / 2 TB/s = 150 ms

åŠ é€Ÿï¼š250 / 150 = 1.67 å€
```

### 2.3 Liger æ”¯æŒçš„ç®—å­

| ç®—å­ | æ ‡å‡†å®ç°ç—›ç‚¹ | Liger ä¼˜åŒ– | æ˜¾å­˜èŠ‚çœ | é€Ÿåº¦æå‡ |
|------|------------|-----------|---------|---------|
| **Fused Linear Cross Entropy** | logits å ç”¨å¤§é‡æ˜¾å­˜ | åˆ†å—è®¡ç®—ï¼Œä¸ç‰©åŒ– logits | 20-30x | 1.5-2x |
| **RMSNorm** | å¤šæ¬¡å†…å­˜è®¿é—® | å• kernel èåˆ | 1.5x | 1.3-1.5x |
| **LayerNorm** | å¤šæ¬¡å†…å­˜è®¿é—® | å• kernel èåˆ | 1.5x | 1.3-1.5x |
| **SwiGLU MLP** | å¤šä¸ªç®—å­åˆ†ç¦» | èåˆ gate/up/silu/down | 2x | 1.2-1.4x |
| **RoPE** | ä½æ•ˆçš„ä½ç½®ç¼–ç  | ä¼˜åŒ–çš„æ—‹è½¬è®¡ç®— | 1.2x | 1.2-1.3x |
| **Cross Entropy** | æ ‡å‡† PyTorch å®ç° | Online softmax | 2x | 1.3-1.5x |

**ç»¼åˆæ•ˆæœ**ï¼ˆLlama-3.1-8B è®­ç»ƒï¼‰ï¼š

```
å• A100 80GBï¼Œåºåˆ—é•¿åº¦ 4096ï¼š

æ ‡å‡† PyTorch + HuggingFaceï¼š
- ååï¼š1500 tokens/s
- å³°å€¼æ˜¾å­˜ï¼š65 GB
- Batch sizeï¼š2

ä½¿ç”¨ Liger Kernelï¼š
- ååï¼š1800 tokens/s (+20%) âœ…
- å³°å€¼æ˜¾å­˜ï¼š26 GB (-60%) âœ…
- Batch sizeï¼š4 (+100%) âœ…

å…³é”®ï¼šæ›´å°‘çš„æ˜¾å­˜ â†’ æ›´å¤§çš„ batch â†’ æ›´é«˜çš„åå
```

---

## 3. Liger Kernel çš„å·¥ä½œåŸç†

### 3.1 Triton ç¼–ç¨‹æ¨¡å‹

Liger Kernel çš„æ ¸å¿ƒæ˜¯ä½¿ç”¨ **Triton** ç¼–å†™ GPU å†…æ ¸ã€‚

#### ä»€ä¹ˆæ˜¯ Tritonï¼Ÿ

```python
# CUDA (ä¼ ç»Ÿæ–¹å¼)ï¼šéœ€è¦æ‰‹åŠ¨ç®¡ç†çº¿ç¨‹ã€å†…å­˜
__global__ void rms_norm_cuda(float* x, float* out, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    // å¤æ‚çš„çº¿ç¨‹åŒæ­¥ã€å†…å­˜ç®¡ç†...
    __syncthreads();
    // ...
}

# Triton (ç°ä»£æ–¹å¼)ï¼šç±»ä¼¼ NumPyï¼Œè‡ªåŠ¨ä¼˜åŒ–
@triton.jit
def rms_norm_triton(x_ptr, out_ptr, size, BLOCK_SIZE: tl.constexpr):
    # è‡ªåŠ¨å¤„ç†ï¼š
    # - çº¿ç¨‹å—åˆ’åˆ†
    # - å†…å­˜åˆå¹¶è®¿é—®
    # - SRAM ç¼“å­˜åˆ©ç”¨
    offsets = tl.arange(0, BLOCK_SIZE)
    x = tl.load(x_ptr + offsets)  # è‡ªåŠ¨ä¼˜åŒ–å†…å­˜è®¿é—®
    # ...
```

**Triton çš„ä¼˜åŠ¿**ï¼š
- âœ… è¯­æ³•ç®€å•ï¼ˆç±»ä¼¼ NumPyï¼‰
- âœ… è‡ªåŠ¨ä¼˜åŒ–ï¼ˆå†…å­˜è®¿é—®ã€çº¿ç¨‹è°ƒåº¦ï¼‰
- âœ… æ€§èƒ½æ¥è¿‘æ‰‹å†™ CUDAï¼ˆ90-95%ï¼‰
- âœ… å¼€å‘æ—¶é—´çŸ­ï¼ˆ1/10 çš„ä»£ç é‡ï¼‰

### 3.2 æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯

#### æŠ€æœ¯ 1ï¼šKernel Fusionï¼ˆç®—å­èåˆï¼‰

**åŸå§‹å®ç°ï¼ˆå¤šä¸ª kernelï¼‰**ï¼š

```python
# PyTorch å®ç°ï¼ˆ3 ä¸ªç‹¬ç«‹çš„ kernelï¼‰
def swiglu_mlp(x, gate_proj, up_proj, down_proj):
    gate = F.linear(x, gate_proj)      # Kernel 1: GEMM
    up = F.linear(x, up_proj)          # Kernel 2: GEMM
    activation = F.silu(gate) * up     # Kernel 3: Element-wise
    output = F.linear(activation, down_proj)  # Kernel 4: GEMM
    return output

# å†…å­˜è®¿é—®æ¨¡å¼ï¼š
# x (HBM) -> GPU -> gate (HBM)  # å†™å› HBM
# x (HBM) -> GPU -> up (HBM)    # å†™å› HBM
# gate (HBM) + up (HBM) -> GPU -> activation (HBM)  # è¯» 2 æ¬¡ï¼Œå†™ 1 æ¬¡
# activation (HBM) -> GPU -> output (HBM)
```

**Liger å®ç°ï¼ˆèåˆ kernelï¼‰**ï¼š

```python
# Liger: èåˆ element-wise æ“ä½œ
@triton.jit
def fused_swiglu_kernel(...):
    # åœ¨åŒä¸€ä¸ª kernel ä¸­ï¼š
    gate = compute_linear(x, gate_weight)  # è®¡ç®—åœ¨ SRAM
    up = compute_linear(x, up_weight)      # è®¡ç®—åœ¨ SRAM
    activation = silu(gate) * up           # å…¨éƒ¨åœ¨ SRAM ä¸­ï¼
    # åªå†™å›æœ€ç»ˆç»“æœ
    store(activation)

# å†…å­˜è®¿é—®ä¼˜åŒ–ï¼š
# - å‡å°‘äº†ä¸­é—´ç»“æœçš„ HBM å†™å…¥
# - activation ç›´æ¥åœ¨ SRAM ä¸­ä¼ é€’ç»™ down_proj
```

**æ”¶ç›Š**ï¼š

```
å‡è®¾ activation å¤§å°ï¼š1 Ã— 4096 Ã— 14336 Ã— 2 bytes = 117 MB

æ ‡å‡†å®ç°ï¼š
- å†™ gateï¼š117 MB
- å†™ upï¼š117 MB
- è¯» gateï¼š117 MB
- è¯» upï¼š117 MB
- å†™ activationï¼š117 MB
æ€»å†…å­˜ä¼ è¾“ï¼š585 MB

Liger èåˆï¼š
- è¯» xï¼š32 MB (åªè¯»ä¸€æ¬¡)
- å†™ activationï¼š117 MB
æ€»å†…å­˜ä¼ è¾“ï¼š149 MB

èŠ‚çœï¼š585 / 149 = 3.9 å€å†…å­˜å¸¦å®½ï¼
```

#### æŠ€æœ¯ 2ï¼šChunked Computationï¼ˆåˆ†å—è®¡ç®—ï¼‰

ç”¨äºå¤„ç†è¶…å¤§å¼ é‡ï¼Œé¿å…æ˜¾å­˜çˆ†ç‚¸ã€‚

**ç¤ºä¾‹ï¼šFused Linear Cross Entropy**

```python
# æ ‡å‡†å®ç°ï¼šä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ logits
logits = hidden @ lm_head.T  # [B, L, V] where V=128K
loss = cross_entropy(logits, labels)  # éœ€è¦ä¿å­˜ 128K ç»´åº¦

# Liger FLCEï¼šåˆ†å—è®¡ç®—
def fused_linear_cross_entropy(hidden, weight, labels):
    """
    æ•°å­¦ç­‰ä»·æ€§ï¼š

    Cross Entropy = -log(exp(logit_correct) / sum(exp(logit_all)))
                  = -logit_correct + log(sum(exp(logit_all)))

    å…³é”®ï¼šsum(exp(logit_all)) å¯ä»¥åˆ†å—ç´¯åŠ ï¼
    sum_{i=0}^{V} exp(logit_i) = sum_{chunk} sum_{i in chunk} exp(logit_i)
    """

    # ç¬¬ 1 æ­¥ï¼šåˆ†å—è®¡ç®— log_sum_expï¼ˆå‰å‘ä¼ æ’­ï¼‰
    log_sum_exp = 0.0
    for chunk_idx in range(0, vocab_size, chunk_size):
        chunk_logits = hidden @ weight[chunk_idx:chunk_idx+chunk_size].T
        log_sum_exp += torch.exp(chunk_logits).sum()
    log_sum_exp = torch.log(log_sum_exp)

    # ç¬¬ 2 æ­¥ï¼šè®¡ç®— loss
    # åªéœ€è¦è®¡ç®—æ­£ç¡®æ ‡ç­¾çš„ logit
    correct_logits = hidden @ weight[labels].T  # åªè®¡ç®—å¿…è¦çš„éƒ¨åˆ†
    loss = -correct_logits + log_sum_exp

    return loss

# å®é™…å®ç°æ›´å¤æ‚ï¼ˆéœ€è¦å¤„ç†æ•°å€¼ç¨³å®šæ€§ã€æ¢¯åº¦è®¡ç®—ç­‰ï¼‰
# ä½†æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå°† O(V) ç©ºé—´å¤æ‚åº¦é™ä½åˆ° O(chunk_size)
```

**æ•°å­¦ç»†èŠ‚ï¼šOnline Softmax**

```python
# Liger ä½¿ç”¨ Online Softmax ç®—æ³•ï¼ˆå•æ¬¡éå†è®¡ç®— log_sum_expï¼‰
# æ¥è‡ª FlashAttention è®ºæ–‡

def online_log_sum_exp(chunks):
    """
    é¿å…æ•°å€¼æº¢å‡ºçš„åœ¨çº¿ç®—æ³•
    """
    max_val = -inf
    sum_exp = 0.0

    for chunk in chunks:
        # æ›´æ–°å…¨å±€æœ€å¤§å€¼
        chunk_max = chunk.max()
        new_max = max(max_val, chunk_max)

        # é‡æ–°ç¼©æ”¾ä¹‹å‰çš„ sum_exp
        sum_exp = sum_exp * exp(max_val - new_max)

        # ç´¯åŠ å½“å‰ chunkï¼ˆä½¿ç”¨æ–°çš„ç¼©æ”¾ï¼‰
        sum_exp += (chunk - new_max).exp().sum()

        max_val = new_max

    return max_val + log(sum_exp)

# ä¼˜åŠ¿ï¼š
# 1. å•æ¬¡éå†ï¼ˆO(1) é¢å¤–å†…å­˜ï¼‰
# 2. æ•°å€¼ç¨³å®šï¼ˆé€šè¿‡åŠ¨æ€ç¼©æ”¾ï¼‰
# 3. å¯å¹¶è¡Œï¼ˆæ¯ä¸ª chunk ç‹¬ç«‹ï¼‰
```

#### æŠ€æœ¯ 3ï¼šMemory Coalescingï¼ˆå†…å­˜åˆå¹¶è®¿é—®ï¼‰

GPU å†…å­˜è®¿é—®æœ€é«˜æ•ˆçš„æ¨¡å¼æ˜¯**åˆå¹¶è®¿é—®**ï¼ˆCoalesced Accessï¼‰ã€‚

```python
# ä½æ•ˆï¼šéåˆå¹¶è®¿é—®
# æ¯ä¸ªçº¿ç¨‹è¯»å–ä¸è¿ç»­çš„å†…å­˜ä½ç½®
for i in range(num_threads):
    data[i * stride]  # stride > 1 æ—¶ï¼Œç¼“å­˜è¡Œæµªè´¹

# é«˜æ•ˆï¼šåˆå¹¶è®¿é—®
# ç›¸é‚»çº¿ç¨‹è¯»å–ç›¸é‚»å†…å­˜
for i in range(num_threads):
    data[i]  # è¿ç»­è®¿é—®ï¼Œä¸€æ¬¡ç¼“å­˜è¡ŒåŠ è½½å¤šä¸ªæ•°æ®

# Triton è‡ªåŠ¨å¤„ç†ï¼š
@triton.jit
def optimized_kernel(...):
    # Triton è‡ªåŠ¨é‡æ’å†…å­˜è®¿é—®æ¨¡å¼ï¼Œç¡®ä¿åˆå¹¶
    offsets = tl.arange(0, BLOCK_SIZE)  # è¿ç»­çš„ offset
    data = tl.load(ptr + offsets)       # è‡ªåŠ¨åˆå¹¶è®¿é—®
```

**ç¤ºä¾‹ï¼šRMSNorm çš„å†…å­˜è®¿é—®ä¼˜åŒ–**

```
å‡è®¾ hidden_size = 4096ï¼Œæ¯ä¸ª warp (32 ä¸ªçº¿ç¨‹) å¤„ç†ä¸€éƒ¨åˆ†ï¼š

æ ‡å‡†å®ç°ï¼ˆå¯èƒ½çš„éåˆå¹¶è®¿é—®ï¼‰ï¼š
Thread 0: reads x[0], x[1024], x[2048], x[3072]  # è·³è·ƒè®¿é—®
Thread 1: reads x[1], x[1025], x[2049], x[3073]
...

Liger/Tritonï¼ˆè‡ªåŠ¨ä¼˜åŒ–ä¸ºåˆå¹¶è®¿é—®ï¼‰ï¼š
Thread 0-31: reads x[0:32]    # ç¬¬ 1 ä¸ªç¼“å­˜è¡Œ
Thread 0-31: reads x[32:64]   # ç¬¬ 2 ä¸ªç¼“å­˜è¡Œ
...

åŠ é€Ÿï¼šåˆå¹¶è®¿é—®å¯æå‡ 10-20 å€å†…å­˜å¸¦å®½åˆ©ç”¨ç‡ï¼
```

### 3.3 å®Œæ•´ç¤ºä¾‹ï¼šRMSNorm å®ç°å¯¹æ¯”

#### æ ‡å‡† PyTorch å®ç°

```python
class RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.eps = eps

    def forward(self, x):
        # Step 1: è®¡ç®—æ–¹å·®ï¼ˆè¯»å– x ä¸€æ¬¡ï¼‰
        variance = x.pow(2).mean(-1, keepdim=True)

        # Step 2: å½’ä¸€åŒ–ï¼ˆè¯»å– x ç¬¬äºŒæ¬¡ï¼‰
        x = x * torch.rsqrt(variance + self.eps)

        # Step 3: ç¼©æ”¾ï¼ˆè¯»å– x ç¬¬ä¸‰æ¬¡ï¼Œè¯»å– weight ä¸€æ¬¡ï¼‰
        return x * self.weight

# å†…å­˜è®¿é—®ï¼š
# - è¯» xï¼š3 æ¬¡
# - è¯» weightï¼š1 æ¬¡
# - å†™ä¸­é—´ç»“æœï¼š2 æ¬¡ï¼ˆvariance, normalized xï¼‰
# - å†™è¾“å‡ºï¼š1 æ¬¡
# æ€»è®¡ï¼š7 æ¬¡å†…å­˜è®¿é—®
```

#### Liger Triton å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰

```python
@triton.jit
def rms_norm_kernel(
    X,  # è¾“å…¥æŒ‡é’ˆ
    Y,  # è¾“å‡ºæŒ‡é’ˆ
    W,  # weight æŒ‡é’ˆ
    stride,  # stride
    N,  # hidden_size
    eps,  # epsilon
    BLOCK_SIZE: tl.constexpr,  # ç¼–è¯‘æ—¶å¸¸é‡
):
    # æ¯ä¸ª program å¤„ç†ä¸€è¡Œ
    row_idx = tl.program_id(0)
    row_start = row_idx * stride

    # 1. åŠ è½½æ•´è¡Œåˆ° SRAMï¼ˆ1 æ¬¡ HBM è¯»å–ï¼‰
    offsets = tl.arange(0, BLOCK_SIZE)
    mask = offsets < N
    x = tl.load(X + row_start + offsets, mask=mask, other=0.0)
    w = tl.load(W + offsets, mask=mask, other=1.0)

    # 2. åœ¨ SRAM ä¸­å®Œæˆæ‰€æœ‰è®¡ç®—ï¼ˆ0 æ¬¡ HBM è®¿é—®ï¼‰
    x_squared = x * x
    var = tl.sum(x_squared, axis=0) / N
    rstd = 1 / tl.sqrt(var + eps)
    x_normed = x * rstd
    y = x_normed * w

    # 3. å†™å›ç»“æœï¼ˆ1 æ¬¡ HBM å†™å…¥ï¼‰
    tl.store(Y + row_start + offsets, y, mask=mask)

# Triton launcher
def liger_rms_norm(x, weight, eps=1e-6):
    output = torch.empty_like(x)
    n_rows, n_cols = x.shape
    BLOCK_SIZE = triton.next_power_of_2(n_cols)

    rms_norm_kernel[(n_rows,)](
        x, output, weight,
        x.stride(0), n_cols, eps,
        BLOCK_SIZE=BLOCK_SIZE,
    )
    return output

# å†…å­˜è®¿é—®ï¼š
# - è¯» xï¼š1 æ¬¡
# - è¯» weightï¼š1 æ¬¡
# - å†™è¾“å‡ºï¼š1 æ¬¡
# æ€»è®¡ï¼š3 æ¬¡å†…å­˜è®¿é—®ï¼ˆvs æ ‡å‡†çš„ 7 æ¬¡ï¼‰

# åŠ é€Ÿæ¯”ï¼š7 / 3 = 2.33 å€ï¼ˆç†è®ºï¼‰
# å®é™…åŠ é€Ÿï¼š1.3-1.5 å€ï¼ˆè€ƒè™‘å…¶ä»–å› ç´ ï¼‰
```

**å…³é”®æŠ€æœ¯ç‚¹**ï¼š

1. **SRAM åˆ©ç”¨**ï¼šä¸€æ¬¡æ€§åŠ è½½æ•°æ®åˆ°ç‰‡ä¸Šç¼“å­˜ï¼Œå‡å°‘ HBM è®¿é—®
2. **Kernel Fusion**ï¼šå¤šä¸ªæ“ä½œèåˆåˆ°å•ä¸ª kernel
3. **åˆå¹¶è®¿é—®**ï¼šTriton è‡ªåŠ¨ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
4. **å¯„å­˜å™¨å¤ç”¨**ï¼šä¸­é—´ç»“æœä¿å­˜åœ¨å¯„å­˜å™¨ä¸­ï¼Œä¸å†™å›å†…å­˜

---

## 4. Axolotl ä¸­çš„å®ç°

Axolotl é€šè¿‡**æ’ä»¶ç³»ç»Ÿ**æ— ç¼é›†æˆ Liger Kernelï¼Œç”¨æˆ·åªéœ€ç®€å•é…ç½®å³å¯å¯ç”¨ã€‚

### 4.1 æ’ä»¶æ¶æ„

```python
# æ–‡ä»¶ï¼šsrc/axolotl/integrations/liger/plugin.py

class LigerPlugin(BasePlugin):
    """
    Liger Kernel æ’ä»¶

    èŒè´£ï¼š
    1. åœ¨æ¨¡å‹åŠ è½½å‰æ›¿æ¢ transformers ä¸­çš„ç®—å­
    2. æ ¹æ®é…ç½®é€‰æ‹©æ€§å¯ç”¨ä¼˜åŒ–
    3. é€‚é…ä¸åŒæ¨¡å‹æ¶æ„
    """

    def get_input_args(self):
        """è¿”å›æ’ä»¶çš„é…ç½®å‚æ•°ç±»"""
        return "axolotl.integrations.liger.LigerArgs"

    def pre_model_load(self, cfg):
        """
        åœ¨æ¨¡å‹åŠ è½½å‰æ‰§è¡Œï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰

        æ—¶æœºï¼štransformers.AutoModelForCausalLM.from_pretrained() ä¹‹å‰
        ä½œç”¨ï¼šæ›¿æ¢ transformers æ¨¡å—ä¸­çš„ç±»å®šä¹‰
        """
        # 1. å¯¼å…¥ Liger çš„å®ç°
        from liger_kernel.transformers.rms_norm import LigerRMSNorm
        from liger_kernel.transformers.swiglu import LigerSwiGLUMLP
        # ...

        # 2. æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åº”ç”¨ç­–ç•¥
        if cfg.model_config_type in MODEL_TYPE_TO_APPLY_LIGER_FN:
            # ä½¿ç”¨ Liger å®˜æ–¹æ”¯æŒçš„æ¨¡å‹
            apply_liger_fn = MODEL_TYPE_TO_APPLY_LIGER_FN[cfg.model_config_type]
            apply_liger_fn(
                rope=cfg.liger_rope,
                rms_norm=cfg.liger_rms_norm,
                # ...
            )
        elif cfg.model_config_type == "llama4":
            # è‡ªå®šä¹‰æ”¯æŒï¼ˆLiger æœªå®˜æ–¹æ”¯æŒçš„æ¨¡å‹ï¼‰
            from .models.llama4 import apply_liger_kernel_to_llama4
            apply_liger_kernel_to_llama4(...)
        # ...
```

**æ’ä»¶ç”Ÿå‘½å‘¨æœŸ**ï¼š

```
è®­ç»ƒæµç¨‹ï¼š
1. ç”¨æˆ·è¿è¡Œï¼šaxolotl train config.yaml
2. Axolotl åŠ è½½é…ç½®
3. æ£€æµ‹åˆ° plugins: [LigerPlugin]
4. å®ä¾‹åŒ– LigerPlugin
5. è°ƒç”¨ plugin.pre_model_load(cfg)  # â† åœ¨è¿™é‡Œæ›¿æ¢ç®—å­
6. åŠ è½½æ¨¡å‹ï¼šAutoModelForCausalLM.from_pretrained()
   â””â”€ æ­¤æ—¶æ¨¡å‹å†…éƒ¨å·²ç»ä½¿ç”¨ Liger çš„å®ç°ï¼
7. å¼€å§‹è®­ç»ƒ
```

### 4.2 é…ç½®å‚æ•°

```python
# æ–‡ä»¶ï¼šsrc/axolotl/integrations/liger/args.py

class LigerArgs(BaseModel):
    """Liger é…ç½®å‚æ•°"""

    # å„ä¸ªç®—å­çš„å¼€å…³
    liger_rope: bool | None = None                      # RoPE ä½ç½®ç¼–ç 
    liger_rms_norm: bool | None = None                  # RMS å½’ä¸€åŒ–
    liger_layer_norm: bool | None = None                # Layer å½’ä¸€åŒ–
    liger_glu_activation: bool | None = None            # SwiGLU MLP
    liger_cross_entropy: bool | None = None             # Cross Entropy Loss
    liger_fused_linear_cross_entropy: bool | None = None  # FLCE (æ¨è)

    @model_validator(mode="before")
    def check_conflicts(cls, data):
        """é…ç½®æ ¡éªŒ"""
        # å†²çª 1ï¼šCE å’Œ FLCE ä¸èƒ½åŒæ—¶å¯ç”¨
        if data.get("liger_cross_entropy") and data.get("liger_fused_linear_cross_entropy"):
            raise ValueError("Cannot have both CE and FLCE enabled")

        # å†²çª 2ï¼šliger_glu_activation ä¸ tiled_mlp å†²çª
        if data.get("liger_glu_activation") and data.get("tiled_mlp"):
            if not data.get("tiled_mlp_use_original_mlp"):
                raise ValueError("liger_glu + tiled_mlp requires tiled_mlp_use_original_mlp")

        # å†²çª 3ï¼šliger_rms_norm ä¸ TP ä¸å…¼å®¹
        if data.get("liger_rms_norm") and data.get("tensor_parallel_size", 1) > 1:
            raise ValueError("liger_rms_norm incompatible with TP")

        return data
```

**é…ç½®ç¤ºä¾‹**ï¼š

```yaml
# æœ€å°é…ç½®ï¼ˆåªå¯ç”¨ FLCEï¼‰
plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_fused_linear_cross_entropy: true

# å®Œæ•´é…ç½®ï¼ˆå¯ç”¨æ‰€æœ‰ä¼˜åŒ–ï¼‰
plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_rope: true
liger_rms_norm: true
liger_glu_activation: true
liger_fused_linear_cross_entropy: true
```

### 4.3 ç®—å­æ›¿æ¢æœºåˆ¶

Liger çš„æ›¿æ¢æ˜¯é€šè¿‡ **Monkey Patch** å®ç°çš„ï¼Œå³åŠ¨æ€ä¿®æ”¹å·²å¯¼å…¥æ¨¡å—çš„å±æ€§ã€‚

#### æ–¹å¼ 1ï¼šæ›¿æ¢ç±»å®šä¹‰

```python
# æ›¿æ¢ RMSNorm
import transformers.models.llama.modeling_llama as modeling_llama
from liger_kernel.transformers.rms_norm import LigerRMSNorm

# æ›¿æ¢å‰ï¼š
# modeling_llama.LlamaRMSNorm = <class 'transformers...LlamaRMSNorm'>

# æ›¿æ¢åï¼š
modeling_llama.LlamaRMSNorm = LigerRMSNorm

# æ•ˆæœï¼š
# åç»­è°ƒç”¨ AutoModelForCausalLM.from_pretrained() æ—¶ï¼Œ
# ä¼šä½¿ç”¨ LigerRMSNorm è€Œä¸æ˜¯åŸå§‹çš„ LlamaRMSNorm
```

**æ›¿æ¢æ—¶æœºçš„å…³é”®**ï¼š

```python
# âŒ é”™è¯¯ï¼šæ¨¡å‹åŠ è½½åæ›¿æ¢ï¼ˆæ— æ•ˆï¼‰
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B")
# æ­¤æ—¶æ¨¡å‹å·²ç»å®ä¾‹åŒ–ï¼Œå†…éƒ¨ä½¿ç”¨çš„æ˜¯åŸå§‹ LlamaRMSNorm
modeling_llama.LlamaRMSNorm = LigerRMSNorm  # å¤ªæ™šäº†ï¼

# âœ… æ­£ç¡®ï¼šæ¨¡å‹åŠ è½½å‰æ›¿æ¢
modeling_llama.LlamaRMSNorm = LigerRMSNorm
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B")
# æ¨¡å‹å®ä¾‹åŒ–æ—¶ä¼šä½¿ç”¨ LigerRMSNorm
```

#### æ–¹å¼ 2ï¼šæ›¿æ¢æ–¹æ³•ï¼ˆForward Passï¼‰

```python
# æ›¿æ¢æ•´ä¸ª forward æ–¹æ³•ï¼ˆç”¨äº FLCEï¼‰
from .models.llama4 import lce_forward

# è·å–æ¨¡å‹ç±»
import transformers.models.llama4.modeling_llama4 as modeling_llama4
ModelClass = modeling_llama4.Llama4ForCausalLM

# æ›¿æ¢ forward æ–¹æ³•
ModelClass.forward = lce_forward

# æ•ˆæœï¼š
# è°ƒç”¨ model(input_ids, labels=...) æ—¶ï¼Œ
# ä¼šæ‰§è¡Œæˆ‘ä»¬è‡ªå®šä¹‰çš„ lce_forwardï¼Œè€Œä¸æ˜¯åŸå§‹çš„ forward
```

#### æ–¹å¼ 3ï¼šæ›¿æ¢å‡½æ•°ï¼ˆFunction Patchingï¼‰

```python
# æ›¿æ¢ PyTorch çš„ functional API
import torch.nn.functional as F
from liger_kernel.transformers.functional import liger_cross_entropy

# æ›¿æ¢å…¨å±€å‡½æ•°
F.cross_entropy = liger_cross_entropy

# æ•ˆæœï¼š
# æ‰€æœ‰è°ƒç”¨ F.cross_entropy(...) çš„åœ°æ–¹éƒ½ä¼šä½¿ç”¨ Liger å®ç°
```

### 4.4 æ¨¡å‹é€‚é…æµç¨‹

ä»¥ Llama4 ä¸ºä¾‹ï¼Œå±•ç¤ºå®Œæ•´çš„é€‚é…è¿‡ç¨‹ï¼š

```python
# æ–‡ä»¶ï¼šsrc/axolotl/integrations/liger/models/llama4.py

def apply_liger_kernel_to_llama4(
    cross_entropy: bool = False,
    fused_linear_cross_entropy: bool = False,
    rms_norm: bool = False,
    glu_activation: bool = False,
    layer_norm: bool = False,
):
    """åº”ç”¨ Liger Kernel åˆ° Llama4 æ¨¡å‹"""

    # 1. å¯¼å…¥ transformers çš„ Llama4 æ¨¡å—
    import transformers.models.llama4.modeling_llama4 as modeling_llama4

    # 2. å¯¼å…¥ Liger çš„å®ç°
    from liger_kernel.transformers.rms_norm import LigerRMSNorm
    from liger_kernel.transformers.swiglu import LigerSwiGLUMLP
    from liger_kernel.transformers.layer_norm import LigerLayerNorm

    # 3. é€ä¸ªæ›¿æ¢ç»„ä»¶
    if rms_norm:
        # æ›¿æ¢ RMSNorm
        modeling_llama4.Llama4TextRMSNorm = LigerRMSNorm

    if glu_activation:
        # æ›¿æ¢ MLPï¼ˆéœ€è¦é€‚é… Llama4 çš„ intermediate_size å‚æ•°ï¼‰
        def _liger_swiglu_mlp_wrapper(config, intermediate_size=None, **kwargs):
            # Llama4 çš„ MoE ä¸“å®¶å¯èƒ½æœ‰ä¸åŒçš„ intermediate_size
            config = deepcopy(config)
            if intermediate_size:
                config.intermediate_size = intermediate_size
            return LigerSwiGLUMLP(config, **kwargs)

        modeling_llama4.Llama4TextMLP = _liger_swiglu_mlp_wrapper

    if layer_norm:
        # æ›¿æ¢ LayerNormï¼ˆå…¨å±€æ›¿æ¢ï¼‰
        modeling_llama4.nn.LayerNorm = LigerLayerNorm

    if cross_entropy:
        # æ›¿æ¢ cross_entropy å‡½æ•°
        from liger_kernel.transformers.functional import liger_cross_entropy
        from transformers.loss.loss_utils import nn
        nn.functional.cross_entropy = liger_cross_entropy

    if fused_linear_cross_entropy:
        # æ›¿æ¢æ•´ä¸ª forward æ–¹æ³•
        modeling_llama4.Llama4ForCausalLM.forward = lce_forward
```

**æ›¿æ¢åçš„æ¨¡å‹ç»“æ„**ï¼š

```
æ›¿æ¢å‰ï¼ˆæ ‡å‡† Llama4ï¼‰ï¼š
Llama4ForCausalLM
â”œâ”€ Llama4TextModel
â”‚  â”œâ”€ Embedding
â”‚  â”œâ”€ Llama4TextDecoderLayer Ã— 32
â”‚  â”‚  â”œâ”€ Llama4TextAttention
â”‚  â”‚  â”œâ”€ Llama4TextRMSNorm  â† æ ‡å‡†å®ç°
â”‚  â”‚  â”œâ”€ Llama4TextMLP      â† æ ‡å‡†å®ç°
â”‚  â”‚  â”‚  â”œâ”€ gate_proj
â”‚  â”‚  â”‚  â”œâ”€ up_proj
â”‚  â”‚  â”‚  â””â”€ down_proj
â”‚  â”‚  â””â”€ Llama4TextRMSNorm  â† æ ‡å‡†å®ç°
â”‚  â””â”€ Llama4TextRMSNorm     â† æ ‡å‡†å®ç°
â””â”€ lm_head
â””â”€ forward() â†’ logits â†’ CrossEntropy(logits, labels)  â† æ ‡å‡†å®ç°

æ›¿æ¢åï¼ˆLiger ä¼˜åŒ–ï¼‰ï¼š
Llama4ForCausalLM
â”œâ”€ Llama4TextModel
â”‚  â”œâ”€ Embedding
â”‚  â”œâ”€ Llama4TextDecoderLayer Ã— 32
â”‚  â”‚  â”œâ”€ Llama4TextAttention
â”‚  â”‚  â”œâ”€ LigerRMSNorm       â† Liger Triton kernel
â”‚  â”‚  â”œâ”€ LigerSwiGLUMLP     â† Liger èåˆ kernel
â”‚  â”‚  â”‚  (å†…éƒ¨èåˆäº† gate/up/silu/down)
â”‚  â”‚  â””â”€ LigerRMSNorm       â† Liger Triton kernel
â”‚  â””â”€ LigerRMSNorm          â† Liger Triton kernel
â””â”€ lm_head
â””â”€ lce_forward() â†’ LigerForCausalLMLoss(hidden, lm_head, labels)  â† ä¸ç‰©åŒ– logitsï¼
```

---

## 5. æºç å®ç°åˆ†æ

### 5.1 Fused Linear Cross Entropyï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰

è¿™æ˜¯ Liger æœ€é‡è¦çš„ä¼˜åŒ–ï¼Œæˆ‘ä»¬è¯¦ç»†åˆ†æå…¶å®ç°ã€‚

#### æ ‡å‡† Forward å®ç°

```python
# æ ‡å‡† HuggingFace å®ç°ï¼ˆç®€åŒ–ï¼‰
def standard_forward(self, input_ids, labels=None, **kwargs):
    # 1. æ¨¡å‹å‰å‘ä¼ æ’­
    outputs = self.model(input_ids, ...)
    hidden_states = outputs[0]  # [batch, seq_len, hidden_size]

    # 2. è®¡ç®— logits
    logits = self.lm_head(hidden_states)  # [batch, seq_len, vocab_size]
    # â† è¿™é‡Œç‰©åŒ–äº†æ•´ä¸ª logits å¼ é‡ï¼ˆå·¨å¤§ï¼ï¼‰

    # 3. è®¡ç®— loss
    loss = None
    if labels is not None:
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        loss = F.cross_entropy(
            shift_logits.view(-1, vocab_size),
            shift_labels.view(-1)
        )

    return CausalLMOutputWithPast(loss=loss, logits=logits, ...)
```

#### Liger FLCE Forward å®ç°

```python
# æ–‡ä»¶ï¼šsrc/axolotl/integrations/liger/models/llama4.py

def lce_forward(self, input_ids, labels=None, **kwargs):
    """
    Liger çš„ FLCE forward å®ç°

    å…³é”®åŒºåˆ«ï¼š
    1. è®­ç»ƒæ—¶ï¼šä¸ç‰©åŒ– logitsï¼Œç›´æ¥è®¡ç®— loss
    2. æ¨ç†æ—¶ï¼šä»ç„¶è¿”å› logitsï¼ˆå…¼å®¹æ€§ï¼‰
    """
    # 1. æ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆåŒæ ‡å‡†å®ç°ï¼‰
    outputs = self.model(input_ids, ...)
    hidden_states = outputs[0]  # [batch, seq_len, hidden_size]

    # 2. åˆ¤æ–­æ˜¯å¦éœ€è¦ç‰©åŒ– logits
    logits = None
    loss = None

    if self.training and (labels is not None):
        # è®­ç»ƒæ¨¡å¼ + æœ‰æ ‡ç­¾ â†’ ä½¿ç”¨ FLCEï¼ˆä¸ç‰©åŒ– logitsï¼‰
        loss = LigerForCausalLMLoss(
            hidden_states=hidden_states,      # è¾“å…¥éšè—çŠ¶æ€
            lm_head_weight=self.lm_head.weight,  # lm_head æƒé‡
            labels=labels,                     # æ ‡ç­¾
            hidden_size=self.config.hidden_size,
            # å†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç† shiftï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼‰
        )
        # â† å…³é”®ï¼šæ²¡æœ‰è®¡ç®— logitsï¼

    else:
        # æ¨ç†æ¨¡å¼ or æ— æ ‡ç­¾ â†’ ç‰©åŒ– logits
        logits = self.lm_head(hidden_states)
        if labels is not None:
            # ä½¿ç”¨æ ‡å‡† loss è®¡ç®—ï¼ˆå…¼å®¹æ€§ï¼‰
            loss = self.loss_function(
                logits=logits,
                labels=labels,
                vocab_size=self.config.vocab_size,
            )

    return CausalLMOutputWithPast(
        loss=loss,
        logits=logits,  # è®­ç»ƒæ—¶ä¸º None
        past_key_values=outputs.past_key_values,
        hidden_states=outputs.hidden_states,
        attentions=outputs.attentions,
    )
```

#### LigerForCausalLMLoss å®ç°ï¼ˆç®€åŒ–ï¼‰

```python
# Liger Kernel å†…éƒ¨å®ç°ï¼ˆæ¦‚å¿µæ€§ä»£ç ï¼‰

class LigerForCausalLMLoss(torch.autograd.Function):
    """
    èåˆäº†çº¿æ€§å±‚å’Œäº¤å‰ç†µçš„è‡ªå®šä¹‰ autograd å‡½æ•°
    """

    @staticmethod
    def forward(ctx, hidden_states, lm_head_weight, labels, hidden_size):
        """
        å‰å‘ä¼ æ’­ï¼šåˆ†å—è®¡ç®— loss

        Args:
            hidden_states: [batch * seq_len, hidden_size]
            lm_head_weight: [vocab_size, hidden_size]
            labels: [batch * seq_len]
        """
        batch_seq_len, hidden_size = hidden_states.shape
        vocab_size = lm_head_weight.shape[0]

        # ä¿å­˜ä¸Šä¸‹æ–‡ï¼ˆåå‘ä¼ æ’­éœ€è¦ï¼‰
        ctx.save_for_backward(hidden_states, lm_head_weight, labels)

        # åˆ†å—å¤§å°ï¼ˆå¹³è¡¡æ˜¾å­˜å’Œæ€§èƒ½ï¼‰
        chunk_size = 4096

        # åˆå§‹åŒ–ç´¯åŠ å™¨
        total_loss = 0.0
        total_elements = 0

        # é€ chunk è®¡ç®—
        for chunk_start in range(0, vocab_size, chunk_size):
            chunk_end = min(chunk_start + chunk_size, vocab_size)

            # 1. è®¡ç®—å½“å‰ chunk çš„ logits
            chunk_weight = lm_head_weight[chunk_start:chunk_end, :]
            chunk_logits = hidden_states @ chunk_weight.T
            # [batch_seq_len, chunk_size] â† åªæœ‰ chunk_size ç»´åº¦

            # 2. ä½¿ç”¨ Online Softmax ç´¯åŠ 
            # ï¼ˆè¯¦ç»†å®ç°æ¶‰åŠ Triton kernelï¼Œè¿™é‡Œç®€åŒ–ï¼‰
            chunk_loss = compute_ce_loss_chunk(
                chunk_logits, labels, chunk_start, chunk_end
            )
            total_loss += chunk_loss
            total_elements += (labels >= chunk_start) & (labels < chunk_end).sum()

        # è¿”å›å¹³å‡ loss
        loss = total_loss / max(total_elements, 1)
        return loss

    @staticmethod
    def backward(ctx, grad_output):
        """
        åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦ï¼ˆä¹Ÿæ˜¯åˆ†å—çš„ï¼‰
        """
        hidden_states, lm_head_weight, labels = ctx.saved_tensors

        # åˆå§‹åŒ–æ¢¯åº¦
        grad_hidden = torch.zeros_like(hidden_states)
        grad_weight = torch.zeros_like(lm_head_weight)

        # é€ chunk è®¡ç®—æ¢¯åº¦
        for chunk_start in range(0, vocab_size, chunk_size):
            # é‡æ–°è®¡ç®— chunk_logitsï¼ˆæ¿€æ´»å€¼é‡è®¡ç®—ï¼‰
            chunk_weight = lm_head_weight[chunk_start:chunk_end, :]
            chunk_logits = hidden_states @ chunk_weight.T

            # è®¡ç®— softmax å’Œæ¢¯åº¦
            chunk_grad_logits = compute_softmax_grad(
                chunk_logits, labels, chunk_start, chunk_end
            )

            # é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦
            grad_hidden += chunk_grad_logits @ chunk_weight
            grad_weight[chunk_start:chunk_end] = chunk_grad_logits.T @ hidden_states

        # ä¹˜ä»¥ä¸Šæ¸¸æ¢¯åº¦
        grad_hidden *= grad_output
        grad_weight *= grad_output

        return grad_hidden, grad_weight, None, None
```

**å…³é”®æŠ€æœ¯ç»†èŠ‚**ï¼š

1. **åˆ†å—å¤§å°é€‰æ‹©**ï¼š
   ```python
   # Liger ä½¿ç”¨è‡ªé€‚åº”åˆ†å—
   # ç›®æ ‡ï¼šå•ä¸ª chunk çš„æ¿€æ´»å€¼ < å¯ç”¨æ˜¾å­˜çš„ 1/8

   chunk_size = min(
       4096,  # é»˜è®¤æœ€å¤§å€¼
       available_memory // (batch_size * seq_len * dtype_size * 8)
   )
   ```

2. **æ•°å€¼ç¨³å®šæ€§**ï¼š
   ```python
   # ä½¿ç”¨ LogSumExp trick é¿å…æº¢å‡º
   # log(sum(exp(x))) = max(x) + log(sum(exp(x - max(x))))

   @triton.jit
   def online_softmax_kernel(...):
       # ç»´æŠ¤å…¨å±€æœ€å¤§å€¼
       max_val = tl.maximum(max_val, tl.max(chunk_logits))

       # é‡æ–°ç¼©æ”¾ä¹‹å‰çš„ç´¯åŠ å™¨
       sum_exp *= tl.exp(old_max - max_val)

       # ç´¯åŠ å½“å‰ chunk
       sum_exp += tl.sum(tl.exp(chunk_logits - max_val))
   ```

3. **FSDP å…¼å®¹æ€§**ï¼š
   ```python
   # æ–‡ä»¶ï¼šsrc/axolotl/integrations/liger/models/base.py

   def lce_maybe_trainable_lm_head(self, hidden_states, lm_head, labels):
       # å¦‚æœ lm_head è¢« FSDP åŒ…è£¹
       if isinstance(lm_head, FullyShardedDataParallel):
           # éœ€è¦åœ¨ FSDP forward context ä¸­è¯»å–æƒé‡
           return _FSDPForwardRedirection()(
               lm_head,
               _liger_for_causal_lm_loss,
               lm_head.module,  # è§£åŒ…è·å–åŸå§‹ module
               hidden_states,
               labels,
           )
       else:
           # ç›´æ¥è°ƒç”¨
           return _liger_for_causal_lm_loss(lm_head, hidden_states, labels)
   ```

### 5.2 SwiGLU MLP èåˆ

#### æ ‡å‡† SwiGLU å®ç°

```python
# HuggingFace Llama MLP
class LlamaMLP(nn.Module):
    def __init__(self, config):
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.act_fn = F.silu

    def forward(self, x):
        # 3 ä¸ªç‹¬ç«‹çš„ kernel è°ƒç”¨
        gate = self.gate_proj(x)      # Kernel 1: GEMM
        up = self.up_proj(x)          # Kernel 2: GEMM
        activation = self.act_fn(gate) * up  # Kernel 3: Element-wise
        return self.down_proj(activation)    # Kernel 4: GEMM
```

#### Liger SwiGLU èåˆå®ç°

```python
# Liger SwiGLUMLPï¼ˆç®€åŒ–æ¦‚å¿µï¼‰
class LigerSwiGLUMLP(nn.Module):
    def __init__(self, config):
        # æƒé‡å®šä¹‰ç›¸åŒ
        self.gate_proj = nn.Linear(...)
        self.up_proj = nn.Linear(...)
        self.down_proj = nn.Linear(...)

    def forward(self, x):
        # 1. èåˆ gate/up æŠ•å½± + SiLU æ¿€æ´»
        gate_up = fused_gate_up_proj(x, self.gate_proj.weight, self.up_proj.weight)
        # â† å•ä¸ª Triton kernel å®Œæˆï¼šgate, up, silu(gate) * up

        # 2. Down æŠ•å½±
        return self.down_proj(gate_up)


@triton.jit
def fused_swiglu_kernel(
    X, gate_W, up_W, Out,
    M, K, N,  # çŸ©é˜µç»´åº¦
    BLOCK_M: tl.constexpr,
    BLOCK_K: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    """
    èåˆ kernelï¼š
    out = silu(x @ gate_W.T) * (x @ up_W.T)

    ä¼˜åŒ–ï¼š
    1. åŒæ—¶è®¡ç®— gate å’Œ up æŠ•å½±
    2. ç«‹å³åº”ç”¨ SiLU å’Œ element-wise ä¹˜æ³•
    3. ä¸­é—´ç»“æœä¿æŒåœ¨ SRAMï¼Œä¸å†™å› HBM
    """
    # è·å–å½“å‰ block çš„ä½ç½®
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    # åˆå§‹åŒ–ç´¯åŠ å™¨
    gate_acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    up_acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # åˆ†å— GEMMï¼ˆæ²¿ K ç»´åº¦ï¼‰
    for k_block in range(0, K, BLOCK_K):
        # åŠ è½½ X çš„ä¸€ä¸ª blockï¼ˆå¤ç”¨äºä¸¤ä¸ªæŠ•å½±ï¼‰
        x_block = tl.load(X + offsets_x, mask=mask_x)

        # åŠ è½½ gate_W å’Œ up_W çš„å¯¹åº” block
        gate_w_block = tl.load(gate_W + offsets_gate, mask=mask_gate)
        up_w_block = tl.load(up_W + offsets_up, mask=mask_up)

        # ç´¯åŠ çŸ©é˜µä¹˜æ³•ï¼ˆåœ¨å¯„å­˜å™¨ä¸­ï¼‰
        gate_acc += tl.dot(x_block, gate_w_block)
        up_acc += tl.dot(x_block, up_w_block)

    # åº”ç”¨ SiLU æ¿€æ´»ï¼šsilu(x) = x * sigmoid(x) = x / (1 + exp(-x))
    gate_silu = gate_acc * tl.sigmoid(gate_acc)

    # Element-wise ä¹˜æ³•
    output = gate_silu * up_acc

    # å†™å›ç»“æœï¼ˆåªå†™ä¸€æ¬¡ï¼ï¼‰
    tl.store(Out + offsets_out, output, mask=mask_out)
```

**æ€§èƒ½åˆ†æ**ï¼š

```
å‡è®¾ï¼š
- M (batch_seq_len) = 4096
- K (hidden_size) = 4096
- N (intermediate_size) = 14336
- æ•°æ®ç±»å‹ï¼šbf16

æ ‡å‡†å®ç°å†…å­˜è®¿é—®ï¼š
1. gate_proj:
   - Read X: 4096 Ã— 4096 Ã— 2 = 33.5 MB
   - Read gate_W: 4096 Ã— 14336 Ã— 2 = 117.4 MB
   - Write gate: 4096 Ã— 14336 Ã— 2 = 117.4 MB
2. up_proj:
   - Read X: 33.5 MBï¼ˆé‡å¤è¯»å–ï¼ï¼‰
   - Read up_W: 117.4 MB
   - Write up: 117.4 MB
3. silu + mul:
   - Read gate: 117.4 MB
   - Read up: 117.4 MB
   - Write activation: 117.4 MB
æ€»è®¡ï¼š908.2 MB

Liger èåˆå®ç°ï¼š
1. Fused kernel:
   - Read X: 33.5 MBï¼ˆåªè¯»ä¸€æ¬¡ï¼‰
   - Read gate_W: 117.4 MB
   - Read up_W: 117.4 MB
   - Write output: 117.4 MB
æ€»è®¡ï¼š385.7 MB

èŠ‚çœï¼š908.2 / 385.7 = 2.35 å€å†…å­˜å¸¦å®½ï¼
```

### 5.3 torch.compile å…¼å®¹æ€§å¤„ç†

Liger Kernel ä½¿ç”¨ Triton ç¼–å†™ï¼Œä½† `torch.compile` ä¼šå°è¯•ä¼˜åŒ–æ‰€æœ‰ä»£ç ï¼ŒåŒ…æ‹¬ Triton kernelï¼Œå¯¼è‡´å†²çªã€‚

```python
# æ–‡ä»¶ï¼šsrc/axolotl/integrations/liger/utils.py

def patch_with_compile_disable(module, function_name):
    """
    ç¦ç”¨ torch.compile å¯¹ Triton kernel çš„ä¼˜åŒ–

    åŸå› ï¼š
    - Triton kernel å·²ç»æ˜¯é«˜åº¦ä¼˜åŒ–çš„ GPU ä»£ç 
    - torch.compile å°è¯•ä¼˜åŒ–ä¼šå¯¼è‡´é”™è¯¯æˆ–æ€§èƒ½ä¸‹é™
    """
    original_function = getattr(module, function_name)

    @wraps(original_function)
    @torch.compiler.disable  # â† å…³é”®è£…é¥°å™¨
    def wrapped_function(*args, **kwargs):
        return original_function(*args, **kwargs)

    setattr(module, function_name, wrapped_function)

# ä½¿ç”¨ï¼š
if cfg.torch_compile:
    import liger_kernel.ops.fused_linear_cross_entropy

    patch_with_compile_disable(
        liger_kernel.ops.fused_linear_cross_entropy,
        "fused_linear_cross_entropy_forward"
    )
    patch_with_compile_disable(
        liger_kernel.ops.fused_linear_cross_entropy,
        "fused_linear_cross_entropy_backward"
    )
```

**ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªï¼Ÿ**

```python
# é—®é¢˜åœºæ™¯ï¼š
model = AutoModelForCausalLM.from_pretrained(...)  # å·²åº”ç”¨ Liger
model = torch.compile(model)  # å¯ç”¨ torch.compile

# é”™è¯¯ï¼š
# torch.compile ä¼šå°è¯•å°† Triton kernel ç¼–è¯‘æˆ TorchInductor ä»£ç 
# å¯¼è‡´ï¼š
# 1. æ€§èƒ½ä¸‹é™ï¼ˆTorchInductor ä¸å¦‚æ‰‹å†™ Tritonï¼‰
# 2. ç¼–è¯‘é”™è¯¯ï¼ˆTriton è¯­æ³•ä¸å…¼å®¹ï¼‰

# è§£å†³ï¼š
# ç”¨ @torch.compiler.disable æ ‡è®° Triton kernel
# torch.compile ä¼šè·³è¿‡è¿™äº›å‡½æ•°ï¼Œä¿æŒåŸæ ·
```

---

## 6. å®æˆ˜ç¤ºä¾‹

### 6.1 åŸºç¡€é…ç½®ï¼šLlama-3.1-8B å…¨å‚æ•°å¾®è°ƒ

```yaml
# æ–‡ä»¶ï¼šexamples/llama-3/fft-8b-liger-fsdp.yaml

base_model: NousResearch/Meta-Llama-3.1-8B

# ========== Liger Kernel é…ç½® ==========
plugins:
  - axolotl.integrations.liger.LigerPlugin

# å¯ç”¨æ‰€æœ‰ Liger ä¼˜åŒ–
liger_rope: true                        # RoPE ä½ç½®ç¼–ç ä¼˜åŒ–
liger_rms_norm: true                    # RMSNorm ä¼˜åŒ–
liger_glu_activation: true              # SwiGLU MLP èåˆ
liger_fused_linear_cross_entropy: true  # FLCEï¼ˆæœ€é‡è¦ï¼ï¼‰

# ========== æ•°æ®é›† ==========
chat_template: llama3
datasets:
  - path: mlabonne/FineTome-100k
    type: chat_template
    split: train[:20%]

sequence_len: 4096
sample_packing: true

# ========== è®­ç»ƒå‚æ•° ==========
micro_batch_size: 2
gradient_accumulation_steps: 4
num_epochs: 1
optimizer: adamw_torch_fused
learning_rate: 2e-5

# ========== ç²¾åº¦ ==========
bf16: auto
tf32: false

# ========== æ˜¾å­˜ä¼˜åŒ– ==========
gradient_checkpointing: true
flash_attention: true

# ========== FSDP é…ç½® ==========
fsdp:
  - full_shard
  - auto_wrap
fsdp_config:
  fsdp_offload_params: true
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

| é…ç½® | åå (tokens/s) | å³°å€¼æ˜¾å­˜ (GB) | Batch Size |
|------|----------------|--------------|------------|
| æ—  Liger | 1500 | 65 | 2 |
| å¯ç”¨ Liger | 1800 (+20%) | 26 (-60%) | 4 (+100%) |

### 6.2 é«˜çº§é…ç½®ï¼šLiger + FSDP2 + é•¿ä¸Šä¸‹æ–‡

```yaml
base_model: meta-llama/Llama-3.1-8B

# ========== Liger é…ç½® ==========
plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_rms_norm: true
liger_glu_activation: true
liger_fused_linear_cross_entropy: true
# æ³¨æ„ï¼šliger_rope åœ¨ FSDP2 + CP ä¸‹å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜ï¼Œå…ˆç¦ç”¨

# ========== é•¿ä¸Šä¸‹æ–‡é…ç½® ==========
sequence_len: 32768
sample_packing: true

# ========== å¤šç»´å¹¶è¡Œ ==========
fsdp_version: 2
context_parallel_size: 4  # åºåˆ—å¹¶è¡Œ
fsdp_config:
  reshard_after_forward: true
  auto_wrap_policy: TRANSFORMER_BASED_WRAP
  transformer_layer_cls_to_wrap: LlamaDecoderLayer

# ========== è®­ç»ƒå‚æ•° ==========
micro_batch_size: 1  # CP è¦æ±‚ batch=1
gradient_accumulation_steps: 8
optimizer: adamw_torch_8bit  # 8-bit ä¼˜åŒ–å™¨è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜

# ========== æ˜¾å­˜ä¼˜åŒ– ==========
gradient_checkpointing: true
flash_attention: true
bf16: auto

datasets:
  - path: emozilla/pg_books-tokenized-bos-eos-chunked-65536
    type: completion
    field: text
```

**æ˜¾å­˜åˆ†æ**ï¼š

```
8 Ã— A100 80GB è®­ç»ƒ Llama-3.1-8Bï¼Œ32K ä¸Šä¸‹æ–‡

é…ç½®ï¼šCP=4, FSDP, Liger FLCE

æ¯ GPU å¤„ç†åºåˆ—é•¿åº¦ï¼š32K / 4 = 8K

æ˜¾å­˜å ç”¨ï¼ˆæ¯ GPUï¼‰ï¼š
1. å‚æ•°ï¼ˆFSDP åˆ†ç‰‡ï¼‰ï¼š
   8B Ã— 2 bytes / 8 GPUs = 2 GB

2. ä¼˜åŒ–å™¨ï¼ˆ8-bit Adamï¼‰ï¼š
   8B Ã— 1 byte Ã— 2 / 8 = 2 GB

3. æ¿€æ´»å€¼ï¼š
   - Attentionï¼ˆFlash Attnï¼‰ï¼š~4 GB
   - MLPï¼ˆLiger SwiGLUï¼‰ï¼š~3 GBï¼ˆvs æ ‡å‡† 8 GBï¼‰
   - å…¶ä»–ï¼š~2 GB

4. Loss è®¡ç®—ï¼ˆLiger FLCEï¼‰ï¼š
   - æ ‡å‡† CEï¼š1 Ã— 8K Ã— 128K Ã— 2 = 2 GB
   - Liger FLCEï¼š~0.1 GBï¼ˆåˆ†å—è®¡ç®—ï¼‰

æ€»è®¡ï¼š2 + 2 + 9 + 0.1 = 13.1 GB / GPU âœ…

å¯¹æ¯”æ—  Ligerï¼š
å‚æ•° + ä¼˜åŒ–å™¨ + æ¿€æ´»å€¼ï¼ˆæ ‡å‡†ï¼‰+ Lossï¼ˆæ ‡å‡†ï¼‰
= 2 + 2 + 14 + 2 = 20 GB / GPU

èŠ‚çœï¼š(20 - 13.1) / 20 = 34%
```

### 6.3 å…¼å®¹æ€§é…ç½®ï¼šLiger + DeepSpeed + LoRA

```yaml
base_model: meta-llama/Llama-3.1-70B

# ========== Liger é…ç½® ==========
plugins:
  - axolotl.integrations.liger.LigerPlugin

# æ³¨æ„å…¼å®¹æ€§ï¼š
liger_rms_norm: false  # LoRA è®­ç»ƒå»ºè®®ç¦ç”¨ï¼ˆå¯èƒ½å½±å“æ¢¯åº¦ï¼‰
liger_glu_activation: false  # åŒä¸Š
liger_fused_linear_cross_entropy: true  # FLCE å…¼å®¹ LoRA âœ…

# ========== LoRA é…ç½® ==========
adapter: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

# ========== DeepSpeed ZeRO-3 ==========
deepspeed: deepspeed_configs/zero3.json

# ========== è®­ç»ƒå‚æ•° ==========
sequence_len: 4096
micro_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 1e-4
```

**DeepSpeed é…ç½®**ï¼š

```json
// deepspeed_configs/zero3.json
{
  "zero_optimization": {
    "stage": 3,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "offload_param": {
      "device": "cpu",
      "pin_memory": true
    }
  },
  "bf16": {
    "enabled": true
  },
  "train_micro_batch_size_per_gpu": 1,
  "gradient_accumulation_steps": 16
}
```

**å…³é”®æ³¨æ„äº‹é¡¹**ï¼š

```yaml
# âŒ ä¸å…¼å®¹çš„ç»„åˆ
liger_rms_norm: true
tensor_parallel_size: 2  # Liger RMSNorm ä¸æ”¯æŒ TP

# âŒ å†²çª
liger_cross_entropy: true
liger_fused_linear_cross_entropy: true  # åªèƒ½é€‰ä¸€ä¸ª

# âŒ å¯èƒ½æœ‰é—®é¢˜
liger_glu_activation: true
tiled_mlp: true  # éœ€è¦è®¾ç½® tiled_mlp_use_original_mlp: true

# âœ… æ¨èç»„åˆ
liger_fused_linear_cross_entropy: true  # FLCE æ˜¯æ ¸å¿ƒ
flash_attention: true  # Flash Attn å…¼å®¹
gradient_checkpointing: true  # æ¢¯åº¦æ£€æŸ¥ç‚¹å…¼å®¹
```

### 6.4 å¯åŠ¨å‘½ä»¤

```bash
# å•èŠ‚ç‚¹ 8 å¡ FSDP
axolotl train examples/llama-3/fft-8b-liger-fsdp.yaml \
    --launcher accelerate \
    --num-processes 8

# å•èŠ‚ç‚¹ 8 å¡ DeepSpeed
axolotl train examples/llama-3/lora-70b-liger-deepspeed.yaml \
    --launcher deepspeed \
    --num-processes 8

# å¤šèŠ‚ç‚¹è®­ç»ƒï¼ˆ2 èŠ‚ç‚¹ Ã— 8 GPUï¼‰
# èŠ‚ç‚¹ 0ï¼š
axolotl train config.yaml \
    --launcher accelerate \
    --num-processes 16 \
    --num-machines 2 \
    --machine-rank 0 \
    --main-process-ip 192.168.1.1 \
    --main-process-port 29500

# èŠ‚ç‚¹ 1ï¼š
axolotl train config.yaml \
    --launcher accelerate \
    --num-processes 16 \
    --num-machines 2 \
    --machine-rank 1 \
    --main-process-ip 192.168.1.1 \
    --main-process-port 29500
```

### 6.5 éªŒè¯ Liger æ˜¯å¦ç”Ÿæ•ˆ

```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ éªŒè¯ä»£ç 
import sys

# æ£€æŸ¥ RMSNorm æ˜¯å¦è¢«æ›¿æ¢
import transformers.models.llama.modeling_llama as modeling_llama
print(f"LlamaRMSNorm: {modeling_llama.LlamaRMSNorm}")
# é¢„æœŸè¾“å‡ºï¼š<class 'liger_kernel.transformers.rms_norm.LigerRMSNorm'>

# æ£€æŸ¥ MLP æ˜¯å¦è¢«æ›¿æ¢
print(f"LlamaMLP: {modeling_llama.LlamaMLP}")
# é¢„æœŸè¾“å‡ºï¼š<class 'liger_kernel.transformers.swiglu.LigerSwiGLUMLP'>

# æ£€æŸ¥ forward æ–¹æ³•
from transformers import LlamaForCausalLM
print(f"Forward function: {LlamaForCausalLM.forward.__module__}")
# å¦‚æœå¯ç”¨ FLCEï¼Œåº”è¯¥æŒ‡å‘ axolotl.integrations.liger.models

# è¿è¡Œæ—¶éªŒè¯
import torch
model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B")

# æ£€æŸ¥å®ä¾‹ç±»å‹
for name, module in model.named_modules():
    if 'norm' in name.lower():
        print(f"{name}: {type(module)}")
    if 'mlp' in name.lower():
        print(f"{name}: {type(module)}")

# é¢„æœŸçœ‹åˆ° LigerRMSNorm, LigerSwiGLUMLP
```

**ç›‘æ§è®­ç»ƒæŒ‡æ ‡**ï¼š

```bash
# ç›‘æ§ GPU æ˜¾å­˜
watch -n 1 nvidia-smi

# é¢„æœŸï¼š
# - å¯ç”¨ Liger åæ˜¾å­˜å ç”¨æ˜¾è‘—é™ä½ï¼ˆ40-60%ï¼‰
# - ååæå‡ 15-25%

# è®­ç»ƒæ—¥å¿—ç¤ºä¾‹
# Liger ä¼šåœ¨å¯åŠ¨æ—¶æ‰“å°åº”ç”¨ä¿¡æ¯ï¼š
[INFO] Applying LIGER to llama with kwargs: {
    'rope': True,
    'rms_norm': True,
    'swiglu': True,
    'fused_linear_cross_entropy': True
}
```

---

## 7. å¸¸è§é—®é¢˜ä¸æœ€ä½³å®è·µ

### 7.1 å¸¸è§é—®é¢˜

#### é—®é¢˜ 1ï¼šLiger ä¸ Tensor Parallelism å†²çª

**ç—‡çŠ¶**ï¼š
```
ValueError: `liger_rms_norm` is incompatible with tensor parallelism
```

**åŸå› **ï¼š
- Liger çš„ RMSNorm å®ç°ä½¿ç”¨ Triton kernel
- Tensor Parallelism éœ€è¦æ¨¡å‹å±‚æ”¯æŒ DTensor
- Liger çš„ Triton kernel ä¸æ”¯æŒ DTensor æ“ä½œ

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# æ–¹æ¡ˆ 1ï¼šç¦ç”¨ liger_rms_norm
tensor_parallel_size: 2
liger_rms_norm: false  # â† ç¦ç”¨
liger_glu_activation: true  # å…¶ä»–ä¼˜åŒ–å¯ä¿ç•™
liger_fused_linear_cross_entropy: true

# æ–¹æ¡ˆ 2ï¼šä¸ä½¿ç”¨ TP
tensor_parallel_size: 1
liger_rms_norm: true
# æ”¹ç”¨å…¶ä»–å¹¶è¡Œç­–ç•¥ï¼ˆFSDP, CPï¼‰
```

#### é—®é¢˜ 2ï¼šFLCE åœ¨æ¨ç†æ—¶è¿”å› None logits

**ç—‡çŠ¶**ï¼š
```python
output = model.generate(input_ids, ...)
# AttributeError: 'NoneType' object has no attribute 'argmax'
```

**åŸå› **ï¼š
- FLCE çš„ forward åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä¸ç‰©åŒ– logits
- æ¨ç†æ—¶ä¹Ÿéœ€è¦ logits æ¥ç”Ÿæˆ tokens

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# Liger çš„å®ç°å·²ç»å¤„ç†äº†è¿™ä¸ªé—®é¢˜
# ç¡®ä¿æ¨ç†æ—¶è®¾ç½® model.eval()

model.eval()  # â† å…³é”®ï¼
with torch.no_grad():
    outputs = model.generate(input_ids, max_length=100)

# lce_forward ä¼šæ£€æµ‹ self.training çŠ¶æ€
# æ¨ç†æ¨¡å¼ä¸‹ä¼šç‰©åŒ– logits
```

**éªŒè¯ä»£ç **ï¼š
```python
# æ£€æŸ¥ forward é€»è¾‘
def lce_forward(self, input_ids, labels=None):
    ...
    if self.training and labels is not None:
        # è®­ç»ƒæ¨¡å¼ï¼šFLCEï¼Œä¸ç‰©åŒ– logits
        loss = LigerForCausalLMLoss(...)
        logits = None
    else:
        # æ¨ç†æ¨¡å¼ï¼šç‰©åŒ– logits
        logits = self.lm_head(hidden_states)
    ...
```

#### é—®é¢˜ 3ï¼šæ˜¾å­˜ä¸é™åå‡

**ç—‡çŠ¶**ï¼š
```
å¯ç”¨ Liger åï¼Œæ˜¾å­˜ä» 40GB å¢åŠ åˆ° 50GB
```

**å¯èƒ½åŸå› **ï¼š

1. **æœªå¯ç”¨ FLCE**ï¼š
   ```yaml
   # âŒ é”™è¯¯é…ç½®
   liger_cross_entropy: true  # æ ‡å‡† CEï¼Œæ˜¾å­˜èŠ‚çœæœ‰é™

   # âœ… æ­£ç¡®é…ç½®
   liger_fused_linear_cross_entropy: true  # FLCEï¼Œå¤§å¹…èŠ‚çœ
   ```

2. **åˆ†å—å¤§å°ä¸åˆé€‚**ï¼š
   ```python
   # Liger å†…éƒ¨åŠ¨æ€è°ƒæ•´ï¼Œä½†å¯èƒ½ä¸optimal
   # æ£€æŸ¥æ—¥å¿—ä¸­çš„ chunk_size

   # å¦‚æœæ˜¾å­˜ä»ä¸å¤Ÿï¼Œå¯ä»¥ä¿®æ”¹ Liger æºç ï¼ˆé«˜çº§ï¼‰
   # liger_kernel/ops/fused_linear_cross_entropy.py
   chunk_size = 2048  # é»˜è®¤ 4096ï¼Œå‡åŠè¿›ä¸€æ­¥é™ä½æ˜¾å­˜
   ```

3. **Triton kernel ç¼–è¯‘ç¼“å­˜**ï¼š
   ```bash
   # Triton ä¼šç¼“å­˜ç¼–è¯‘ç»“æœï¼Œé¦–æ¬¡è¿è¡Œæ˜¾å­˜å ç”¨é«˜
   # æ¸…é™¤ç¼“å­˜ï¼š
   rm -rf ~/.triton/cache

   # æˆ–è®¾ç½®ç¯å¢ƒå˜é‡é™åˆ¶ç¼“å­˜å¤§å°
   export TRITON_CACHE_DIR=/tmp/triton_cache
   ```

#### é—®é¢˜ 4ï¼šä¸ torch.compile å†²çª

**ç—‡çŠ¶**ï¼š
```
RuntimeError: Triton kernel compilation failed when using torch.compile
```

**åŸå› **ï¼š
- torch.compile å°è¯•ä¼˜åŒ– Triton kernel
- Liger çš„ patch æœªæ­£ç¡®åº”ç”¨

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# ç¡®ä¿é…ç½®ä¸­å¯ç”¨äº† compile ç¦ç”¨ patch
torch_compile: true
plugins:
  - axolotl.integrations.liger.LigerPlugin

# Liger ä¼šè‡ªåŠ¨æ£€æµ‹ torch_compile å¹¶åº”ç”¨ patch
```

**æ‰‹åŠ¨éªŒè¯**ï¼š
```python
import liger_kernel.ops.fused_linear_cross_entropy as flce_ops

# æ£€æŸ¥æ˜¯å¦è¢« @torch.compiler.disable è£…é¥°
print(flce_ops.fused_linear_cross_entropy_forward.__wrapped__)
# åº”è¯¥æ˜¾ç¤ºè¢«åŒ…è£…çš„å‡½æ•°
```

#### é—®é¢˜ 5ï¼šè®­ç»ƒä¸ç¨³å®š / Loss å‘æ•£

**ç—‡çŠ¶**ï¼š
```
Loss ä» 2.5 çªç„¶è·³åˆ° NaN æˆ– 1e10
```

**å¯èƒ½åŸå› **ï¼š

1. **æ•°å€¼ç²¾åº¦é—®é¢˜**ï¼š
   ```yaml
   # Liger ä½¿ç”¨ bf16ï¼ŒæŸäº›æ¨¡å‹å¯èƒ½éœ€è¦ fp32 ç´¯åŠ 
   # æ£€æŸ¥é…ç½®ï¼š
   bf16: auto  # è®© Accelerate è‡ªåŠ¨é€‰æ‹©
   # æˆ–å¼ºåˆ¶ fp32
   bf16: false
   fp16: false
   ```

2. **å­¦ä¹ ç‡è¿‡é«˜**ï¼š
   ```yaml
   # Liger æå‡ååï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡
   # åŸé…ç½®ï¼š
   learning_rate: 2e-5

   # ä½¿ç”¨ Liger åå»ºè®®ï¼š
   learning_rate: 1.5e-5  # ç•¥å¾®é™ä½
   warmup_ratio: 0.1      # å¢åŠ  warmup
   ```

3. **æ¢¯åº¦ç´¯åŠ é—®é¢˜**ï¼š
   ```yaml
   # FLCE çš„æ¢¯åº¦è®¡ç®—å¯èƒ½ä¸æ ‡å‡†å®ç°ç•¥æœ‰å·®å¼‚
   # æ£€æŸ¥æ¢¯åº¦è£å‰ªï¼š
   gradient_clipping: 1.0  # æ·»åŠ æ¢¯åº¦è£å‰ª
   ```

**è°ƒè¯•æ–¹æ³•**ï¼š
```python
# å¯¹æ¯” Liger å’Œæ ‡å‡†å®ç°çš„æ¢¯åº¦
# åœ¨å°æ•°æ®é›†ä¸Šæµ‹è¯•

# 1. æ ‡å‡†å®ç°è®­ç»ƒ 1 step
liger_fused_linear_cross_entropy: false
loss_standard, grads_standard = train_one_step()

# 2. Liger å®ç°è®­ç»ƒåŒä¸€ batch
liger_fused_linear_cross_entropy: true
loss_liger, grads_liger = train_one_step()

# 3. å¯¹æ¯”
print(f"Loss diff: {abs(loss_standard - loss_liger)}")
for name in grads_standard:
    diff = (grads_standard[name] - grads_liger[name]).abs().max()
    print(f"{name}: max_grad_diff={diff}")

# é¢„æœŸï¼šdiff < 1e-4ï¼ˆbf16 ç²¾åº¦ä¸‹å¯æ¥å—ï¼‰
```

### 7.2 æœ€ä½³å®è·µ

#### 1. Liger ä¼˜åŒ–ä¼˜å…ˆçº§

```
æ ¹æ®æ”¶ç›Šæ’åºï¼š

1. liger_fused_linear_cross_entropy (å¿…é€‰) â­â­â­â­â­
   - æ˜¾å­˜èŠ‚çœï¼š20-30x
   - é€Ÿåº¦æå‡ï¼š1.5-2x
   - é€‚ç”¨ï¼šæ‰€æœ‰åœºæ™¯

2. liger_glu_activation (å¼ºçƒˆæ¨è) â­â­â­â­
   - æ˜¾å­˜èŠ‚çœï¼š2x
   - é€Ÿåº¦æå‡ï¼š1.2-1.4x
   - é€‚ç”¨ï¼šMLP å æ¯”å¤§çš„æ¨¡å‹ï¼ˆæ ‡å‡† Transformerï¼‰

3. liger_rms_norm (æ¨è) â­â­â­
   - æ˜¾å­˜èŠ‚çœï¼š1.5x
   - é€Ÿåº¦æå‡ï¼š1.3-1.5x
   - é™åˆ¶ï¼šä¸å…¼å®¹ TP

4. liger_rope (å¯é€‰) â­â­
   - æ˜¾å­˜èŠ‚çœï¼š1.2x
   - é€Ÿåº¦æå‡ï¼š1.2-1.3x
   - æ³¨æ„ï¼šæŸäº›æ¨¡å‹ï¼ˆDeepSeek-V2ï¼‰ä¸æ”¯æŒ

5. liger_cross_entropy (ä¸æ¨è) â­
   - ä½¿ç”¨ FLCE ä»£æ›¿
   - åªåœ¨ FLCE ä¸å¯ç”¨æ—¶ä½¿ç”¨
```

#### 2. é…ç½®æ¨¡æ¿

**æ¨¡æ¿ 1ï¼šæœ€å¤§æ€§èƒ½ï¼ˆæ¨èï¼‰**
```yaml
plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_fused_linear_cross_entropy: true
liger_glu_activation: true
liger_rms_norm: true
liger_rope: true

# é€‚ç”¨ï¼š
# - å•èŠ‚ç‚¹è®­ç»ƒ
# - ä¸ä½¿ç”¨ TP
# - è¿½æ±‚æœ€å¤§ååå’Œæœ€å°æ˜¾å­˜
```

**æ¨¡æ¿ 2ï¼šä¿å®ˆé…ç½®ï¼ˆç¨³å®šï¼‰**
```yaml
plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_fused_linear_cross_entropy: true
# åªå¯ç”¨ FLCEï¼Œæœ€ç¨³å®š

# é€‚ç”¨ï¼š
# - å¤šèŠ‚ç‚¹è®­ç»ƒ
# - ä½¿ç”¨ TP / å¤æ‚å¹¶è¡Œç­–ç•¥
# - è¿½æ±‚ç¨³å®šæ€§
```

**æ¨¡æ¿ 3ï¼šå…¼å®¹é…ç½®ï¼ˆTP + Ligerï¼‰**
```yaml
plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_fused_linear_cross_entropy: false  # FLCE ä¸å…¼å®¹ TP
liger_glu_activation: true
liger_rms_norm: false  # RMSNorm ä¸å…¼å®¹ TP
liger_rope: true

tensor_parallel_size: 2

# é€‚ç”¨ï¼š
# - éœ€è¦ä½¿ç”¨ TP çš„åœºæ™¯
# - å¤§æ¨¡å‹å•å±‚æ˜¾å­˜ä»è¶…æ ‡
```

#### 3. æ€§èƒ½è°ƒä¼˜

**è°ƒä¼˜ 1ï¼šBatch Size è°ƒæ•´**

```yaml
# Liger èŠ‚çœæ˜¾å­˜ â†’ å¯ä»¥å¢å¤§ batch size

# åŸé…ç½®ï¼ˆæ—  Ligerï¼‰ï¼š
micro_batch_size: 2
gradient_accumulation_steps: 8
# Effective batch = 2 Ã— 8 = 16

# å¯ç”¨ Liger åï¼š
micro_batch_size: 4  # â† ç¿»å€
gradient_accumulation_steps: 4  # â† å‡åŠ
# Effective batch = 4 Ã— 4 = 16ï¼ˆä¿æŒä¸å˜ï¼‰

# æ”¶ç›Šï¼š
# - æ›´å°‘çš„æ¢¯åº¦ç´¯åŠ æ­¥éª¤ â†’ æ›´å¿«çš„è¿­ä»£
# - æ›´å¤§çš„ micro_batch â†’ æ›´é«˜çš„ GPU åˆ©ç”¨ç‡
```

**è°ƒä¼˜ 2ï¼šä¸å…¶ä»–ä¼˜åŒ–ç»„åˆ**

```yaml
# æœ€ä¼˜ç»„åˆï¼ˆLlama-3.1-8Bï¼Œ32K ä¸Šä¸‹æ–‡ï¼‰ï¼š
plugins:
  - axolotl.integrations.liger.LigerPlugin

# Liger ä¼˜åŒ–
liger_fused_linear_cross_entropy: true
liger_glu_activation: true
liger_rms_norm: true

# Flash Attentionï¼ˆå¿…éœ€ï¼‰
flash_attention: true

# æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå¯é€‰ï¼‰
gradient_checkpointing: true

# Sequence Parallelismï¼ˆé•¿ä¸Šä¸‹æ–‡ï¼‰
context_parallel_size: 4

# FSDP2
fsdp_version: 2
fsdp_config:
  reshard_after_forward: true

# æ•ˆæœï¼š
# - å•å¡ 32K ä¸Šä¸‹æ–‡ï¼š20 GBï¼ˆvs æ— ä¼˜åŒ– 80+ GBï¼‰
# - ååï¼š~1200 tokens/sï¼ˆvs æ— ä¼˜åŒ– 600 tokens/sï¼‰
```

**è°ƒä¼˜ 3ï¼šå­¦ä¹ ç‡ç¼©æ”¾**

```yaml
# Liger å…è®¸æ›´å¤§ batch size â†’ éœ€è¦è°ƒæ•´å­¦ä¹ ç‡

# å…¬å¼ï¼šlr_new = lr_base Ã— sqrt(batch_new / batch_base)
# æˆ–çº¿æ€§ç¼©æ”¾ï¼šlr_new = lr_base Ã— (batch_new / batch_base)

# åŸé…ç½®ï¼š
micro_batch_size: 2
learning_rate: 2e-5

# Liger åï¼ˆbatch ç¿»å€ï¼‰ï¼š
micro_batch_size: 4
learning_rate: 2.8e-5  # 2e-5 Ã— sqrt(2) â‰ˆ 2.8e-5

# æˆ–ä¿å®ˆç­–ç•¥ï¼ˆçº¿æ€§ç¼©æ”¾ï¼‰ï¼š
learning_rate: 4e-5  # 2e-5 Ã— 2
warmup_ratio: 0.1    # å¢åŠ  warmup ç¨³å®šè®­ç»ƒ
```

#### 4. è°ƒè¯•æŠ€å·§

**æŠ€å·§ 1ï¼šé€æ­¥å¯ç”¨ä¼˜åŒ–**

```bash
# ç¬¬ 1 æ­¥ï¼šbaselineï¼ˆæ—  Ligerï¼‰
liger_fused_linear_cross_entropy: false
# è®­ç»ƒ 100 stepsï¼Œè®°å½•ååå’Œæ˜¾å­˜

# ç¬¬ 2 æ­¥ï¼šåªå¯ç”¨ FLCE
liger_fused_linear_cross_entropy: true
# éªŒè¯ï¼šæ˜¾å­˜åº”é™ä½ 30-40%

# ç¬¬ 3 æ­¥ï¼šæ·»åŠ  MLP ä¼˜åŒ–
liger_glu_activation: true
# éªŒè¯ï¼šæ˜¾å­˜å†é™ä½ 10-15%

# ç¬¬ 4 æ­¥ï¼šæ·»åŠ  Norm ä¼˜åŒ–
liger_rms_norm: true
# éªŒè¯ï¼šååæå‡ 5-10%

# å¦‚æœæŸæ­¥å‡ºé—®é¢˜ï¼Œå›é€€åˆ°ä¸Šä¸€æ­¥
```

**æŠ€å·§ 2ï¼šæ•°å€¼éªŒè¯**

```python
# éªŒè¯ Liger è¾“å‡ºä¸æ ‡å‡†å®ç°ä¸€è‡´
import torch
from transformers import LlamaForCausalLM

# 1. åŠ è½½æ ‡å‡†æ¨¡å‹
model_std = LlamaForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B")
model_std.eval()

# 2. åº”ç”¨ Liger å¹¶åŠ è½½ç›¸åŒæƒé‡
from axolotl.integrations.liger.models.llama4 import apply_liger_kernel_to_llama4
apply_liger_kernel_to_llama4(fused_linear_cross_entropy=True)
model_liger = LlamaForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B")
model_liger.eval()

# 3. å¯¹æ¯”è¾“å‡º
input_ids = torch.randint(0, 32000, (1, 100))
labels = torch.randint(0, 32000, (1, 100))

with torch.no_grad():
    out_std = model_std(input_ids, labels=labels)
    out_liger = model_liger(input_ids, labels=labels)

print(f"Loss diff: {abs(out_std.loss - out_liger.loss).item()}")
# é¢„æœŸï¼š< 1e-4ï¼ˆæ•°å€¼è¯¯å·®å¯æ¥å—ï¼‰

# æ³¨æ„ï¼šlogits ä¼šä¸åŒï¼ˆLiger åœ¨è®­ç»ƒæ¨¡å¼ä¸ç‰©åŒ–ï¼‰
# éœ€è¦åœ¨ eval æ¨¡å¼ä¸‹å¯¹æ¯”
```

### 7.3 Liger vs å…¶ä»–ä¼˜åŒ–å¯¹æ¯”

| ä¼˜åŒ–æŠ€æœ¯ | æ˜¾å­˜èŠ‚çœ | é€Ÿåº¦æå‡ | å®ç°éš¾åº¦ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|---------|---------|
| **Liger FLCE** | â­â­â­â­â­ (20-30x lossè®¡ç®—) | â­â­â­â­ (1.5-2x) | â­ (é…ç½®å³ç”¨) | æ‰€æœ‰è®­ç»ƒ |
| **Flash Attention** | â­â­â­ (2-4x attn) | â­â­â­â­â­ (2-4x) | â­ (é…ç½®å³ç”¨) | æ‰€æœ‰è®­ç»ƒ |
| **Gradient Checkpointing** | â­â­â­â­ (2-4x) | â­â­ (-20~-30%) | â­ (é…ç½®å³ç”¨) | æ˜¾å­˜å—é™ |
| **TiledMLP** | â­â­â­â­â­ (4-16x MLP) | â­ (-30~-50%) | â­ (é…ç½®å³ç”¨) | é•¿ä¸Šä¸‹æ–‡ |
| **torch.compile** | â­ (10-20%) | â­â­â­ (1.3-1.8x) | â­â­ (éœ€è°ƒè¯•) | PyTorch 2.0+ |
| **FSDP** | â­â­â­â­ (Nxå‚æ•°) | â­â­ (é€šä¿¡å¼€é”€) | â­â­ (é…ç½®å¤æ‚) | å¤šGPU |

**ç»„åˆå»ºè®®**ï¼š

```
æ ‡å‡†è®­ç»ƒï¼ˆ< 8K tokensï¼‰ï¼š
  Flash Attention + Liger (FLCE + MLP + Norm) âœ…

é•¿ä¸Šä¸‹æ–‡ï¼ˆ8K-128K tokensï¼‰ï¼š
  Flash Attention + Liger + TiledMLP + Sequence Parallelism âœ…

è¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆ128K+ tokensï¼‰ï¼š
  ALST (Flash + Liger + TiledMLP + CP + Activation Offloading) âœ…

å¤šGPU è®­ç»ƒï¼š
  FSDP2 + Flash Attention + Liger + Gradient Checkpointing âœ…

æé™æ˜¾å­˜ä¼˜åŒ–ï¼š
  DeepSpeed ZeRO-3 + Liger FLCE + å‚æ•°å¸è½½ + 8-bit Adam âœ…
```

---

## æ€»ç»“

### Liger Kernel çš„æ ¸å¿ƒè¦ç‚¹

1. **æœ¬è´¨**ï¼šç”¨ Triton ç¼–å†™çš„é«˜æ€§èƒ½ GPU å†…æ ¸ï¼Œæ›¿æ¢ PyTorch/HuggingFace çš„æ ‡å‡†å®ç°
2. **æ ¸å¿ƒä¼˜åŒ–**ï¼šKernel Fusionï¼ˆç®—å­èåˆï¼‰+ Chunked Computationï¼ˆåˆ†å—è®¡ç®—ï¼‰
3. **æœ€å¤§æ”¶ç›Š**ï¼šFused Linear Cross Entropyï¼ˆ20-30å€æ˜¾å­˜èŠ‚çœï¼‰
4. **å®ç°æ–¹å¼**ï¼šMonkey Patchï¼ˆè¿è¡Œæ—¶æ›¿æ¢æ¨¡å—ï¼‰

### Axolotl ä¸­çš„ Liger ç‰¹ç‚¹

1. **æ— ç¼é›†æˆ**ï¼šé€šè¿‡æ’ä»¶ç³»ç»Ÿï¼Œé…ç½®å³ç”¨
2. **çµæ´»é…ç½®**ï¼šæ”¯æŒé€‰æ‹©æ€§å¯ç”¨å„ä¸ªä¼˜åŒ–
3. **å¹¿æ³›å…¼å®¹**ï¼šæ”¯æŒ FSDPã€DeepSpeedã€LoRAã€å¤šæ¨¡å‹æ¶æ„
4. **ç”Ÿäº§çº§**ï¼šLinkedIn å†…éƒ¨éªŒè¯ï¼Œå¼€æºç¤¾åŒºæ´»è·ƒ

### ä½•æ—¶ä½¿ç”¨ Ligerï¼Ÿ

```
âœ… ä½¿ç”¨ Liger çš„åœºæ™¯ï¼š
- æ‰€æœ‰ LLM è®­ç»ƒï¼ˆé»˜è®¤å¯ç”¨ï¼‰
- æ˜¾å­˜å—é™ï¼ˆFLCE å¿…é€‰ï¼‰
- è¿½æ±‚æœ€å¤§åå
- é•¿ä¸Šä¸‹æ–‡è®­ç»ƒï¼ˆé…åˆå…¶ä»–ä¼˜åŒ–ï¼‰

âš ï¸ éœ€è¦æ³¨æ„çš„åœºæ™¯ï¼š
- ä½¿ç”¨ Tensor Parallelismï¼ˆéƒ¨åˆ†ä¼˜åŒ–ä¸å…¼å®¹ï¼‰
- ä½¿ç”¨ torch.compileï¼ˆéœ€è¦ç¦ç”¨ patchï¼‰
- è‡ªå®šä¹‰æ¨¡å‹æ¶æ„ï¼ˆå¯èƒ½éœ€è¦é€‚é…ï¼‰

âŒ ä¸é€‚ç”¨çš„åœºæ™¯ï¼š
- çº¯æ¨ç†éƒ¨ç½²ï¼ˆæ”¶ç›Šæœ‰é™ï¼‰
- é Transformer æ¨¡å‹
- éœ€è¦å®Œå…¨å¤ç°æ ‡å‡†å®ç°çš„åœºæ™¯
```

### ä¸å…¶ä»–ä¼˜åŒ–çš„æ¯”è¾ƒ

**å›åˆ°"æ¬æ¡Œå­"æ¯”å–»**ï¼š

- **Tensor Parallelism**ï¼šå¤šäººåä½œæ¬**åŒä¸€å¼ æ¡Œå­çš„ä¸åŒéƒ¨åˆ†**ï¼ˆæ¨¡å‹åˆ‡åˆ†ï¼‰
- **TiledMLP**ï¼šæŠŠ**è¶…é•¿æ¡Œå­åˆ‡æˆå¤šæ®µ**ï¼Œé€æ®µæ¬è¿ï¼ˆåºåˆ—åˆ‡åˆ†ï¼‰
- **Liger Kernel**ï¼šä½¿ç”¨**æ›´å¥½çš„å·¥å…·**æ¬æ¡Œå­ï¼ˆç®—å­ä¼˜åŒ–ï¼‰

**ä¸‰è€…å¯ä»¥ç»„åˆä½¿ç”¨**ï¼š
```yaml
# 8 GPUs è®­ç»ƒ 70Bï¼Œ128K ä¸Šä¸‹æ–‡
tensor_parallel_size: 2    # TPï¼šæ¨¡å‹å±‚åˆ‡åˆ†ï¼ˆé™ä½å‚æ•°æ˜¾å­˜ï¼‰
context_parallel_size: 4   # CPï¼šåºåˆ—åˆ‡åˆ†ï¼ˆé™ä½æ¿€æ´»å€¼æ˜¾å­˜ï¼‰
plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_fused_linear_cross_entropy: true  # Ligerï¼šä¼˜åŒ–è®¡ç®—ï¼ˆé™ä½lossæ˜¾å­˜ï¼‰

å®Œç¾ååŒï¼
```

### è¿›ä¸€æ­¥å­¦ä¹ èµ„æº

- [Liger Kernel è®ºæ–‡](https://arxiv.org/abs/2410.10989)
- [Liger Kernel GitHub](https://github.com/linkedin/Liger-Kernel)
- [Triton æ•™ç¨‹](https://triton-lang.org/main/getting-started/tutorials/index.html)
- [Axolotl Liger é›†æˆæ–‡æ¡£](../custom_integrations.qmd#liger-kernels)
- [Flash Attention è®ºæ–‡](https://arxiv.org/abs/2205.14135)ï¼ˆç›¸å…³ä¼˜åŒ–ï¼‰

---

*æœ¬æ–‡æ¡£ç”± Claude åˆ›ä½œï¼Œæ—¨åœ¨å¸®åŠ© infra åˆå­¦è€…ç†è§£ Liger Kernelã€‚å¦‚æœ‰ç–‘é—®æˆ–å‘ç°é”™è¯¯ï¼Œæ¬¢è¿æ Issueï¼*

# Context Parallelism å¿«é€Ÿå‚è€ƒå¡ç‰‡ ğŸš€

> ä¸€é¡µçº¸é€ŸæŸ¥æ‰‹å†Œï¼Œé€‚åˆå¿«é€ŸæŸ¥é˜… CP é…ç½®å’Œå‘½ä»¤

---

## âš™ï¸ åŸºæœ¬é…ç½®

### æœ€å°åŒ– CP é…ç½®
```yaml
base_model: meta-llama/Llama-3.1-8B
context_parallel_size: 2  # ä»…æ­¤ä¸€è¡Œï¼

# CP è¦æ±‚é…ç½®
micro_batch_size: 1  # å¿…é¡»ä¸º 1
sequence_len: 8192   # é•¿ä¸Šä¸‹æ–‡

# å…¶ä»–å¿…éœ€é…ç½®
datasets:
  - path: tatsu-lab/alpaca
    type: alpaca
output_dir: ./outputs/cp-test/
bf16: true
flash_attention: true
```

### æ¨èçš„å®Œæ•´é…ç½®
```yaml
base_model: meta-llama/Llama-3.1-8B

# === å¹¶è¡Œé…ç½® ===
dp_shard_size: 2           # FSDP
tensor_parallel_size: 2    # TP
context_parallel_size: 2   # CP
# æ€»è®¡ï¼š2 Ã— 2 Ã— 2 = 8 GPUs

# === FSDP é…ç½® ===
fsdp_version: 2
fsdp_config:
  reshard_after_forward: true
  state_dict_type: FULL_STATE_DICT
  auto_wrap_policy: TRANSFORMER_BASED_WRAP
  transformer_layer_cls_to_wrap: LlamaDecoderLayer

# === é•¿ä¸Šä¸‹æ–‡é…ç½® ===
sequence_len: 16384        # 16K ä¸Šä¸‹æ–‡
micro_batch_size: 1        # CP å¼ºåˆ¶è¦æ±‚
gradient_accumulation_steps: 16  # è¡¥å¿å° batch

# === è®­ç»ƒé…ç½® ===
num_epochs: 1
optimizer: adamw_torch_fused
lr_scheduler: cosine
learning_rate: 2e-5

# === æ€§èƒ½ä¼˜åŒ– ===
bf16: true
tf32: true
flash_attention: true      # å¿…éœ€ï¼
gradient_checkpointing: true

# === è¾“å‡º ===
output_dir: ./outputs/cp-long-context/
logging_steps: 1
```

---

## ğŸ¯ å¸¸ç”¨åœºæ™¯é…ç½®

### åœºæ™¯ 1ï¼šå•èŠ‚ç‚¹ 8 å¡ï¼Œ16K ä¸Šä¸‹æ–‡
```yaml
# é€‰é¡¹ Aï¼šåªä½¿ç”¨ CPï¼ˆé€‚åˆå°æ¨¡å‹ï¼‰
dp_shard_size: 4
context_parallel_size: 2
# 4 Ã— 2 = 8 GPUs

# é€‰é¡¹ Bï¼šTP + CP ç»„åˆï¼ˆé€‚åˆå¤§æ¨¡å‹ï¼‰
dp_shard_size: 2
tensor_parallel_size: 2
context_parallel_size: 2  # â† æ¨è
# 2 Ã— 2 Ã— 2 = 8 GPUs

sequence_len: 16384
micro_batch_size: 1
```

### åœºæ™¯ 2ï¼šå•èŠ‚ç‚¹ 8 å¡ï¼Œ32K è¶…é•¿ä¸Šä¸‹æ–‡
```yaml
# æ›´æ¿€è¿›çš„ CP åˆ‡åˆ†
dp_shard_size: 2
context_parallel_size: 4
# 2 Ã— 4 = 8 GPUs

sequence_len: 32768
micro_batch_size: 1
gradient_accumulation_steps: 32  # å¢å¤§ä»¥è¡¥å¿
```

### åœºæ™¯ 3ï¼šåŒèŠ‚ç‚¹ 16 å¡ï¼Œ64K æé•¿ä¸Šä¸‹æ–‡
```yaml
# 4D å¹¶è¡Œï¼ˆFSDP + DDP + TP + CPï¼‰
dp_shard_size: 2           # èŠ‚ç‚¹å†… FSDP
dp_replicate_size: 2       # èŠ‚ç‚¹é—´ DDP
tensor_parallel_size: 2    # æ¨¡å‹å¹¶è¡Œ
context_parallel_size: 2   # åºåˆ—å¹¶è¡Œ
# 2 Ã— 2 Ã— 2 Ã— 2 = 16 GPUs

sequence_len: 65536
micro_batch_size: 1
```

### åœºæ™¯ 4ï¼šæµ‹è¯• CPï¼ˆæœ€å°é…ç½®ï¼‰
```yaml
# 2 å¡å¿«é€Ÿæµ‹è¯•
dp_shard_size: 1
context_parallel_size: 2
sequence_len: 4096
micro_batch_size: 1
max_steps: 10  # åªè·‘ 10 æ­¥æµ‹è¯•
```

---

## ğŸš€ è¿è¡Œå‘½ä»¤

### åŸºæœ¬å‘½ä»¤
```bash
# å•èŠ‚ç‚¹è®­ç»ƒ
axolotl train config.yaml

# æŒ‡å®š GPU æ•°é‡ï¼ˆCP éœ€è¦ç²¾ç¡®åŒ¹é…é…ç½®ï¼‰
axolotl train config.yaml --num-processes 8

# ä½¿ç”¨ torchrunï¼ˆæ¨èï¼‰
axolotl train config.yaml --launcher torchrun
```

### å¤šèŠ‚ç‚¹è®­ç»ƒ
```bash
# === Node 0 (master) ===
axolotl train config.yaml \
    --num-processes 16 \
    --num-machines 2 \
    --machine-rank 0 \
    --main-process-ip <NODE0_IP> \
    --main-process-port 29500

# === Node 1 ===
axolotl train config.yaml \
    --num-processes 16 \
    --num-machines 2 \
    --machine-rank 1 \
    --main-process-ip <NODE0_IP> \
    --main-process-port 29500
```

### è°ƒè¯•å‘½ä»¤
```bash
# æµ‹è¯• Ring é€šä¿¡
NCCL_DEBUG=INFO axolotl train config.yaml --max-steps 2

# æ£€æŸ¥åºåˆ—åˆ‡åˆ†æ˜¯å¦æ­£ç¡®
# åœ¨é…ç½®ä¸­æ·»åŠ ï¼š
# logging_steps: 1
# è§‚å¯Ÿæ—¥å¿—ä¸­çš„ sequence length

# éªŒè¯æ˜¾å­˜èŠ‚çœ
nvidia-smi dmon -s mu -c 10
```

---

## ğŸ” è°ƒè¯•é€ŸæŸ¥

### é—®é¢˜ï¼šæ˜¾å­˜ä»ç„¶ OOMï¼ˆå³ä½¿å¼€å¯ CPï¼‰

#### æ£€æŸ¥æ¸…å•
```bash
âœ“ ç¡®è®¤ micro_batch_size = 1
âœ“ ç¡®è®¤ flash_attention = true
âœ“ ç¡®è®¤ sequence_len èƒ½è¢« context_parallel_size æ•´é™¤
âœ“ å¼€å¯ gradient_checkpointing
âœ“ å¢å¤§ context_parallel_size
```

#### é…ç½®è°ƒæ•´
```yaml
# ä¹‹å‰
context_parallel_size: 2
sequence_len: 16384
gradient_accumulation_steps: 8

# ä¹‹å
context_parallel_size: 4    # å¢å¤§ CP
sequence_len: 16384
gradient_accumulation_steps: 32  # å¢å¤§ä»¥è¡¥å¿
# æˆ–é™ä½åºåˆ—é•¿åº¦
sequence_len: 8192         # å‡åŠ
```

---

### é—®é¢˜ï¼šè®­ç»ƒé€Ÿåº¦ææ…¢

#### æ£€æŸ¥ GPU äº’è¿
```bash
nvidia-smi topo -m

# âœ… å¥½ï¼ˆNVLinkï¼‰:
#   GPU0  GPU1  GPU2  GPU3
# 0   X    NV12  NV12  NV12
# 1  NV12   X    NV12  NV12
# 2  NV12  NV12   X    NV12
# 3  NV12  NV12  NV12   X

# âŒ å·®ï¼ˆPCIeï¼‰:
#   GPU0  GPU1
# 0   X    PHB
# 1  PHB   X
```

**CP å¯¹é€šä¿¡å¸¦å®½æåº¦æ•æ„Ÿï¼**
- Ring-Flash-Attention éœ€è¦æ¯ä¸ª GPU ä¸ç›¸é‚» GPU é¢‘ç¹é€šä¿¡
- PCIe å¸¦å®½å¯èƒ½å¯¼è‡´ 10-100Ã— æ€§èƒ½ä¸‹é™
- **å¼ºçƒˆå»ºè®®**ï¼šåŒä¸€ CP ç»„çš„ GPU å¿…é¡»åœ¨åŒä¸€èŠ‚ç‚¹ä¸”æœ‰ NVLink

#### æ€§èƒ½ä¼˜åŒ–é…ç½®
```yaml
# Flash Attentionï¼ˆå¿…éœ€ï¼‰
flash_attention: true

# æ··åˆç²¾åº¦
bf16: true
tf32: true

# Fused ç®—å­
optimizer: adamw_torch_fused

# æ¢¯åº¦æ£€æŸ¥ç‚¹
gradient_checkpointing: true

# æ•°æ®åŠ è½½ä¼˜åŒ–
dataloader_num_workers: 4
dataloader_pin_memory: true
```

---

### é—®é¢˜ï¼šLoss NaN æˆ–ä¸ç¨³å®š

#### é…ç½®è°ƒæ•´
```yaml
# ä½¿ç”¨ bf16ï¼ˆæ¯” fp16 æ›´ç¨³å®šï¼‰
bf16: true
fp16: false

# æ¢¯åº¦è£å‰ªï¼ˆCP å°¤å…¶é‡è¦ï¼‰
max_grad_norm: 1.0

# é™ä½å­¦ä¹ ç‡
learning_rate: 1e-5  # CP å¯èƒ½éœ€è¦æ›´å°çš„ LR

# Warmupï¼ˆè®©æ¨¡å‹é€‚åº” Ring-Attentionï¼‰
warmup_steps: 100
warmup_ratio: 0.1

# æ£€æŸ¥åºåˆ—é•¿åº¦æ˜¯å¦è¿‡é•¿
sequence_len: 8192  # å…ˆä»è¾ƒçŸ­åºåˆ—å¼€å§‹æµ‹è¯•
```

---

### é—®é¢˜ï¼šåºåˆ—åˆ‡åˆ†é”™è¯¯

#### è¯Šæ–­
```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ ï¼š
print(f"åŸå§‹åºåˆ—é•¿åº¦: {input_ids.shape[1]}")
print(f"CP size: {context_parallel_size}")
print(f"æ¯ä¸ª GPU åºåˆ—é•¿åº¦: {input_ids.shape[1] // context_parallel_size}")
```

#### è¦æ±‚
```yaml
# sequence_len å¿…é¡»èƒ½è¢« context_parallel_size æ•´é™¤
sequence_len: 16384
context_parallel_size: 4  # 16384 / 4 = 4096 âœ…

# é”™è¯¯ç¤ºä¾‹ï¼š
sequence_len: 10000
context_parallel_size: 3  # 10000 / 3 = 3333.33... âŒ
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”è¡¨

### åºåˆ—é•¿åº¦ â†’ CP é…ç½®æ˜ å°„

| åºåˆ—é•¿åº¦ | å•èŠ‚ç‚¹ 8 å¡ | æ˜¾å­˜èŠ‚çœ | å¤‡æ³¨ |
|---------|------------|---------|------|
| 2K-4K | CP=1 (ä¸éœ€è¦) | - | å¸¸è§„ FSDP å³å¯ |
| 8K | CP=2, TP=2, FSDP=2 | ~30% | æ¨èèµ·ç‚¹ |
| 16K | CP=2, TP=2, FSDP=2 | ~50% | å¸¸ç”¨é…ç½® |
| 32K | CP=4, TP=2, FSDP=1 | ~70% | éœ€è¦ NVLink |
| 64K+ | CP=4, TP=2 (å¤šèŠ‚ç‚¹) | ~75% | éœ€è¦å¤šèŠ‚ç‚¹ |

### Llama-8B é•¿ä¸Šä¸‹æ–‡æ€§èƒ½å‚è€ƒ (8Ã—A100 80GB)

| Seq Len | CP Size | æ˜¾å­˜/GPU | Tokens/s/GPU | é€šä¿¡å¼€é”€ |
|---------|---------|---------|--------------|---------|
| 4K | 1 (çº¯FSDP) | ~35GB | 3000 | 0% |
| 8K | 2 | ~25GB | 2400 | ~20% |
| 16K | 2 | ~30GB | 2000 | ~33% |
| 32K | 4 | ~25GB | 1200 | ~60% |

**æ³¨æ„**ï¼šé€šä¿¡å¼€é”€éš CP size çº¿æ€§å¢é•¿

---

## ğŸ› ï¸ å¸¸ç”¨ä»£ç ç‰‡æ®µ

### æ£€æŸ¥ CP æ˜¯å¦ç”Ÿæ•ˆ
```python
# åœ¨è®­ç»ƒå‰æ·»åŠ 
import torch.distributed as dist

if dist.is_initialized():
    world_size = dist.get_world_size()
    rank = dist.get_rank()

    # æ£€æŸ¥åºåˆ—åˆ‡åˆ†
    for batch in train_dataloader:
        local_seq_len = batch['input_ids'].shape[1]
        print(f"Rank {rank}: æœ¬åœ°åºåˆ—é•¿åº¦ = {local_seq_len}")

        # éªŒè¯æ€»é•¿åº¦
        expected_total = local_seq_len * world_size  # å¦‚æœåªæœ‰ CP
        print(f"é¢„æœŸæ€»åºåˆ—é•¿åº¦ = {expected_total}")
        break  # åªæ£€æŸ¥ç¬¬ä¸€ä¸ª batch
```

### ç›‘æ§ Ring é€šä¿¡
```python
# æ·»åŠ  NCCL æ—¥å¿—
import os
os.environ['NCCL_DEBUG'] = 'INFO'
os.environ['NCCL_DEBUG_SUBSYS'] = 'COLL'

# è®­ç»ƒæ—¶è§‚å¯Ÿæ—¥å¿—ï¼Œåº”è¯¥çœ‹åˆ°ç±»ä¼¼ï¼š
# NCCL INFO Ring 00 : 0 -> 1 -> 2 -> 3 -> 0
```

### è®¡ç®—æœ‰æ•ˆ Batch Size (CP åœºæ™¯)
```python
# CP ä¸‹çš„æœ‰æ•ˆ batch size è®¡ç®—
effective_batch_size = (
    micro_batch_size *              # å¿…é¡»ä¸º 1
    gradient_accumulation_steps *   # è¡¥å¿å° batch
    dp_shard_size *                 # FSDP å¹¶è¡Œåº¦
    dp_replicate_size               # DDP å¹¶è¡Œåº¦ï¼ˆå¦‚æœæœ‰ï¼‰
)

# ä¾‹å¦‚ï¼š
# micro_batch_size = 1
# gradient_accumulation_steps = 32
# dp_shard_size = 4
# dp_replicate_size = 1
# effective_batch_size = 1 Ã— 32 Ã— 4 Ã— 1 = 128

# æ³¨æ„ï¼šCP ä¸å‚ä¸ effective batch size è®¡ç®—ï¼
# å› ä¸º CP æ˜¯åºåˆ—å¹¶è¡Œï¼Œä¸æ˜¯æ•°æ®å¹¶è¡Œ
```

### æ£€æŸ¥åºåˆ—å¯¹é½
```python
# éªŒè¯åºåˆ—é•¿åº¦æ˜¯å¦æ­£ç¡®åˆ‡åˆ†
def check_sequence_alignment(sequence_len, cp_size):
    if sequence_len % cp_size != 0:
        raise ValueError(
            f"åºåˆ—é•¿åº¦ {sequence_len} ä¸èƒ½è¢« CP size {cp_size} æ•´é™¤ï¼"
            f"å»ºè®®è°ƒæ•´ä¸º {(sequence_len // cp_size + 1) * cp_size}"
        )
    chunk_size = sequence_len // cp_size
    print(f"âœ… æ¯ä¸ª GPU å¤„ç† {chunk_size} tokens")
    return chunk_size

# ä½¿ç”¨
check_sequence_alignment(16384, 4)  # âœ… è¾“å‡ºï¼šæ¯ä¸ª GPU å¤„ç† 4096 tokens
check_sequence_alignment(10000, 3)  # âŒ æŠ¥é”™
```

---

## âš¡ æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•

### å¿…åšä¼˜åŒ– âœ…
- [ ] å¯ç”¨ Flash Attention (`flash_attention: true`) **â† CP å¼ºåˆ¶è¦æ±‚**
- [ ] ä½¿ç”¨ bf16 (`bf16: true`)
- [ ] è®¾ç½® `micro_batch_size: 1` **â† CP å¼ºåˆ¶è¦æ±‚**
- [ ] éªŒè¯åºåˆ—é•¿åº¦å¯æ•´é™¤ (`sequence_len % cp_size == 0`)
- [ ] å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (`gradient_checkpointing: true`)

### é€šä¿¡ä¼˜åŒ– ğŸš€
- [ ] ç¡®ä¿ CP ç»„å†… GPU æœ‰ NVLink (`nvidia-smi topo -m`)
- [ ] CP ç»„å†… GPU åœ¨åŒä¸€èŠ‚ç‚¹ï¼ˆé¿å…è·¨èŠ‚ç‚¹ Ringï¼‰
- [ ] å¯ç”¨ NCCL ä¼˜åŒ– (`NCCL_IB_DISABLE=0` for InfiniBand)
- [ ] ä½¿ç”¨é«˜é€Ÿç½‘ç»œï¼ˆè‡³å°‘ 100Gbpsï¼‰

### æ˜¾å­˜ä¼˜åŒ– ğŸ’¾
- [ ] å¢å¤§ `context_parallel_size`ï¼ˆçº¿æ€§å‡å°‘æ˜¾å­˜ï¼‰
- [ ] å¼€å¯ FSDP reshard (`reshard_after_forward: true`)
- [ ] å¢å¤§ `gradient_accumulation_steps` è¡¥å¿å° batch
- [ ] è€ƒè™‘é™ä½ `sequence_len`ï¼ˆå¦‚æœä¸šåŠ¡å…è®¸ï¼‰

### è°ƒè¯•ä¼˜åŒ– ğŸ›
- [ ] æ£€æŸ¥åºåˆ—åˆ‡åˆ†æ—¥å¿—
- [ ] ç›‘æ§ NCCL é€šä¿¡æ—¶é—´ (`NCCL_DEBUG=INFO`)
- [ ] éªŒè¯æ¯ä¸ª GPU çš„åºåˆ—é•¿åº¦ä¸€è‡´
- [ ] æµ‹è¯•ä¸åŒ CP size çš„æ€§èƒ½æ›²çº¿

---

## ğŸ”„ Ring-Flash-Attention æ ¸å¿ƒåŸç†

### ä¸€å¥è¯æ€»ç»“
**æ¯ä¸ª GPU ä¿ç•™å®Œæ•´ Qï¼Œä½† K/V åœ¨ Ring ä¸Šè½®æµä¼ é€’ï¼Œæ¯ä¸€è½®è®¡ç®—éƒ¨åˆ† Attention å¹¶ç”¨ Online Softmax å¢é‡åˆå¹¶ã€‚**

### æ‰§è¡Œæµç¨‹ï¼ˆ4 ä¸ª GPU ç¤ºä¾‹ï¼‰
```
Step 0: åˆå§‹çŠ¶æ€
GPU 0: Qâ‚€, Kâ‚€, Vâ‚€ â†’ è®¡ç®— Attnâ‚€
GPU 1: Qâ‚, Kâ‚, Vâ‚ â†’ è®¡ç®— Attnâ‚
GPU 2: Qâ‚‚, Kâ‚‚, Vâ‚‚ â†’ è®¡ç®— Attnâ‚‚
GPU 3: Qâ‚ƒ, Kâ‚ƒ, Vâ‚ƒ â†’ è®¡ç®— Attnâ‚ƒ

Step 1: Ring ä¼ é€’ K/V
GPU 0: Qâ‚€, Kâ‚ƒ, Vâ‚ƒ â†’ è®¡ç®— Attnâ‚€' å¹¶åˆå¹¶
GPU 1: Qâ‚, Kâ‚€, Vâ‚€ â†’ è®¡ç®— Attnâ‚' å¹¶åˆå¹¶
GPU 2: Qâ‚‚, Kâ‚, Vâ‚ â†’ è®¡ç®— Attnâ‚‚' å¹¶åˆå¹¶
GPU 3: Qâ‚ƒ, Kâ‚‚, Vâ‚‚ â†’ è®¡ç®— Attnâ‚ƒ' å¹¶åˆå¹¶

Step 2: ç»§ç»­ä¼ é€’...
ï¼ˆæ€»å…± 4 è½®ï¼Œæ¯ä¸ª GPU çœ‹åˆ°æ‰€æœ‰ K/Vï¼‰

æœ€ç»ˆ: æ¯ä¸ª GPU å¾—åˆ°å®Œæ•´çš„ Attention è¾“å‡º
```

### å…³é”®æŠ€æœ¯ï¼šOnline Softmax
```python
# ä¼ ç»Ÿ Softmaxï¼ˆéœ€è¦å®Œæ•´åºåˆ—ï¼‰
scores = Q @ K^T / sqrt(d)
attn_weights = softmax(scores)  # éœ€è¦çŸ¥é“æ‰€æœ‰ scores
output = attn_weights @ V

# Online Softmaxï¼ˆå¢é‡æ›´æ–°ï¼‰
# ç¬¬ 1 è½®
scoresâ‚ = Q @ Kâ‚^T
maxâ‚ = max(scoresâ‚)
exp_scoresâ‚ = exp(scoresâ‚ - maxâ‚)
sumâ‚ = sum(exp_scoresâ‚)
outputâ‚ = (exp_scoresâ‚ @ Vâ‚) / sumâ‚

# ç¬¬ 2 è½®ï¼ˆåˆå¹¶ï¼‰
scoresâ‚‚ = Q @ Kâ‚‚^T
maxâ‚‚ = max(maxâ‚, max(scoresâ‚‚))
# é‡æ–°ç¼©æ”¾ä¹‹å‰çš„ç»“æœ
exp_scoresâ‚ *= exp(maxâ‚ - maxâ‚‚)
exp_scoresâ‚‚ = exp(scoresâ‚‚ - maxâ‚‚)
sumâ‚‚ = sumâ‚ * exp(maxâ‚ - maxâ‚‚) + sum(exp_scoresâ‚‚)
outputâ‚‚ = (outputâ‚ * sumâ‚ * exp(maxâ‚ - maxâ‚‚) + exp_scoresâ‚‚ @ Vâ‚‚) / sumâ‚‚

# ç»§ç»­è¿­ä»£...
```

---

## ğŸ“ é…ç½®å…¬å¼

### GPU æ•°é‡è®¡ç®—
```
æ€» GPU æ•° = dp_shard_size Ã— dp_replicate_size Ã— tensor_parallel_size Ã— context_parallel_size
```

### æ˜¾å­˜èŠ‚çœä¼°ç®—
```
æ˜¾å­˜èŠ‚çœ â‰ˆ 1 - (1 / context_parallel_size)

ä¾‹å¦‚ï¼š
CP=1 (æ— CP): èŠ‚çœ 0%
CP=2: èŠ‚çœ ~50%
CP=4: èŠ‚çœ ~75%
CP=8: èŠ‚çœ ~87.5%
```

### é€šä¿¡å¼€é”€ä¼°ç®—
```
é¢å¤–é€šä¿¡æ—¶é—´ â‰ˆ (context_parallel_size - 1) Ã— (å•æ¬¡ K/V ä¼ è¾“æ—¶é—´)

é€šä¿¡ä¸è®¡ç®—é‡å åï¼Œå®é™…å¼€é”€çº¦ä¸ºï¼š
é€šä¿¡å¼€é”€% â‰ˆ 20% + 15% Ã— (context_parallel_size - 1)

ä¾‹å¦‚ï¼š
CP=2: ~35% å¼€é”€
CP=4: ~65% å¼€é”€
CP=8: ~125% å¼€é”€ï¼ˆå¯èƒ½å˜æ…¢ï¼ï¼‰
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### âœ… æ¨è
- **åºåˆ—é•¿åº¦ â‰¥ 8K** æ—¶æ‰è€ƒè™‘ CPï¼ˆå¦åˆ™é€šä¿¡å¼€é”€ä¸åˆ’ç®—ï¼‰
- **CP size = 2 æˆ– 4**ï¼ˆæ›´å¤§çš„ CP é€šä¿¡å¼€é”€è¿‡é«˜ï¼‰
- **CP ç»„å†… GPU åœ¨åŒä¸€èŠ‚ç‚¹**ï¼ˆå¿…é¡»æœ‰ NVLinkï¼‰
- **TP Ã— CP â‰¤ 8**ï¼ˆå•èŠ‚ç‚¹å†…ï¼‰
- **å…ˆç”¨ TP åˆ‡æ¨¡å‹ï¼Œæ˜¾å­˜è¿˜ä¸å¤Ÿå†ç”¨ CP åˆ‡åºåˆ—**

### âŒ é¿å…
- CP size > 8ï¼ˆé€šä¿¡å¼€é”€çˆ†ç‚¸ï¼‰
- CP è·¨èŠ‚ç‚¹ï¼ˆRing å»¶è¿Ÿè¿‡é«˜ï¼‰
- çŸ­åºåˆ—ï¼ˆ< 4Kï¼‰ä½¿ç”¨ CPï¼ˆå¾—ä¸å¿å¤±ï¼‰
- micro_batch_size > 1ï¼ˆCP ä¸æ”¯æŒï¼‰
- ä¸æ£€æŸ¥ `sequence_len % cp_size == 0`

### ğŸ¯ å†³ç­–æ ‘
```
æ˜¾å­˜ä¸å¤Ÿï¼Ÿ
â”œâ”€ æ¨¡å‹å¤ªå¤§ â†’ ç”¨ TP
â””â”€ åºåˆ—å¤ªé•¿ â†’ ç”¨ CP
    â”œâ”€ 8K-16K â†’ CP=2
    â”œâ”€ 32K â†’ CP=4
    â””â”€ 64K+ â†’ CP=4 + å¤šèŠ‚ç‚¹
```

---

## ğŸ“š å¿«é€Ÿé“¾æ¥

- [è¯¦ç»†æ•™ç¨‹](./context_parallelism_deep_dive.md)
- [æºç è§£æ](./cp_source_code_walkthrough.md)
- [TP å¿«é€Ÿå‚è€ƒ](./tp_quick_reference.md)
- [åˆ†ææ–‡æ¡£ç´¢å¼•](./README.md)

---

## ğŸ’¡ é€Ÿè®°å£è¯€

```
CP ä¸“æ²»åºåˆ—é•¿ï¼Œ
Ring ä¼ é€’æ˜¾å­˜é™ã€‚
Flash Attention æ˜¯åŸºç¡€ï¼Œ
Batch å¿…é¡»ç­‰äºä¸€ã€‚

åºåˆ—é•¿åº¦èƒ½æ•´é™¤ï¼Œ
NVLink é€šä¿¡æ•ˆç‡é«˜ã€‚
å…ˆç”¨ TP åˆ‡æ¨¡å‹ï¼Œ
å†ç”¨ CP åˆ‡åºåˆ—å¥½ã€‚

æ˜¾å­˜èŠ‚çœçœ‹ CP æ•°ï¼Œ
é€šä¿¡å¼€é”€ä¹Ÿéšä¹‹æ¶¨ã€‚
å…«åƒä»¥ä¸‹ä¸å»ºè®®ï¼Œ
é•¿ä¸Šä¸‹æ–‡æ˜¾ç¥é€šã€‚
```

---

## ğŸ”¢ é…ç½®ç¤ºä¾‹é€ŸæŸ¥

### 16K ä¸Šä¸‹æ–‡ï¼Œ8 å¡
```yaml
dp_shard_size: 2
tensor_parallel_size: 2
context_parallel_size: 2
sequence_len: 16384
micro_batch_size: 1
gradient_accumulation_steps: 16
```

### 32K ä¸Šä¸‹æ–‡ï¼Œ8 å¡
```yaml
dp_shard_size: 2
context_parallel_size: 4
sequence_len: 32768
micro_batch_size: 1
gradient_accumulation_steps: 32
```

### 64K ä¸Šä¸‹æ–‡ï¼Œ16 å¡ï¼ˆåŒèŠ‚ç‚¹ï¼‰
```yaml
dp_shard_size: 2
dp_replicate_size: 2
tensor_parallel_size: 2
context_parallel_size: 2
sequence_len: 65536
micro_batch_size: 1
gradient_accumulation_steps: 64
```

---

*æ‰“å°æ­¤é¡µä½œä¸ºé€ŸæŸ¥æ‰‹å†Œ | æœ€åæ›´æ–°ï¼š2025-11*

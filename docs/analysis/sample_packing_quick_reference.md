# Sample Packing å¿«é€Ÿå‚è€ƒå¡ç‰‡ ğŸš€

> ä¸€é¡µçº¸é€ŸæŸ¥æ‰‹å†Œï¼Œå¿«é€Ÿä¸Šæ‰‹ Sample Packing

---

## âš¡ 30 ç§’å†³ç­–æŒ‡å—

```
éœ€è¦å¯ç”¨ Sample Packingï¼Ÿçœ‹è¿™é‡Œï¼š

â”œâ”€ é¢„è®­ç»ƒï¼Ÿ
â”‚  â””â”€ âœ… å¿…é¡»å¯ç”¨ (æ”¶ç›Šæœ€å¤§ï¼Œ2-4å€åŠ é€Ÿ)
â”‚
â”œâ”€ æŒ‡ä»¤å¾®è°ƒ/SFTï¼Ÿ
â”‚  â””â”€ âœ… å¼ºçƒˆæ¨è (2-3å€åŠ é€Ÿ)
â”‚
â”œâ”€ åºåˆ—é•¿åº¦å·®å¼‚å¤§ï¼Ÿ
â”‚  â””â”€ âœ… å¯ç”¨ (åˆ©ç”¨ç‡ä»30% â†’ 95%)
â”‚
â”œâ”€ åºåˆ—é•¿åº¦å‡åŒ€(>80%æ¥è¿‘max_len)ï¼Ÿ
â”‚  â””â”€ âŒ æ”¶ç›Šæœ‰é™(<10%)ï¼Œå¯ä¸å¯ç”¨
â”‚
â””â”€ è¶…é•¿åºåˆ—è®­ç»ƒ(>16K)ï¼Ÿ
   â””â”€ âš ï¸ è¯„ä¼°æ”¶ç›Šï¼Œå¯èƒ½ç”¨CPæ›´å¥½
```

---

## âš™ï¸ åŸºæœ¬é…ç½®

### æœ€ç®€é…ç½®ï¼ˆæ¨èæ–°æ‰‹ï¼‰

```yaml
base_model: meta-llama/Llama-3.1-8B
sequence_len: 2048

# âœ… å¯ç”¨Sample Packing
sample_packing: true

# âœ… æ¨èï¼šé…åˆFlash Attention
flash_attention: true

# âœ… æ¨èï¼šä¸è¦padåˆ°å›ºå®šé•¿åº¦
pad_to_sequence_len: false

# è®­ç»ƒå‚æ•°
micro_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 2e-5
```

### å®Œæ•´é…ç½®ï¼ˆè¿›é˜¶ï¼‰

```yaml
base_model: meta-llama/Llama-3.1-13B
sequence_len: 4096

# === Sample Packingæ ¸å¿ƒé…ç½® ===
sample_packing: true                    # å¯ç”¨packing
sample_packing_eff_est: 0.95            # é¢„ä¼°æ•ˆç‡(ç”¨äºè°ƒæ•´batch size)

# Packingæ¨¡å¼
sample_packing_sequentially: false      # false=å¹¶è¡ŒFFD(æ¨è), true=é¡ºåº

# Biné…ç½®
sample_packing_bin_size: 200            # æ¯binæœ€å¤š200ä¸ªåºåˆ—
sample_packing_group_size: 100000       # æ¯ç»„å¤„ç†100Kåºåˆ—

# Evalé…ç½®
eval_sample_packing: true               # Evalä¹Ÿå¯ç”¨packing

# === å¿…è¦é…ç½® ===
flash_attention: true                   # âš ï¸ å¿…é¡»å¯ç”¨Flash Attention!
pad_to_sequence_len: false              # âš ï¸ å…³é”®ï¼šå…³é—­å›ºå®špadding

# === ä¼˜åŒ–é…ç½® ===
gradient_checkpointing: true
bf16: true
```

---

## ğŸ“Š å¿«é€Ÿå¯¹æ¯”è¡¨

| ç»´åº¦ | æ— Sample Packing | å¯ç”¨Sample Packing |
|------|-----------------|-------------------|
| **Tokenåˆ©ç”¨ç‡** | 30-40% | 90-95% |
| **Throughput** | åŸºå‡† | +2-3å€ |
| **è®­ç»ƒæ—¶é—´** | åŸºå‡† | -50-60% |
| **GPUåˆ©ç”¨ç‡** | 40-50% | 80-90% |
| **å†…å­˜ä½¿ç”¨** | é«˜ (å¤§é‡padding) | ä½ (å°‘padding) |
| **æ”¶æ•›æ€§** | æ ‡å‡† | ç›¸åŒ âœ… |
| **é…ç½®å¤æ‚åº¦** | ç®€å• | ç®€å• (1-2è¡Œé…ç½®) |
| **æˆæœ¬** | åŸºå‡† | -50-60% ğŸ’° |

---

## ğŸ¯ å¸¸è§åœºæ™¯é…ç½®

### åœºæ™¯ 1ï¼šé¢„è®­ç»ƒï¼ˆæ”¶ç›Šæœ€å¤§ï¼‰

```yaml
base_model: meta-llama/Llama-3.1-8B
sequence_len: 4096

pretraining_dataset: path/to/data

# Sample Packing
sample_packing: true
sample_packing_eff_est: 0.98  # é¢„è®­ç»ƒé€šå¸¸æ•ˆç‡æ›´é«˜
pretrain_multipack_attn: true  # é¢„è®­ç»ƒä¸“ç”¨

flash_attention: true
pad_to_sequence_len: false

# å¤§batchè®­ç»ƒ
micro_batch_size: 8
gradient_accumulation_steps: 16

# é¢„æœŸæ”¶ç›Š: 3-4å€åŠ é€Ÿ ğŸš€
```

### åœºæ™¯ 2ï¼šæŒ‡ä»¤å¾®è°ƒ (SFT)

```yaml
base_model: meta-llama/Llama-3.1-8B
sequence_len: 2048

datasets:
  - path: tatsu-lab/alpaca
    type: alpaca

# Sample Packing
sample_packing: true
sample_packing_eff_est: 0.95

flash_attention: true
pad_to_sequence_len: false

micro_batch_size: 4
gradient_accumulation_steps: 4

# é¢„æœŸæ”¶ç›Š: 2-3å€åŠ é€Ÿ
```

### åœºæ™¯ 3ï¼šDDP + Sample Packing

```yaml
base_model: meta-llama/Llama-3.1-8B
sequence_len: 2048

# Sample Packing
sample_packing: true
flash_attention: true

# DDPé€šè¿‡launcherè‡ªåŠ¨å¯ç”¨
# torchrun --nproc_per_node=8 ...

micro_batch_size: 4
gradient_accumulation_steps: 4
# æœ‰æ•ˆbatch: 4 Ã— 8 (GPUs) Ã— 4 = 128
```

### åœºæ™¯ 4ï¼šFSDP + Sample Packing

```yaml
base_model: meta-llama/Llama-3.1-13B
sequence_len: 2048

# FSDP-2
fsdp_version: 2
fsdp_config:
  reshard_after_forward: true

# Sample Packing
sample_packing: true
sample_packing_eff_est: 0.95
flash_attention: true

micro_batch_size: 2
gradient_accumulation_steps: 8
```

### åœºæ™¯ 5ï¼šTP + DP + Sample Packing

```yaml
base_model: meta-llama/Llama-3.1-70B
sequence_len: 2048

# TP=2, DP=4 (8 GPUs total)
tensor_parallel_size: 2

# Sample Packing
sample_packing: true
sample_packing_eff_est: 0.95
flash_attention: true

micro_batch_size: 2  # æ¯DP rank
gradient_accumulation_steps: 4

# é¢„æœŸæ”¶ç›Š: 2-3å€åŠ é€Ÿ
```

### åœºæ™¯ 6ï¼šCurriculum Learning (é¡ºåºé‡è¦)

```yaml
base_model: meta-llama/Llama-3.1-8B
sequence_len: 2048

# Sample Packing with sequential mode
sample_packing: true
sample_packing_sequentially: true  # â† ä¿æŒæ•°æ®é¡ºåº

flash_attention: true
pad_to_sequence_len: false

# æ³¨æ„: æ•ˆç‡ç•¥ä½äºå¹¶è¡Œæ¨¡å¼ï¼Œä½†ä¿æŒé¡ºåº
```

---

## ğŸ”§ å‚æ•°è¯¦è§£

### æ ¸å¿ƒå‚æ•°

```yaml
# sample_packing (bool)
sample_packing: true
# æ˜¯å¦å¯ç”¨Sample Packing
# é»˜è®¤: false
# æ¨è: true (é™¤éåºåˆ—é•¿åº¦éå¸¸å‡åŒ€)

# sample_packing_eff_est (float, 0.0-1.0)
sample_packing_eff_est: 0.95
# é¢„ä¼°çš„æ‰“åŒ…æ•ˆç‡ï¼Œç”¨äºè°ƒæ•´batch size
# é»˜è®¤: 1.0 (ä¸è°ƒæ•´)
# æ¨è: 0.90-0.95 (æ ¹æ®å®é™…æ—¥å¿—è°ƒæ•´)
# ä½œç”¨: é¿å…OOM (å®é™…tokensæ¯”é¢„æœŸå¤š)

# sample_packing_sequentially (bool)
sample_packing_sequentially: false
# false: å¹¶è¡ŒFFDï¼Œæœ€é«˜æ•ˆ
# true: é¡ºåºpackingï¼Œä¿æŒæ•°æ®é¡ºåº
# é»˜è®¤: false
# æ¨è: false (é™¤ééœ€è¦ä¿æŒé¡ºåº)

# sample_packing_bin_size (int)
sample_packing_bin_size: 200
# æ¯ä¸ªbinæœ€å¤šå®¹çº³çš„åºåˆ—æ•°
# é»˜è®¤: 200
# æ¨è: ä¿æŒé»˜è®¤ (é™¤éç‰¹æ®Šéœ€æ±‚)

# sample_packing_group_size (int)
sample_packing_group_size: 100000
# FFDåˆ†ç»„å¤§å° (å¹¶è¡Œå¤„ç†)
# é»˜è®¤: 100000
# æ¨è: ä¿æŒé»˜è®¤

# eval_sample_packing (bool)
eval_sample_packing: true
# Evaluationæ—¶æ˜¯å¦å¯ç”¨packing
# é»˜è®¤: false
# æ¨è: true (å¦‚æœevalæ•°æ®ä¹Ÿé•¿åº¦ä¸å‡)
```

### å…³è”å‚æ•°

```yaml
# flash_attention (bool)
flash_attention: true
# âš ï¸ Sample Packingå‡ ä¹å¿…é¡»é…åˆFlash Attention
# æ¨è: å¿…é¡»å¯ç”¨

# pad_to_sequence_len (bool)
pad_to_sequence_len: false
# âš ï¸ Sample Packingæ—¶å¿…é¡»è®¾ä¸ºfalse
# trueä¼šç ´åpackingæ•ˆæœ

# micro_batch_size (int)
micro_batch_size: 4
# æ¯ä¸ªGPUçš„batch size (binsæ•°é‡)
# Sample Packingæ—¶å¯èƒ½éœ€è¦è°ƒå°
# (å› ä¸ºå®é™…tokensæ›´å¤š)
```

---

## ğŸ› é—®é¢˜æ’æŸ¥

### é—®é¢˜ 1ï¼šOOM (Out of Memory)

```
é”™è¯¯: CUDA out of memory
```

**åŸå› **: Sample Packingæé«˜äº†tokenåˆ©ç”¨ç‡ï¼Œå®é™…è®¡ç®—é‡å¢åŠ 

**è§£å†³æ–¹æ¡ˆ**:

```yaml
# æ–¹æ¡ˆ1: å‡å°micro_batch_size
micro_batch_size: 2  # ä»4é™åˆ°2

# æ–¹æ¡ˆ2: é™ä½æ•ˆç‡ä¼°è®¡
sample_packing_eff_est: 0.8  # ä¿å®ˆä¼°è®¡

# æ–¹æ¡ˆ3: å¯ç”¨gradient checkpointing
gradient_checkpointing: true

# æ–¹æ¡ˆ4: å‡å°sequence_len
sequence_len: 1024  # ä»2048é™åˆ°1024
```

---

### é—®é¢˜ 2ï¼šæ‰“åŒ…æ•ˆç‡ä½

```
[INFO] Sample packing efficiency: 0.65
```

**åŸå› **: åºåˆ—é•¿åº¦åˆ†å¸ƒä¸å‡æˆ–é…ç½®ä¸å½“

**è¯Šæ–­**:

```python
# æ£€æŸ¥æ•°æ®é›†åºåˆ—é•¿åº¦åˆ†å¸ƒ
from datasets import load_dataset
import matplotlib.pyplot as plt

ds = load_dataset('your_dataset')
lengths = [len(x['input_ids']) for x in ds['train']]

plt.hist(lengths, bins=50)
plt.xlabel('Sequence Length')
plt.ylabel('Count')
plt.title('Sequence Length Distribution')
plt.show()

# è®¡ç®—ç»Ÿè®¡
import numpy as np
print(f"Mean: {np.mean(lengths):.0f}")
print(f"Std: {np.std(lengths):.0f}")
print(f"Min: {np.min(lengths)}")
print(f"Max: {np.max(lengths)}")
print(f"Median: {np.median(lengths):.0f}")
```

**è§£å†³æ–¹æ¡ˆ**:

```yaml
# æ–¹æ¡ˆ1: ä½¿ç”¨å¹¶è¡ŒFFD (æ›´é«˜æ•ˆ)
sample_packing_sequentially: false

# æ–¹æ¡ˆ2: å¢å¤§bin_size
sample_packing_bin_size: 500

# æ–¹æ¡ˆ3: è°ƒæ•´sequence_len
# å¦‚æœå¤§éƒ¨åˆ†åºåˆ—<1000ï¼Œè®¾ç½®sequence_len=1024æ›´åˆé€‚
sequence_len: 1024  # ä»2048é™ä½

# æ–¹æ¡ˆ4: è¿‡æ»¤è¶…é•¿/è¶…çŸ­åºåˆ—
# åœ¨æ•°æ®é¢„å¤„ç†æ—¶è¿‡æ»¤å¼‚å¸¸å€¼
```

---

### é—®é¢˜ 3ï¼šè®­ç»ƒä¸ç¨³å®š

```
Losså‡ºç°NaNæˆ–éœ‡è¡
```

**åŸå› **: batchå†…tokenæ•°é‡æ³¢åŠ¨å¤§

**è§£å†³æ–¹æ¡ˆ**:

```yaml
# æ–¹æ¡ˆ1: ä½¿ç”¨multipack_real_batches
multipack_real_batches: false
# false: æ¯binç®—ä¸€ä¸ªsample (é»˜è®¤ï¼Œæ¨è)
# true: æ¯sequenceç®—ä¸€ä¸ªsample (æ›´ç¨³å®šä½†æ…¢)

# æ–¹æ¡ˆ2: é™ä½å­¦ä¹ ç‡
learning_rate: 1e-5  # ä»2e-5é™ä½

# æ–¹æ¡ˆ3: å¢åŠ warmup
warmup_steps: 500  # ä»100å¢åŠ 

# æ–¹æ¡ˆ4: ä½¿ç”¨æ›´ä¿å®ˆçš„æ‰“åŒ…
sample_packing_eff_est: 0.85
```

---

### é—®é¢˜ 4ï¼šEvalæ—¶OOM

```
Trainæ­£å¸¸ï¼ŒEvalæ—¶OOM
```

**è§£å†³æ–¹æ¡ˆ**:

```yaml
# æ–¹æ¡ˆ1: å…³é—­eval packing
eval_sample_packing: false

# æ–¹æ¡ˆ2: å‡å°eval batch size
eval_batch_size: 2  # ç‹¬ç«‹äºmicro_batch_size

# æ–¹æ¡ˆ3: å‡å°evalæ•°æ®é›†
# åªevaléƒ¨åˆ†æ•°æ®
```

---

### é—®é¢˜ 5ï¼šæ¨¡å‹ä¸æ”¯æŒ

```
ValueError: Model xxx does not support sample packing
```

**æ£€æŸ¥æ”¯æŒåˆ—è¡¨**:

```python
from axolotl.monkeypatch.multipack import SUPPORTED_MULTIPACK_MODEL_TYPES

print(SUPPORTED_MULTIPACK_MODEL_TYPES)
# ['llama', 'mistral', 'mixtral', 'qwen2', 'gemma', ...]
```

**è§£å†³æ–¹æ¡ˆ**:

```yaml
# å¦‚æœæ¨¡å‹ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œå°è¯•ä½¿ç”¨V2 collator
# (V2æ›´å¹¿æ³›å…¼å®¹)

# æˆ–è€…ä¸å¯ç”¨sample_packing
sample_packing: false
```

---

### é—®é¢˜ 6ï¼šæ•ˆç‡æ²¡æå‡

```
å¯ç”¨Sample Packingåthroughputæ²¡å˜åŒ–
```

**æ£€æŸ¥æ¸…å•**:

```bash
# 1. ç¡®è®¤Sample Packingç”Ÿæ•ˆ
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼Œåº”è¯¥çœ‹åˆ°:
[INFO] Sample packing efficiency: 0.XXX
[INFO] MultipackBatchSampler: using parallel packing

# 2. ç¡®è®¤Flash Attentionå¯ç”¨
# æ—¥å¿—ä¸­åº”è¯¥æœ‰:
[INFO] Using Flash Attention 2

# 3. æ£€æŸ¥pad_to_sequence_len
# å¿…é¡»æ˜¯false

# 4. æ£€æŸ¥åºåˆ—é•¿åº¦åˆ†å¸ƒ
# å¦‚æœ90%åºåˆ—éƒ½æ¥è¿‘sequence_lenï¼Œæ”¶ç›Šæœ‰é™

# 5. ç›‘æ§GPUåˆ©ç”¨ç‡
nvidia-smi dmon -s u
# åº”è¯¥ä»40-50% â†’ 80-90%+
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### âœ… æ¨èåšæ³•

```yaml
# 1. å¯ç”¨Sample Packing (æ–°é¡¹ç›®)
sample_packing: true

# 2. é…åˆFlash Attention
flash_attention: true

# 3. å…³é—­å›ºå®špadding
pad_to_sequence_len: false

# 4. ä½¿ç”¨å¹¶è¡ŒFFD
sample_packing_sequentially: false

# 5. è®¾ç½®åˆç†çš„æ•ˆç‡ä¼°è®¡
sample_packing_eff_est: 0.95  # æ ¹æ®æ—¥å¿—è°ƒæ•´

# 6. Evalä¹Ÿå¯ç”¨packing (å¦‚æœæ•°æ®é•¿åº¦ä¸å‡)
eval_sample_packing: true

# 7. å¯ç”¨gradient checkpointingèŠ‚çœå†…å­˜
gradient_checkpointing: true

# 8. ä½¿ç”¨bf16æˆ–fp16
bf16: true
```

### âŒ é¿å…

```yaml
# 1. Sample Packing + pad_to_sequence_len
sample_packing: true
pad_to_sequence_len: true  # âŒ å†²çªï¼ä¼šç ´åpacking

# 2. Sample Packing without Flash Attention
sample_packing: true
flash_attention: false  # âŒ æ•ˆç‡ä½ï¼Œä¸æ¨è

# 3. è¿‡å¤§çš„micro_batch_size
sample_packing: true
micro_batch_size: 16  # âŒ å®¹æ˜“OOM

# 4. å¿˜è®°è®¾ç½®æ•ˆç‡ä¼°è®¡
sample_packing: true
# sample_packing_eff_est: 0.95  â† å¿˜è®°è®¾ç½®ï¼Œå¯èƒ½OOM

# 5. åºåˆ—é•¿åº¦å‡åŒ€æ—¶å¼ºè¡Œä½¿ç”¨
# 90%åºåˆ—é•¿åº¦åœ¨1900-2048ä¹‹é—´
sample_packing: true  # âŒ æ”¶ç›Š<5%ï¼Œä¸å€¼å¾—
```

---

## ğŸ“ˆ æ€§èƒ½å‚è€ƒ

### Llama-8B, 8Ã—A100 40GB, DDP

| é…ç½® | Throughput (tokens/s/GPU) | è®­ç»ƒ1B tokensæ—¶é—´ | GPUåˆ©ç”¨ç‡ |
|------|--------------------------|------------------|----------|
| æ— Packing | ~1800 | ~15.4h | 45% |
| + Sample Packing | ~4500 | ~6.2h | 85% |
| **æå‡** | **+2.5x** | **-60%** | **+89%** |

### Llama-13B, 8Ã—A100 80GB, DDP

| é…ç½® | Throughput (tokens/s/GPU) | è®­ç»ƒ1B tokensæ—¶é—´ | GPUåˆ©ç”¨ç‡ |
|------|--------------------------|------------------|----------|
| æ— Packing | ~1200 | ~23.1h | 42% |
| + Sample Packing | ~3000 | ~9.3h | 82% |
| **æå‡** | **+2.5x** | **-60%** | **+95%** |

### Llama-70B, 8Ã—A100 80GB, TP=2, DP=4

| é…ç½® | Throughput (tokens/s/GPU) | è®­ç»ƒ1B tokensæ—¶é—´ |
|------|--------------------------|------------------|
| æ— Packing | ~600 | ~46h |
| + Sample Packing | ~1500 | ~18.5h |
| **æå‡** | **+2.5x** | **-60%** |

---

## ğŸ› ï¸ è°ƒè¯•å‘½ä»¤

### æ£€æŸ¥Sample Packingæ˜¯å¦ç”Ÿæ•ˆ

```bash
# æ–¹æ³•1: æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
# åº”è¯¥çœ‹åˆ°:
[INFO] Sample packing efficiency: 0.XXX
[INFO] MultipackBatchSampler: using parallel packing
```

```python
# æ–¹æ³•2: åœ¨callbackä¸­æ£€æŸ¥batch shape
from transformers import TrainerCallback

class DebugCallback(TrainerCallback):
    def on_step_begin(self, args, state, control, **kwargs):
        if state.global_step % 10 == 0:
            batch = kwargs.get('inputs', {})
            print(f"Step {state.global_step}:")
            print(f"  input_ids shape: {batch['input_ids'].shape}")
            print(f"  attention_mask unique: {batch['attention_mask'].unique()}")
            # Sample Packing enabledåº”è¯¥çœ‹åˆ°:
            # attention_mask unique: tensor([0, 1, 2, 3, ...])
            #                                â†‘ åºåˆ—IDs
```

### æ€§èƒ½åˆ†æ

```bash
# å¯¹æ¯” Sample Packing vs æ— Sample Packing

# æµ‹è¯•1: æ— Sample Packing
sample_packing: false
axolotl train config.yaml --max-steps 100
# è®°å½•: Throughput, GPUåˆ©ç”¨ç‡, å†…å­˜ä½¿ç”¨

# æµ‹è¯•2: Sample Packing
sample_packing: true
axolotl train config.yaml --max-steps 100
# å¯¹æ¯”æŒ‡æ ‡
```

### GPUç›‘æ§

```bash
# å®æ—¶ç›‘æ§GPUåˆ©ç”¨ç‡
watch -n 1 nvidia-smi

# æˆ–ä½¿ç”¨dmon
nvidia-smi dmon -s u -d 1

# Sample Packing enabledå:
# GPU-Utilåº”è¯¥ä»40-50% â†’ 80-90%+
```

---

## ğŸ’¬ å¿«é€Ÿ FAQ

**Q: Sample Packingä¼šå½±å“æ”¶æ•›å—ï¼Ÿ**
A: âœ… ä¸ä¼šã€‚Attention maskç¡®ä¿åºåˆ—éš”ç¦»ï¼Œæ¢¯åº¦è®¡ç®—ç­‰ä»·äºépackingã€‚

**Q: æ‰€æœ‰æ¨¡å‹éƒ½æ”¯æŒå—ï¼Ÿ**
A: âš ï¸ å¤§éƒ¨åˆ†ä¸»æµæ¨¡å‹æ”¯æŒã€‚æ£€æŸ¥ `SUPPORTED_MULTIPACK_MODEL_TYPES`ã€‚

**Q: å¿…é¡»é…åˆFlash Attentionå—ï¼Ÿ**
A: âœ… å¼ºçƒˆæ¨èã€‚è™½ç„¶V2 collatoræ”¯æŒéFlash Attentionï¼Œä½†æ•ˆç‡ä¼šé™ä½ã€‚

**Q: DeepSpeedå…¼å®¹å—ï¼Ÿ**
A: âœ… å…¼å®¹ã€‚Axolotlä¼šè‡ªåŠ¨å¤„ç†DeepSpeedé…ç½®ã€‚

**Q: å¯ä»¥å’ŒFSDP/TP/CPä¸€èµ·ç”¨å—ï¼Ÿ**
A: âœ… DDP/FSDP/TPå®Œç¾å…¼å®¹ã€‚âš ï¸ CPéœ€è¦æ³¨æ„ï¼Œæ¨èåˆ†å¼€ä½¿ç”¨ã€‚

**Q: å¦‚ä½•ä¼°ç®—æ‰“åŒ…æ•ˆç‡ï¼Ÿ**
A: å…ˆä¸è®¾ç½® `sample_packing_eff_est`ï¼ŒæŸ¥çœ‹æ—¥å¿—ä¸­çš„å®é™…æ•ˆç‡ï¼Œç„¶åè®¾ç½®è¯¥å€¼ã€‚

**Q: Evalå¿…é¡»å¯ç”¨packingå—ï¼Ÿ**
A: âŒ å¯é€‰ã€‚å¦‚æœevalæ•°æ®é•¿åº¦ä¹Ÿä¸å‡ï¼Œå»ºè®®å¯ç”¨ï¼›å¦åˆ™å¯å…³é—­ã€‚

**Q: ä¼šå¢åŠ è®­ç»ƒæ—¶é—´å—ï¼Ÿ**
A: âŒ ç›¸åï¼Œä¼šå‡å°‘50-60%è®­ç»ƒæ—¶é—´ï¼

---

## ğŸ”¢ é…ç½®ç¤ºä¾‹é€ŸæŸ¥

### æœ€ç®€é…ç½® (1åˆ†é’Ÿä¸Šæ‰‹)

```yaml
base_model: meta-llama/Llama-3.1-8B
sequence_len: 2048
sample_packing: true
flash_attention: true
pad_to_sequence_len: false
micro_batch_size: 4
```

### ç”Ÿäº§ç¯å¢ƒé…ç½®

```yaml
base_model: meta-llama/Llama-3.1-13B
sequence_len: 2048

# Sample Packing
sample_packing: true
sample_packing_eff_est: 0.95
sample_packing_sequentially: false
eval_sample_packing: true

# æ€§èƒ½ä¼˜åŒ–
flash_attention: true
gradient_checkpointing: true
bf16: true
pad_to_sequence_len: false

# FSDP-2
fsdp_version: 2
fsdp_config:
  reshard_after_forward: true

# è®­ç»ƒå‚æ•°
micro_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2e-5
lr_scheduler: cosine
warmup_steps: 100

# Logging
logging_steps: 10
eval_steps: 500
save_steps: 1000
```

### å¤§æ¨¡å‹é…ç½® (70B+)

```yaml
base_model: meta-llama/Llama-3.1-70B
sequence_len: 2048

# TP + DP
tensor_parallel_size: 2  # 8 GPUs â†’ 4 TP groups

# Sample Packing
sample_packing: true
sample_packing_eff_est: 0.95
flash_attention: true
pad_to_sequence_len: false

# å†…å­˜ä¼˜åŒ–
gradient_checkpointing: true
bf16: true

# è®­ç»ƒå‚æ•°
micro_batch_size: 1  # TPåå†…å­˜ç´§å¼ 
gradient_accumulation_steps: 16
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [è¯¦ç»†è§£æ](./sample_packing_deep_dive.md)
- [æºç è§£æ](./sample_packing_source_walkthrough.md)
- [Data Parallelism](./data_parallelism_deep_dive.md)
- [Tensor Parallelism](./tensor_parallelism_deep_dive.md)
- [Context Parallelism](./context_parallelism_deep_dive.md)
- [FSDP Versions](./fsdp_versions_comparison.md)
- [ä¸»ç´¢å¼•](./README.md)

---

## ğŸ’¡ é€Ÿè®°å£è¯€

```
Sample Packing å¥½å¤„å¤šï¼Œ
å‡å°‘ padding æ•ˆç‡é«˜ã€‚
åºåˆ—æ‰“åŒ…åƒæ‹¼å›¾ï¼Œ
GPU åˆ©ç”¨ç‡é£™å‡å•¦ã€‚

Flash Attention å¿…é¡»é…ï¼Œ
sequence_len åˆ«å›ºå®šã€‚
æ•ˆç‡ä¼°è®¡è¦åˆç†ï¼Œ
OOM é—®é¢˜ä¸ç”¨æ€•ã€‚

é¢„è®­ç»ƒæ”¶ç›Šæœ€æ˜¾è‘—ï¼Œ
å¾®è°ƒä¹Ÿèƒ½å¿«ä¸¤å€ã€‚
DDP FSDP éƒ½å…¼å®¹ï¼Œ
ç”Ÿäº§ç¯å¢ƒæ”¾å¿ƒç”¨ï¼
```

---

*æ‰“å°æ­¤é¡µä½œä¸ºé€ŸæŸ¥æ‰‹å†Œ | æœ€åæ›´æ–°ï¼š2025-11*

# Sample Packing æ·±åº¦è§£æ ğŸ“¦

> **æ ¸å¿ƒæ€æƒ³**ï¼šæŠŠä¸åŒé•¿åº¦çš„åºåˆ—åƒä¿„ç½—æ–¯æ–¹å—ä¸€æ ·ç´§å¯†æ‰“åŒ…è¿› GPU å†…å­˜ï¼Œæœ€å¤§åŒ–åˆ©ç”¨ç‡

---

## ç›®å½•

- [1. ä»€ä¹ˆæ˜¯ Sample Packingï¼Ÿ](#1-ä»€ä¹ˆæ˜¯-sample-packing)
- [2. ä¸ºä»€ä¹ˆéœ€è¦ Sample Packingï¼Ÿ](#2-ä¸ºä»€ä¹ˆéœ€è¦-sample-packing)
- [3. Sample Packing å·¥ä½œåŸç†](#3-sample-packing-å·¥ä½œåŸç†)
- [4. Sample Packing ä¸å„ç§å¹¶è¡Œç­–ç•¥çš„ç»“åˆ](#4-sample-packing-ä¸å„ç§å¹¶è¡Œç­–ç•¥çš„ç»“åˆ)
- [5. Sample Packing vs é Sample Packing](#5-sample-packing-vs-é-sample-packing)
- [6. å®ç°ç»†èŠ‚ä¸æºç è§£æ](#6-å®ç°ç»†èŠ‚ä¸æºç è§£æ)
- [7. é…ç½®ç¤ºä¾‹](#7-é…ç½®ç¤ºä¾‹)
- [8. æœ€ä½³å®è·µ](#8-æœ€ä½³å®è·µ)

---

## 1. ä»€ä¹ˆæ˜¯ Sample Packingï¼Ÿ

### 1.1 åŸºæœ¬æ¦‚å¿µ

**Sample Packingï¼ˆæ ·æœ¬æ‰“åŒ…ï¼‰** æ˜¯ä¸€ç§è®­ç»ƒä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡å°†å¤šä¸ªä¸åŒé•¿åº¦çš„åºåˆ—æ‰“åŒ…åˆ°åŒä¸€ä¸ª batch ä¸­ï¼Œå‡å°‘ padding æµªè´¹ï¼Œæé«˜ GPU åˆ©ç”¨ç‡ã€‚

### 1.2 æ¬æ¡Œå­æ¯”å–» ğŸª‘

ç»§ç»­ä½¿ç”¨æˆ‘ä»¬çš„"æ¬æ¡Œå­"æ¯”å–»ç³»ç»Ÿï¼š

```
ä¼ ç»Ÿè®­ç»ƒï¼ˆæ—  Sample Packingï¼‰ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åºåˆ—1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ â† 8ä¸ªtoken + 18ä¸ªpadding
â”‚ åºåˆ—2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ â† 6ä¸ªtoken + 20ä¸ªpadding
â”‚ åºåˆ—3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ â† 11ä¸ªtoken + 15ä¸ªpadding
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
æ€»å®¹é‡ï¼š78ä¸ªslotï¼Œå®é™…ä½¿ç”¨ï¼š25ä¸ªtoken
åˆ©ç”¨ç‡ï¼š25/78 = 32% âŒ

Sample Packingï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bin1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ â† åºåˆ—1+åºåˆ—2+åºåˆ—3 = 25ä¸ªtoken
â”‚ Bin2: (empty)                    â”‚
â”‚ Bin3: (empty)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
æ€»å®¹é‡ï¼š26ä¸ªslotï¼ˆåªéœ€1ä¸ªbinï¼‰ï¼Œå®é™…ä½¿ç”¨ï¼š25ä¸ªtoken
åˆ©ç”¨ç‡ï¼š25/26 = 96% âœ…
```

**æ ¸å¿ƒæ€æƒ³**ï¼š
- **ä¼ ç»Ÿæ–¹å¼**ï¼šæ¯ä¸ªåºåˆ—ç‹¬å ä¸€ä¸ª"å¡è½¦"ï¼ŒçŸ­åºåˆ—æµªè´¹ç©ºé—´
- **Sample Packing**ï¼šå¤šä¸ªåºåˆ—å…±äº«åŒä¸€ä¸ª"å¡è½¦"ï¼Œåƒä¿„ç½—æ–¯æ–¹å—ä¸€æ ·ç´§å¯†æ’åˆ—
- **ç›®æ ‡**ï¼šæœ€å¤§åŒ–æ¯ä¸ª batch çš„ token åˆ©ç”¨ç‡

---

## 2. ä¸ºä»€ä¹ˆéœ€è¦ Sample Packingï¼Ÿ

### 2.1 Padding æµªè´¹é—®é¢˜

åœ¨ LLM è®­ç»ƒä¸­ï¼Œåºåˆ—é•¿åº¦å·®å¼‚å¾ˆå¤§ï¼š

```python
# å…¸å‹æ•°æ®é›†çš„åºåˆ—é•¿åº¦åˆ†å¸ƒ
åºåˆ—1: "Hello"                          â†’ 5 tokens
åºåˆ—2: "How are you?"                   â†’ 10 tokens
åºåˆ—3: "Please explain quantum physics" â†’ 100 tokens
åºåˆ—4: "A very long article..."         â†’ 2048 tokens

# å¦‚æœ batch_size=4, sequence_len=2048
# ä¼ ç»Ÿæ–¹å¼ï¼šæ‰€æœ‰åºåˆ—éƒ½ pad åˆ° 2048
æ€»token slots = 4 Ã— 2048 = 8192
å®é™…tokens = 5 + 10 + 100 + 2048 = 2163
åˆ©ç”¨ç‡ = 2163 / 8192 = 26.4% âŒ
```

**é—®é¢˜**ï¼š
- âŒ **GPU å†…å­˜æµªè´¹**ï¼š74% çš„ GPU ç®—åŠ›åœ¨å¤„ç†æ— æ„ä¹‰çš„ padding
- âŒ **è®­ç»ƒé€Ÿåº¦æ…¢**ï¼šè®¡ç®—é‡åŒ…å«å¤§é‡æ— æ•ˆæ“ä½œ
- âŒ **æˆæœ¬é«˜**ï¼šåŒæ ·çš„è®­ç»ƒç›®æ ‡éœ€è¦æ›´å¤š GPU æ—¶é—´

### 2.2 Sample Packing çš„æ”¶ç›Š

```python
# ä½¿ç”¨ Sample Packing
# å°†å¤šä¸ªçŸ­åºåˆ—æ‰“åŒ…åˆ°ä¸€ä¸ª bin ä¸­
Bin 1: [seq1(5), seq2(10), seq3(100), ...] â†’ å¡«æ»¡ 2048 tokens
Bin 2: [seq4(2048)]                        â†’ å¡«æ»¡ 2048 tokens
Bin 3: [seq5(512), seq6(800), seq7(736)]   â†’ å¡«æ»¡ 2048 tokens

æ€»token slots â‰ˆ å®é™…tokens
åˆ©ç”¨ç‡ â‰ˆ 95%+ âœ…
```

**æ”¶ç›Š**ï¼š
- âœ… **å‡å°‘ padding**ï¼šä» 70-80% æµªè´¹é™è‡³ 5-10%
- âœ… **è®­ç»ƒåŠ é€Ÿ**ï¼šåŒæ ·ç¡¬ä»¶ä¸‹ throughput æå‡ 2-3x
- âœ… **æˆæœ¬é™ä½**ï¼šè®­ç»ƒç›¸åŒæ­¥æ•°æ‰€éœ€æ—¶é—´å‡å°‘ 50%+

---

## 3. Sample Packing å·¥ä½œåŸç†

### 3.1 æ ¸å¿ƒç»„ä»¶

Axolotl çš„ Sample Packing å®ç°åŒ…å«ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Sample Packing æµç¨‹                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  1. æ•°æ®å‡†å¤‡                                                  â”‚
â”‚     â”œâ”€ è®¡ç®—æ¯ä¸ªåºåˆ—çš„é•¿åº¦                                     â”‚
â”‚     â””â”€ æŒ‰é•¿åº¦æ’åºï¼ˆå¯é€‰ï¼‰                                     â”‚
â”‚                                                               â”‚
â”‚  2. Bin Packing (MultipackBatchSampler)                      â”‚
â”‚     â”œâ”€ FFDç®—æ³•ï¼šFirst-Fit Decreasing                         â”‚
â”‚     â”‚  â””â”€ å°†åºåˆ—æ‰“åŒ…è¿›å›ºå®šå®¹é‡çš„bins                          â”‚
â”‚     â”œâ”€ Sequentialæ¨¡å¼ï¼šä¿æŒåŸå§‹é¡ºåº                           â”‚
â”‚     â””â”€ Parallelæ¨¡å¼ï¼šå¤šè¿›ç¨‹åŠ é€Ÿ                               â”‚
â”‚                                                               â”‚
â”‚  3. æ•°æ®æ•´ç† (DataCollator)                                   â”‚
â”‚     â”œâ”€ BatchSamplerDataCollatorForSeq2Seq (V1)               â”‚
â”‚     â”‚  â””â”€ è¿æ¥åŒä¸€binå†…çš„åºåˆ—                                 â”‚
â”‚     â”œâ”€ V2BatchSamplerDataCollatorForSeq2Seq (V2)             â”‚
â”‚     â”‚  â””â”€ æ›´æ™ºèƒ½çš„attention_maskå¤„ç†                          â”‚
â”‚     â””â”€ ç”Ÿæˆposition_ids, attention_mask                      â”‚
â”‚                                                               â”‚
â”‚  4. Attentionå¤„ç†                                             â”‚
â”‚     â”œâ”€ get_unpad_data(): æå–æœ‰æ•ˆtokenä½ç½®                    â”‚
â”‚     â”œâ”€ get_cu_seqlens(): è®¡ç®—ç´¯ç§¯åºåˆ—é•¿åº¦                     â”‚
â”‚     â””â”€ Flash Attention / Xformersä¼˜åŒ–                        â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 FFD ç®—æ³•è¯¦è§£

**FFD (First-Fit Decreasing)** æ˜¯ç»å…¸çš„ bin packing ç®—æ³•ï¼š

```python
# ç®—æ³•ä¼ªä»£ç 
def first_fit_decreasing(sequences, bin_capacity):
    # 1. æŒ‰é•¿åº¦é™åºæ’åˆ—
    sorted_seqs = sort_by_length_desc(sequences)

    bins = []

    # 2. éå†æ¯ä¸ªåºåˆ—
    for seq in sorted_seqs:
        # 3. å°è¯•æ”¾å…¥ç°æœ‰bin
        placed = False
        for bin in bins:
            if bin.remaining_capacity >= len(seq):
                bin.add(seq)
                placed = True
                break

        # 4. æ”¾ä¸ä¸‹å°±åˆ›å»ºæ–°bin
        if not placed:
            new_bin = Bin(capacity=bin_capacity)
            new_bin.add(seq)
            bins.append(new_bin)

    return bins
```

**å·¥ä½œæµç¨‹å›¾**ï¼š

```
åºåˆ—é•¿åº¦: [1024, 512, 800, 256, 512, 128]
Binå®¹é‡: 2048

æ­¥éª¤1: æ’åº
[1024, 800, 512, 512, 256, 128]

æ­¥éª¤2: æ‰“åŒ…
Bin 1: [1024] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”œâ†’ å¯ä»¥æ”¾ 800 (1024+800=1824 < 2048) âœ…
       [1024, 800] â”€â”€â”€â”€â”¤
                       â””â†’ ä¸èƒ½æ”¾ 512 (1824+512=2336 > 2048) âŒ

Bin 2: [512, 512] â”€â”€â”€â”€â”€â”
                       â”œâ†’ å¯ä»¥æ”¾ 256 (1024+256=1280 < 2048) âœ…
       [512, 512, 256]â”€â”¤
                       â”œâ†’ å¯ä»¥æ”¾ 128 (1280+128=1408 < 2048) âœ…
       [512, 512, 256, 128]
                       â””â†’ å®Œæˆ

æœ€ç»ˆç»“æœ:
Bin 1: [1024, 800]           â†’ 1824/2048 = 89.1%
Bin 2: [512, 512, 256, 128]  â†’ 1408/2048 = 68.8%
å¹³å‡åˆ©ç”¨ç‡: 79.0%
```

### 3.3 Sequential vs Parallel Packing

Axolotl æ”¯æŒä¸¤ç§æ‰“åŒ…æ¨¡å¼ï¼š

#### Sequential Packingï¼ˆé¡ºåºæ‰“åŒ…ï¼‰

```python
# æºç ä½ç½®: src/axolotl/utils/samplers/multipack.py:194-241
@numba.njit
def allocate_sequentially(sequence_lengths, rank, bin_capacity, num_ranks):
    """æŒ‰åŸå§‹é¡ºåºæ‰“åŒ…ï¼Œä¿æŒæ•°æ®é¡ºåº"""
    # ä¸æ’åºï¼ŒæŒ‰åŸå§‹é¡ºåºéå†
    # æ¯ä¸ªåºåˆ—ä¾æ¬¡æ”¾å…¥èƒ½å®¹çº³å®ƒçš„ç¬¬ä¸€ä¸ªbin
```

**ç‰¹ç‚¹**ï¼š
- âœ… ä¿æŒåŸå§‹æ•°æ®é¡ºåºï¼ˆå¯¹æŸäº›è®­ç»ƒåœºæ™¯å¾ˆé‡è¦ï¼‰
- âŒ æ‰“åŒ…æ•ˆç‡è¾ƒä½ï¼ˆå› ä¸ºä¸æ’åºï¼‰
- é€‚ç”¨åœºæ™¯ï¼šcurriculum learningã€ordered datasets

#### Parallel Packingï¼ˆå¹¶è¡Œæ‰“åŒ…ï¼‰

```python
# æºç ä½ç½®: src/axolotl/utils/samplers/multipack.py:125-190
def pack_parallel(sequence_lengths, bin_capacity, group_size, bin_size, num_processes=None):
    """ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œæ‰“åŒ…ï¼Œæœ€å¤§åŒ–æ•ˆç‡"""
    # 1. æŒ‰é•¿åº¦æ’åº
    # 2. åˆ†ç»„å¤„ç†
    # 3. å¤šè¿›ç¨‹å¹¶è¡ŒFFD
    # 4. åˆå¹¶ç»“æœ
```

**ç‰¹ç‚¹**ï¼š
- âœ… æ‰“åŒ…æ•ˆç‡é«˜ï¼ˆFFD + æ’åºï¼‰
- âœ… é€Ÿåº¦å¿«ï¼ˆå¤šè¿›ç¨‹å¹¶è¡Œï¼‰
- âŒ ä¸ä¿æŒåŸå§‹é¡ºåº
- é€‚ç”¨åœºæ™¯ï¼šå¤§è§„æ¨¡è®­ç»ƒã€è¿½æ±‚æœ€é«˜æ•ˆç‡

**å¯¹æ¯”**ï¼š

```
Sequential Packing:
æ•°æ®: [A(100), B(2000), C(200), D(1800)]
Binå®¹é‡: 2048

Bin 1: [A(100), B(2000)]  â† Bå¤ªå¤§ï¼Œåªèƒ½å•ç‹¬
Bin 2: [C(200), D(1800)]
åˆ©ç”¨ç‡: (2100 + 2000) / (2Ã—2048) = 100% (ç¢°å·§å¾ˆå¥½)

Parallel Packing:
æ•°æ®æ’åº: [B(2000), D(1800), C(200), A(100)]

Bin 1: [B(2000)]          â† æ”¾ä¸ä¸‹D
Bin 2: [D(1800), C(200)]  â† æ­£å¥½å¡«æ»¡
Bin 3: [A(100)]           â† å‰©ä½™
åˆ©ç”¨ç‡: (2000 + 2000 + 100) / (3Ã—2048) = 66.7% (åè€Œæ›´å·®)

ä½†åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šï¼ŒParallelé€šå¸¸æ•ˆç‡æ›´é«˜ï¼
```

### 3.4 Attention Mask å¤„ç†

Sample Packing æœ€å¤æ‚çš„éƒ¨åˆ†æ˜¯å¤„ç† attention maskï¼Œç¡®ä¿ä¸åŒåºåˆ—ä¹‹é—´ä¸äº’ç›¸ attendã€‚

#### é—®é¢˜ç¤ºä¾‹

```
æœªæ‰“åŒ…:
åºåˆ—A: [token1, token2, token3]
Attention Mask (æ ‡å‡†causal):
    1  2  3
1 [ 1  0  0 ]  â† token1åªèƒ½çœ‹è‡ªå·±
2 [ 1  1  0 ]  â† token2èƒ½çœ‹token1,2
3 [ 1  1  1 ]  â† token3èƒ½çœ‹token1,2,3

æ‰“åŒ…å:
Bin: [token1, token2, token3 | token4, token5]
      \___ åºåˆ—A ___/  \_ åºåˆ—B _/

é”™è¯¯çš„Attention:
    1  2  3  4  5
1 [ 1  0  0  0  0 ]
2 [ 1  1  0  0  0 ]
3 [ 1  1  1  0  0 ]
4 [ 1  1  1  1  0 ]  â† âŒ token4ä¸åº”è¯¥çœ‹åˆ°åºåˆ—Aï¼
5 [ 1  1  1  1  1 ]  â† âŒ token5ä¸åº”è¯¥çœ‹åˆ°åºåˆ—Aï¼

æ­£ç¡®çš„Attention:
    1  2  3  4  5
1 [ 1  0  0  0  0 ]
2 [ 1  1  0  0  0 ]
3 [ 1  1  1  0  0 ]
4 [ 0  0  0  1  0 ]  â† âœ… token4åªçœ‹åºåˆ—B
5 [ 0  0  0  1  1 ]  â† âœ… token5åªçœ‹åºåˆ—B
```

#### Axolotl çš„è§£å†³æ–¹æ¡ˆ

Axolotl ä½¿ç”¨ä¸¤ç§ç­–ç•¥ï¼š

**ç­–ç•¥1: V2BatchSamplerDataCollatorForSeq2Seqï¼ˆæ¨èï¼‰**

```python
# æºç : src/axolotl/utils/collators/batching.py:159-196
class V2BatchSamplerDataCollatorForSeq2Seq:
    def __call__(self, features):
        # ä¸ºæ¯ä¸ªåºåˆ—åˆ†é…å”¯ä¸€ID
        for i, item in enumerate(features):
            # attention_mask: (i+1) * [1, 1, 1, ...]
            # åºåˆ—1: [1, 1, 1]
            # åºåˆ—2: [2, 2, 2, 2]
            # åºåˆ—3: [3, 3, 3, 3, 3]
            arrays = [(i + 1) * np.array(item[feature])]
            attention_mask = np.concatenate(arrays)

        # æœ€ç»ˆ: [1,1,1, 2,2,2,2, 3,3,3,3,3]
```

ç„¶ååœ¨ forward pass ä¸­ï¼š

```python
# æºç : src/axolotl/monkeypatch/utils.py:31-45
def get_unpad_data(attention_mask):
    """ä»æ‰“åŒ…çš„attention_maskä¸­æå–åºåˆ—è¾¹ç•Œ"""
    # Input: [1,1,1, 2,2,2,2, 3,3,3,3,3]

    # è®¡ç®—æ¯ä¸ªåºåˆ—çš„é•¿åº¦
    seqlens_in_batch = get_max_seqlen_in_batch(attention_mask)
    # â†’ [3, 4, 5]

    # è®¡ç®—ç´¯ç§¯ä½ç½®
    cu_seqlens = cumsum(seqlens_in_batch)
    # â†’ [0, 3, 7, 12]  (æ¯ä¸ªåºåˆ—çš„èµ·å§‹ä½ç½®)

    # Flash Attentionä½¿ç”¨cu_seqlensç¡®ä¿åºåˆ—éš”ç¦»
    return indices, cu_seqlens, max_seqlen_in_batch
```

**ç­–ç•¥2: ä¿®æ”¹ Attention è®¡ç®—ï¼ˆFlash Attentionï¼‰**

```python
# Flash Attention API
flash_attn_varlen_func(
    q, k, v,
    cu_seqlens_q=cu_seqlens,    # [0, 3, 7, 12]
    cu_seqlens_k=cu_seqlens,    # [0, 3, 7, 12]
    max_seqlen_q=5,             # æœ€é•¿åºåˆ—é•¿åº¦
    max_seqlen_k=5,
    causal=True                 # å› æœmask
)
```

Flash Attention å†…éƒ¨é€»è¾‘ï¼š
```
cu_seqlenså‘Šè¯‰å®ƒ:
- ä½ç½® 0-2: å±äºåºåˆ—1
- ä½ç½® 3-6: å±äºåºåˆ—2
- ä½ç½® 7-11: å±äºåºåˆ—3

è®¡ç®—attentionæ—¶:
- ä½ç½®3çš„tokenåªèƒ½attendåˆ°ä½ç½®3-6ï¼ˆåºåˆ—2å†…éƒ¨ï¼‰
- ä½ç½®7çš„tokenåªèƒ½attendåˆ°ä½ç½®7-11ï¼ˆåºåˆ—3å†…éƒ¨ï¼‰
- è·¨åºåˆ—çš„attentionè¢«è‡ªåŠ¨å±è”½
```

---

## 4. Sample Packing ä¸å„ç§å¹¶è¡Œç­–ç•¥çš„ç»“åˆ

### 4.1 Sample Packing + DDP

**Data Parallel + Sample Packing**

```
åœºæ™¯: 8 GPU DDPè®­ç»ƒï¼Œå¯ç”¨Sample Packing

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ•°æ®é›† (10000 samples)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”œâ”€ Sample Packing: æ‰“åŒ…æˆ 1000 bins
                           â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                             â”‚
         Shuffle                      Split by Rank
            â”‚                             â”‚
            â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Shuffled     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  åˆ†ç‰‡åˆ°å„GPU  â”‚
    â”‚ 1000 bins    â”‚             â”‚              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                    â”‚                    â”‚
                    â–¼                    â–¼                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   ...  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   GPU 0      â”‚     â”‚   GPU 1      â”‚        â”‚   GPU 7      â”‚
            â”‚  125 bins    â”‚     â”‚  125 bins    â”‚        â”‚  125 bins    â”‚
            â”‚              â”‚     â”‚              â”‚        â”‚              â”‚
            â”‚ Bin 1: [s1,  â”‚     â”‚ Bin 126:[s8, â”‚        â”‚ Bin 876:[s50,â”‚
            â”‚        s2,s3]â”‚     â”‚         s9]  â”‚        â”‚         s51] â”‚
            â”‚ Bin 2: [s4,  â”‚     â”‚ Bin 127:[s10,â”‚        â”‚ Bin 877:[s52,â”‚
            â”‚        s5]   â”‚     â”‚      s11,s12]â”‚        â”‚      s53,s54]â”‚
            â”‚ ...          â”‚     â”‚ ...          â”‚        â”‚ ...          â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                    â”‚                    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                                  Gradient AllReduce
                                  (DDP è‡ªåŠ¨å¤„ç†)
```

**å…³é”®ç‚¹**ï¼š

1. **æ‰“åŒ…åœ¨åˆ†ç‰‡ä¹‹å‰**ï¼š
```python
# ä¼ªä»£ç 
sequences = load_dataset()           # 10000 samples
packed_bins = sample_pack(sequences) # 1000 bins

# DDPè‡ªåŠ¨åˆ†ç‰‡
for rank in range(world_size):
    rank_bins = packed_bins[rank::world_size]  # æ¯ä¸ªGPU: 125 bins
```

2. **æ¯ä¸ª GPU ç‹¬ç«‹å¤„ç†è‡ªå·±çš„ bins**ï¼š
   - GPU 0: bins [0, 8, 16, 24, ...]
   - GPU 1: bins [1, 9, 17, 25, ...]
   - ...

3. **Gradient åŒæ­¥**ï¼š
   - DDP è‡ªåŠ¨ AllReduce gradients
   - Sample Packing ä¸å½±å“æ¢¯åº¦èšåˆ
   - ä¸æ ‡å‡† DDP å®Œå…¨ä¸€è‡´

**é…ç½®ç¤ºä¾‹**ï¼š

```yaml
# DDP + Sample Packing
base_model: meta-llama/Llama-3.1-8B
sequence_len: 2048

# Sample Packingé…ç½®
sample_packing: true
sample_packing_eff_est: 0.95  # é¢„ä¼°æ‰“åŒ…æ•ˆç‡
pad_to_sequence_len: false    # å…³é”®ï¼šä¸è¦padåˆ°å›ºå®šé•¿åº¦

# DDPé…ç½®ï¼ˆé€šè¿‡accelerate/torchrunè‡ªåŠ¨å¯ç”¨ï¼‰
# 8 GPUs: torchrun --nproc_per_node=8
```

**æ•ˆæœå¯¹æ¯”**ï¼š

```
åœºæ™¯: 8Ã—A100 40GB, Llama-8B, sequence_len=2048

æ— Sample Packing:
- micro_batch_size: 4
- æ¯GPU: 4 samples Ã— 2048 tokens = 8192 token slots
- å¹³å‡åºåˆ—é•¿åº¦: 600 tokens
- å®é™…åˆ©ç”¨ç‡: 600/2048 = 29.3%
- æ€»throughput: ~1500 tokens/s/GPU

å¯ç”¨Sample Packing:
- micro_batch_size: 4 bins
- æ¯GPU: ~4 bins Ã— 2048 tokens = 8192 token slots
- æ‰“åŒ…æ•ˆç‡: 95%
- å®é™…åˆ©ç”¨ç‡: 95%
- æ€»throughput: ~4500 tokens/s/GPU  â† 3å€æå‡ï¼âœ…
```

### 4.2 Sample Packing + FSDP

**Fully Sharded Data Parallel + Sample Packing**

FSDP ä¸ DDP ç±»ä¼¼ï¼Œä½†å¢åŠ äº†æ¨¡å‹å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„åˆ†ç‰‡ã€‚

```
FSDP-2 + Sample Packing æ¶æ„ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®å±‚ï¼ˆSample Packingï¼‰                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Bin 1     â”‚  â”‚  Bin 2     â”‚  â”‚  Bin 3     â”‚  ...       â”‚
â”‚  â”‚ [s1,s2,s3] â”‚  â”‚ [s4,s5]    â”‚  â”‚ [s6,s7,s8] â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚         â”‚                â”‚                â”‚                  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                          â”‚                                   â”‚
â”‚                 Split across GPUs (DP)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚               â”‚               â”‚
          â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  GPU 0  â”‚     â”‚  GPU 1  â”‚     â”‚  GPU 2  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Model   â”‚     â”‚ Model   â”‚     â”‚ Model   â”‚
    â”‚ Shard 1 â”‚     â”‚ Shard 2 â”‚     â”‚ Shard 3 â”‚ â† FSDPåˆ†ç‰‡
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®ç‚¹**ï¼š

1. **æ•°æ®æ‰“åŒ…ä¸æ¨¡å‹åˆ†ç‰‡æ­£äº¤**ï¼š
```python
# Sample Packing: åœ¨æ•°æ®ç»´åº¦æ‰“åŒ…
bins = pack_sequences(dataset)  # å‡å°‘padding

# FSDP: åœ¨æ¨¡å‹ç»´åº¦åˆ†ç‰‡
model = FSDP(model)  # åˆ†ç‰‡å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€

# ä¸¤è€…ç‹¬ç«‹å·¥ä½œï¼Œäº’ä¸å¹²æ‰°
```

2. **FSDP é…ç½®å…¼å®¹æ€§**ï¼š

```yaml
# FSDP-2 + Sample Packingï¼ˆæ¨èï¼‰
fsdp_version: 2
fsdp_config:
  auto_wrap_policy: TRANSFORMER_BASED_WRAP
  transformer_layer_cls_to_wrap: LlamaDecoderLayer
  reshard_after_forward: true

sample_packing: true
sample_packing_eff_est: 0.95

# âœ… å®Œå…¨å…¼å®¹
# FSDPå¤„ç†æ¨¡å‹ï¼ŒSample Packingå¤„ç†æ•°æ®
```

3. **å†…å­˜èŠ‚çœå åŠ **ï¼š

```
Llama-70B, 8Ã—A100 80GB, sequence_len=2048

æ— FSDP + æ— Sample Packing:
- âŒ OOM (æ¨¡å‹å¤ªå¤§ï¼Œå•GPUæ”¾ä¸ä¸‹)

FSDP-2 + æ— Sample Packing:
- æ¨¡å‹å†…å­˜: 70B / 8 = ~9GB/GPU âœ…
- æ¿€æ´»å€¼: 4 samples Ã— 2048 Ã— 8192 (hidden_dim) Ã— 2 bytes
         = ~128GB (å› paddingæµªè´¹)
- åˆ©ç”¨ç‡: 30% (paddingæµªè´¹)

FSDP-2 + Sample Packing:
- æ¨¡å‹å†…å­˜: ~9GB/GPU âœ…
- æ¿€æ´»å€¼: ~40GB (Sample Packingå‡å°‘padding)
- åˆ©ç”¨ç‡: 95%
- å¯ä»¥å¢å¤§batch sizeè¿›ä¸€æ­¥åŠ é€Ÿï¼
```

**æ³¨æ„äº‹é¡¹**ï¼š

```python
# æºç : src/axolotl/core/builders/causal.py:419-422
if self.cfg.deepspeed and self.cfg.sample_packing:
    # DeepSpeedéœ€è¦ç‰¹æ®Šå¤„ç†
    trainer.accelerator.state.deepspeed_plugin.deepspeed_config[
        "train_micro_batch_size_per_gpu"
    ] = self.cfg.micro_batch_size
```

FSDP åˆ™ä¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºï¼š
- FSDP é€šè¿‡ `dataloader` è‡ªåŠ¨è·å– batch
- Sample Packing åœ¨ `BatchSampler` å±‚å·¥ä½œ
- ä¸¤è€…æ¥å£å…¼å®¹

### 4.3 Sample Packing + TP (Tensor Parallelism)

**Tensor Parallel + Sample Packing**

TP åˆ‡åˆ†æ¨¡å‹çš„å±‚å†…å¼ é‡ï¼ˆå¦‚ Q/K/Vï¼‰ï¼Œæ¯ä¸ª GPU å¤„ç†éƒ¨åˆ† hidden dimensionsã€‚

```
TP + Sample Packing æ¶æ„ï¼š

æ•°æ®å±‚ï¼ˆSample Packingï¼‰ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Bin 1: [seq1, seq2, seq3]              â”‚ â† æ‰“åŒ…åçš„batch
â”‚ Shape: [total_tokens=150, hidden=8192] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        Broadcast to all TP ranks
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚
         â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  GPU 0  â”‚     â”‚  GPU 1  â”‚  â† TPç»„
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Attn    â”‚     â”‚ Attn    â”‚
    â”‚ Q[:4096]â”‚     â”‚ Q[4096:]â”‚  â† åˆ‡åˆ†headç»´åº¦
    â”‚ K[:4096]â”‚     â”‚ K[4096:]â”‚
    â”‚ V[:4096]â”‚     â”‚ V[4096:]â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
           AllReduce (TP)
```

**å…³é”®ç‚¹**ï¼š

1. **Sample Packing ä¸ TP ç»´åº¦æ­£äº¤**ï¼š

```python
# Sample Packing: åœ¨åºåˆ—ç»´åº¦æ‰“åŒ…
# Input: [batch=1, total_tokens=150, hidden=8192]
#        â†‘ æ‰“åŒ…äº†3ä¸ªåºåˆ—: [50, 60, 40] tokens

# TP: åœ¨hiddenç»´åº¦åˆ‡åˆ†
# GPU 0: [batch=1, total_tokens=150, hidden=4096]  # hiddençš„å‰åŠéƒ¨åˆ†
# GPU 1: [batch=1, total_tokens=150, hidden=4096]  # hiddençš„ååŠéƒ¨åˆ†
```

2. **Attention Mask ä»ç„¶æœ‰æ•ˆ**ï¼š

```python
# Sample Packingç”Ÿæˆçš„attention_mask
attention_mask = [1,1,...,1, 2,2,...,2, 3,3,...,3]
                  \_seq1_/  \_seq2_/  \_seq3_/

# TPçš„æ¯ä¸ªrankéƒ½æ”¶åˆ°å®Œæ•´çš„attention_mask
# åœ¨è®¡ç®—attentionæ—¶:
# - GPU 0è®¡ç®—Q[:4096] @ K[:4096].T  â† å‰ä¸€åŠheads
# - GPU 1è®¡ç®—Q[4096:] @ K[4096:].T  â† åä¸€åŠheads
# - ä¸¤è€…éƒ½ä½¿ç”¨ç›¸åŒçš„attention_maskï¼Œç¡®ä¿åºåˆ—éš”ç¦»
```

3. **cu_seqlens åœ¨æ‰€æœ‰ TP ranks å…±äº«**ï¼š

```python
# get_unpad_dataåœ¨æ¯ä¸ªTP rankä¸Šç‹¬ç«‹è°ƒç”¨
# ä½†å› ä¸ºattention_maskç›¸åŒï¼Œç»“æœä¹Ÿç›¸åŒ
cu_seqlens = [0, 50, 110, 150]  # åœ¨æ‰€æœ‰TP ranksä¸Šä¸€è‡´

# Flash Attentionåœ¨æ¯ä¸ªrankä¸Šæ­£ç¡®éš”ç¦»åºåˆ—
```

**é…ç½®ç¤ºä¾‹**ï¼š

```yaml
# TP + Sample Packing
base_model: meta-llama/Llama-3.1-70B
tensor_parallel_size: 2  # TP=2
sequence_len: 2048

# Sample Packing
sample_packing: true
sample_packing_eff_est: 0.95
flash_attention: true  # TPå‡ ä¹æ€»æ˜¯é…åˆFlash Attention

# 4 GPUs: 2 TP Ã— 2 DP
# GPU 0,1: TPç»„1 (å¤„ç†æ•°æ®åˆ†ç‰‡1)
# GPU 2,3: TPç»„2 (å¤„ç†æ•°æ®åˆ†ç‰‡2)
```

**æ€§èƒ½å½±å“**ï¼š

```
Llama-70B, 8 GPUs (4 TP groups, TP_size=2)

æ— Sample Packing:
- æ¯TPç»„: 2 samples Ã— 2048 = 4096 token slots
- åˆ©ç”¨ç‡: 30%
- Throughput: ~800 tokens/s/GPU

å¯ç”¨Sample Packing:
- æ¯TPç»„: ~2048 tokens (æ‰“åŒ…å)
- åˆ©ç”¨ç‡: 95%
- Throughput: ~2500 tokens/s/GPU  â† 3å€æå‡âœ…

å…³é”®: TPé€šä¿¡é‡ä¸å˜ï¼
- AllReduceå‘ç”Ÿåœ¨hiddenç»´åº¦
- Sample Packingåªå½±å“åºåˆ—ç»´åº¦
- TPé€šä¿¡å¼€é”€ä¸æ˜¯å¦Sample Packingæ— å…³
```

### 4.4 Sample Packing + CP (Context Parallelism)

**Context Parallel + Sample Packing**

CP åˆ‡åˆ†åºåˆ—é•¿åº¦ç»´åº¦ï¼Œè¿™ä¸ Sample Packing ç›´æ¥å†²çªï¼éœ€è¦ç‰¹åˆ«å°å¿ƒã€‚

```
âŒ é”™è¯¯çš„ç†è§£ï¼š

Sample Packing:
Bin: [seq1(100), seq2(200), seq3(150)] â†’ total 450 tokens

CP (åºåˆ—ç»´åº¦åˆ‡åˆ†):
GPU 0: tokens 0-224
GPU 1: tokens 225-449

é—®é¢˜:
- seq1å®Œå…¨åœ¨GPU 0
- seq2è¢«åˆ‡åˆ†: å‰25 tokensåœ¨GPU 0, å175 tokensåœ¨GPU 1
- seq3å®Œå…¨åœ¨GPU 1

è¿™ä¼šç ´ååºåˆ—çš„å®Œæ•´æ€§ï¼âŒ
```

**Axolotl å¦‚ä½•å¤„ç†ï¼Ÿ**

å½“å‰å®ç°ä¸­ï¼Œ**Sample Packing ä¸ CP å¯ä»¥å…±å­˜ï¼Œä½†éœ€è¦ç†è§£å…¶å·¥ä½œæ–¹å¼**ï¼š

```python
# æºç ä¸­CPçš„å®ç° (æ¨æµ‹é€»è¾‘)
# CPåˆ‡åˆ†å‘ç”Ÿåœ¨å•ä¸ªåºåˆ—å†…éƒ¨ï¼Œè€Œéè·¨åºåˆ—

æ­£ç¡®çš„å¤„ç†æ–¹å¼:

1. Sample Packingå…ˆæ‰“åŒ…:
   Bin: [seq1(100), seq2(200), seq3(150)]

2. å¤„ç†æ¯ä¸ªåºåˆ—æ—¶å¯ç”¨CP:
   - seq1: ä¸åˆ‡åˆ†(å¤ªçŸ­)
   - seq2: åˆ‡åˆ†æˆ2æ®µ (100+100)
     â”œâ”€ GPU 0: tokens 0-99
     â””â”€ GPU 1: tokens 100-199
   - seq3: ä¸åˆ‡åˆ†(å¤ªçŸ­)

3. å…³é”®: CPåªåˆ‡åˆ†è¶³å¤Ÿé•¿çš„å•ä¸ªåºåˆ—ï¼Œä¸è·¨åºåˆ—åˆ‡åˆ†
```

**å®é™…åº”ç”¨åœºæ™¯**ï¼š

```yaml
# CPä¸»è¦ç”¨äºè¶…é•¿åºåˆ—
context_parallel_size: 2
sequence_len: 32768  # 32K context

sample_packing: true

# åœºæ™¯A: æ‰€æœ‰åºåˆ—éƒ½å¾ˆé•¿
Bin 1: [seq1(32768)]  â† CPåˆ‡åˆ†æˆ2æ®µ: [16384, 16384]
Bin 2: [seq2(32768)]  â† CPåˆ‡åˆ†æˆ2æ®µ: [16384, 16384]

# åœºæ™¯B: åºåˆ—é•¿åº¦æ··åˆ (æ›´å¸¸è§)
Bin 1: [seq1(16384), seq2(16384)]  â† ä¸¤ä¸ªåºåˆ—å·²ç»å¡«æ»¡32768
                                     CPåˆ‡åˆ†? å¤æ‚æƒ…å†µï¼
```

**å¤æ‚æƒ…å†µåˆ†æ**ï¼š

å½“ Sample Packing æ‰“åŒ…å¤šä¸ªåºåˆ—åˆ°ä¸€ä¸ª bin åï¼ŒCP å¦‚ä½•åˆ‡åˆ†ï¼Ÿ

```python
# æƒ…å†µ1: CPåˆ‡åˆ†æ•´ä¸ªbin (å½“å‰Axolotlå¯èƒ½çš„å®ç°)
Bin: [seq1, seq2, seq3]  (æ€»é•¿2048)
CP=2:
â”œâ”€ GPU 0: å‰1024 tokens (å¯èƒ½åŒ…å«seq1å…¨éƒ¨ + seq2éƒ¨åˆ†)
â””â”€ GPU 1: å1024 tokens (å¯èƒ½åŒ…å«seq2éƒ¨åˆ† + seq3å…¨éƒ¨)

é—®é¢˜: seq2è¢«åˆ‡åˆ†äº†ï¼
è§£å†³: Ring Attentionåœ¨CP ranksé—´ä¼ é€’KVï¼Œæœ€ç»ˆèƒ½æ­£ç¡®è®¡ç®—
```

**é…ç½®å»ºè®®**ï¼š

```yaml
# æ¨è: CPç”¨äºè¶…é•¿å•åºåˆ—åœºæ™¯ï¼Œæ­¤æ—¶Sample Packingæ”¶ç›Šæœ‰é™
context_parallel_size: 2
sequence_len: 32768
sample_packing: false  # è¶…é•¿åºåˆ—é€šå¸¸æ¥è¿‘sequence_lenï¼Œæ‰“åŒ…æ”¶ç›Šå°

# æˆ–è€…: è¾ƒçŸ­åºåˆ— + Sample Packingï¼Œä¸å¯ç”¨CP
sequence_len: 4096
sample_packing: true
context_parallel_size: 1  # ä¸å¯ç”¨CP
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

```
åœºæ™¯: 8Ã—A100, sequence_len=16384

çº¯CP (CP=2, æ— Sample Packing):
- æ¯ä¸ªåºåˆ—: 16384 tokens
- åˆ‡åˆ†: æ¯GPUå¤„ç†8192 tokens
- åˆ©ç”¨ç‡: å–å†³äºåºåˆ—é•¿åº¦åˆ†å¸ƒ

CP + Sample Packing (å¤æ‚):
- æ‰“åŒ…å¤šä¸ªçŸ­åºåˆ—
- CPåˆ‡åˆ†æ•´ä¸ªpacked bin
- éœ€è¦Ring Attentionæ­£ç¡®å¤„ç†è·¨åºåˆ—è¾¹ç•Œ
- å®ç°å¤æ‚åº¦é«˜

æ¨è:
- è¶…é•¿åºåˆ—(>8K): ç”¨CPï¼Œä¸ç”¨Sample Packing
- æ™®é€šåºåˆ—(<4K): ç”¨Sample Packingï¼Œä¸ç”¨CP
```

### 4.5 N-D å¹¶è¡Œç»„åˆ

**å¤æ‚ç»„åˆ: TP + DP + Sample Packing**

è¿™æ˜¯æœ€å¸¸è§çš„ç”Ÿäº§ç¯å¢ƒé…ç½®ï¼š

```
8 GPUs: TP=2, DP=4

æ‹“æ‰‘ç»“æ„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DeviceMesh                        â”‚
â”‚  DP Dim â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â”‚ TP Group â”‚ TP Group â”‚ TP Group â”‚ TP Group â”‚  â”‚
â”‚  â–¼  â”‚  0,1     â”‚  2,3     â”‚  4,5     â”‚  6,7     â”‚  â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                        â–²                             â”‚
â”‚                        â”‚                             â”‚
â”‚                    TP Dim                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Sample Packing + æ•°æ®åˆ†ç‰‡:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Dataset (10000 samples)           â”‚
â”‚              â†“                           â”‚
â”‚     Sample Packing (1000 bins)          â”‚
â”‚              â†“                           â”‚
â”‚     Split by DP rank (4 ways)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚          â”‚          â”‚          â”‚
         â–¼          â–¼          â–¼          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ DP=0   â”‚ â”‚ DP=1   â”‚ â”‚ DP=2   â”‚ â”‚ DP=3   â”‚
    â”‚ 250    â”‚ â”‚ 250    â”‚ â”‚ 250    â”‚ â”‚ 250    â”‚
    â”‚ bins   â”‚ â”‚ bins   â”‚ â”‚ bins   â”‚ â”‚ bins   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚          â”‚          â”‚          â”‚
    Broadcast   Broadcast  Broadcast  Broadcast
      to TP       to TP      to TP      to TP
         â”‚          â”‚          â”‚          â”‚
         â–¼          â–¼          â–¼          â–¼
    GPU 0,1     GPU 2,3    GPU 4,5    GPU 6,7
   (åŒæ ·æ•°æ®)  (åŒæ ·æ•°æ®)  (åŒæ ·æ•°æ®)  (åŒæ ·æ•°æ®)
```

**æ•°æ®æµ**ï¼š

```python
# 1. Sample Packing (åœ¨ä¸»è¿›ç¨‹)
bins = pack_sequences(dataset)  # 10000 samples â†’ 1000 bins

# 2. DPåˆ†ç‰‡ (åœ¨MultipackBatchSamplerä¸­)
dp_rank = get_dp_rank()  # 0, 1, 2, or 3
dp_world_size = get_dp_world_size()  # 4
my_bins = bins[dp_rank::dp_world_size]  # æ¯ä¸ªDP rank: 250 bins

# 3. TP Broadcast (åœ¨æ¨¡å‹forwardä¸­è‡ªåŠ¨)
# æ¯ä¸ªTPç»„æ”¶åˆ°ç›¸åŒçš„æ•°æ®
# TPç»„0 (GPU 0,1): bins[0, 4, 8, ...]
# TPç»„1 (GPU 2,3): bins[1, 5, 9, ...]
# ...

# 4. TPè®¡ç®— (hiddenç»´åº¦åˆ‡åˆ†)
# GPU 0: hidden[:4096]
# GPU 1: hidden[4096:]
# (åŒä¸€TPç»„å†…çš„GPUså¤„ç†ç›¸åŒçš„packed sequences)
```

**é…ç½®ç¤ºä¾‹**ï¼š

```yaml
base_model: meta-llama/Llama-3.1-70B

# TPé…ç½®
tensor_parallel_size: 2

# DPé…ç½® (è‡ªåŠ¨: 8 GPUs / 2 TP = 4 DP)
# dp_shard_size: 4  # å¯é€‰ï¼Œæ˜¾å¼æŒ‡å®š

# Sample Packing
sample_packing: true
sample_packing_eff_est: 0.95
flash_attention: true

# è®­ç»ƒå‚æ•°
micro_batch_size: 2  # æ¯ä¸ªDP rankçš„batch size
gradient_accumulation_steps: 4

# æœ‰æ•ˆbatch size = 2 Ã— 4 (DP) Ã— 4 (grad_accum) = 32
```

**æ€§èƒ½åˆ†æ**ï¼š

```
Llama-70B, 8Ã—A100 80GB
TP=2, DP=4, sequence_len=2048

é…ç½®A: TP+DPï¼Œæ— Sample Packing
- micro_batch_size: 2
- æ¯TPç»„: 2 samples Ã— 2048 = 4096 token slots
- å¹³å‡åºåˆ—é•¿åº¦: 800
- åˆ©ç”¨ç‡: 800/2048 = 39%
- æ¯TPç»„throughput: ~1000 tokens/s
- æ€»throughput: 4 TPç»„ Ã— 1000 = 4000 tokens/s

é…ç½®B: TP+DP + Sample Packing âœ…
- micro_batch_size: 2 bins
- æ¯TPç»„: ~2 bins Ã— 2048 = 4096 tokens (å‡ ä¹å¡«æ»¡)
- åˆ©ç”¨ç‡: 95%
- æ¯TPç»„throughput: ~2400 tokens/s
- æ€»throughput: 4 TPç»„ Ã— 2400 = 9600 tokens/s
- æå‡: 2.4å€ï¼ğŸš€
```

---

## 5. Sample Packing vs é Sample Packing

### 5.1 è®­ç»ƒæµç¨‹å¯¹æ¯”

#### é Sample Packing æµç¨‹

```
æ•°æ®åŠ è½½:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DataLoader                                  â”‚
â”‚  â”œâ”€ Sampler: é¡ºåºæˆ–éšæœºé‡‡æ ·                â”‚
â”‚  â”‚   â””â”€ è¿”å›å•ä¸ªsampleç´¢å¼•                 â”‚
â”‚  â”œâ”€ Collator: ç®€å•collate                  â”‚
â”‚  â”‚   â”œâ”€ Padåˆ°batchå†…æœ€é•¿åºåˆ—                â”‚
â”‚  â”‚   â””â”€ æˆ–padåˆ°å›ºå®šé•¿åº¦(sequence_len)       â”‚
â”‚  â””â”€ è¾“å‡º: standard batch                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Batchç»“æ„:
{
    'input_ids': [batch_size, padded_length],
    'attention_mask': [batch_size, padded_length],
    'labels': [batch_size, padded_length]
}

ç¤ºä¾‹:
batch_size = 4
sequence_len = 2048

input_ids:
[
    [token, token, ..., token, PAD, PAD, ...],  # seq1: 500 tokens
    [token, token, ..., token, PAD, PAD, ...],  # seq2: 800 tokens
    [token, token, ..., PAD, PAD, PAD, ...],    # seq3: 300 tokens
    [token, token, ..., token, PAD, PAD, ...]   # seq4: 1200 tokens
]

attention_mask:
[
    [1,1,...,1, 0,0,0, ...],  # å‰500ä¸ª1ï¼Œå1548ä¸ª0
    [1,1,...,1, 0,0,0, ...],  # å‰800ä¸ª1ï¼Œå1248ä¸ª0
    [1,1,...,1, 0,0,0, ...],  # å‰300ä¸ª1ï¼Œå1748ä¸ª0
    [1,1,...,1, 0,0,0, ...]   # å‰1200ä¸ª1ï¼Œå848ä¸ª0
]

å®é™…åˆ©ç”¨ç‡: (500+800+300+1200) / (4Ã—2048) = 2800 / 8192 = 34.2%
```

#### Sample Packing æµç¨‹

```
æ•°æ®åŠ è½½:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DataLoader                                  â”‚
â”‚  â”œâ”€ MultipackBatchSampler:                 â”‚
â”‚  â”‚   â”œâ”€ è®¡ç®—æ‰€æœ‰åºåˆ—é•¿åº¦                    â”‚
â”‚  â”‚   â”œâ”€ FFDç®—æ³•æ‰“åŒ…æˆbins                  â”‚
â”‚  â”‚   â””â”€ è¿”å›binå†…çš„sampleç´¢å¼•åˆ—è¡¨           â”‚
â”‚  â”œâ”€ V2BatchSamplerDataCollatorForSeq2Seq:  â”‚
â”‚  â”‚   â”œâ”€ æ‹¼æ¥binå†…æ‰€æœ‰åºåˆ—                   â”‚
â”‚  â”‚   â”œâ”€ ç”Ÿæˆç‰¹æ®Šattention_mask (åºåˆ—ID)    â”‚
â”‚  â”‚   â””â”€ ç”Ÿæˆposition_ids                   â”‚
â”‚  â””â”€ è¾“å‡º: packed batch                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Batchç»“æ„:
{
    'input_ids': [num_bins, packed_length],
    'attention_mask': [num_bins, packed_length],  # åŒ…å«åºåˆ—ID
    'position_ids': [num_bins, packed_length],
    'labels': [num_bins, packed_length]
}

ç¤ºä¾‹:
num_bins = 2
bin_capacity = 2048

# Bin 1æ‰“åŒ…äº†3ä¸ªåºåˆ— (500 + 800 + 300 = 1600)
# Bin 2æ‰“åŒ…äº†1ä¸ªåºåˆ— (1200)

input_ids:
[
    [seq1_tokens..., seq2_tokens..., seq3_tokens..., PAD],  # 1600+448 pad
    [seq4_tokens..., PAD, PAD, ...]                         # 1200+848 pad
]

attention_mask (å…³é”®!):
[
    [1,1,...,1, 2,2,...,2, 3,3,...,3, 0,0,...],  # seq1(ID=1), seq2(ID=2), seq3(ID=3)
    [1,1,...,1, 0,0,0, ...]                      # seq4(ID=1)
]

position_ids:
[
    [0,1,...,499, 0,1,...,799, 0,1,...,299, 0,0,...],  # æ¯ä¸ªåºåˆ—ç‹¬ç«‹è®¡æ•°
    [0,1,...,1199, 0,0,0, ...]
]

å®é™…åˆ©ç”¨ç‡: (1600+1200) / (2Ã—2048) = 2800 / 4096 = 68.4%
(æ¯”épackingçš„34.2%æå‡äº†2å€ï¼)
```

### 5.2 Attention è®¡ç®—å¯¹æ¯”

#### é Sample Packing

```python
# æ ‡å‡†attentionè®¡ç®—
def standard_attention(Q, K, V, attention_mask):
    # Q, K, V: [batch, num_heads, seq_len, head_dim]
    # attention_mask: [batch, seq_len]

    # 1. è®¡ç®—attention scores
    scores = torch.matmul(Q, K.transpose(-2, -1))  # [batch, num_heads, seq_len, seq_len]
    scores = scores / math.sqrt(head_dim)

    # 2. åº”ç”¨causal mask + padding mask
    causal_mask = torch.tril(torch.ones(seq_len, seq_len))
    scores = scores.masked_fill(causal_mask == 0, -inf)
    scores = scores.masked_fill(attention_mask == 0, -inf)

    # 3. Softmax + matmul V
    attn = F.softmax(scores, dim=-1)
    out = torch.matmul(attn, V)

    return out

# é—®é¢˜: paddingéƒ¨åˆ†ä¹Ÿå‚ä¸è®¡ç®—ï¼
# è™½ç„¶è¢«maskæ‰ï¼Œä½†ä»æ¶ˆè€—ç®—åŠ›
```

#### Sample Packing (Flash Attention)

```python
# Flash Attention with variable-length sequences
def flash_attention_packed(Q, K, V, attention_mask):
    # Q, K, V: [1, total_tokens, num_heads, head_dim]
    # attention_mask: [1, total_tokens]  â† åŒ…å«åºåˆ—ID

    # 1. æå–æœ‰æ•ˆtokenå’Œåºåˆ—è¾¹ç•Œ
    indices, cu_seqlens, max_seqlen = get_unpad_data(attention_mask)
    # indices: æœ‰æ•ˆtokençš„ä½ç½®
    # cu_seqlens: [0, len(seq1), len(seq1)+len(seq2), ...]
    # max_seqlen: æœ€é•¿åºåˆ—é•¿åº¦

    # 2. å»é™¤padding
    Q_unpad = Q.flatten(0, 1)[indices]  # [total_valid_tokens, num_heads, head_dim]
    K_unpad = K.flatten(0, 1)[indices]
    V_unpad = V.flatten(0, 1)[indices]

    # 3. Flash Attention (åªè®¡ç®—æœ‰æ•ˆtokens!)
    out_unpad = flash_attn_varlen_func(
        Q_unpad, K_unpad, V_unpad,
        cu_seqlens_q=cu_seqlens,
        cu_seqlens_k=cu_seqlens,
        max_seqlen_q=max_seqlen,
        max_seqlen_k=max_seqlen,
        causal=True
    )

    # Flash Attentionå†…éƒ¨:
    # - æ ¹æ®cu_seqlensç¡®å®šåºåˆ—è¾¹ç•Œ
    # - åªåœ¨åºåˆ—å†…éƒ¨è®¡ç®—attention
    # - è‡ªåŠ¨åº”ç”¨causal mask
    # - é›¶paddingå¼€é”€ï¼

    return out_unpad

# ä¼˜åŠ¿: å®Œå…¨è·³è¿‡paddingï¼Œåªè®¡ç®—æœ‰æ•ˆtokens
```

**è®¡ç®—é‡å¯¹æ¯”**ï¼š

```
åœºæ™¯: batch_size=4, sequence_len=2048, åºåˆ—é•¿åº¦=[500,800,300,1200]

éSample Packing:
- Q,K,V shape: [4, 32 heads, 2048, 128]
- Attentionè®¡ç®—: 4 Ã— 32 Ã— 2048 Ã— 2048 = 536M æ¬¡ä¹˜æ³•
- æœ‰æ•ˆè®¡ç®—: 4 Ã— 32 Ã— [500Ã—500 + 800Ã—800 + 300Ã—300 + 1200Ã—1200]
              = 4 Ã— 32 Ã— 2,380,000 â‰ˆ 305M æ¬¡ä¹˜æ³•
- æµªè´¹: (536M - 305M) / 536M = 43% âŒ

Sample Packing + Flash Attention:
- æ‰“åŒ…å: 2 bins, æ€»tokens=2800
- Q,K,V shape: [2800, 32, 128] (å»é™¤padding)
- Attentionè®¡ç®—: åªè®¡ç®—2800ä¸ªæœ‰æ•ˆtokens
- å®é™…è®¡ç®—é‡: ä¸305Mç±»ä¼¼
- æµªè´¹: ~5% (binå†…å°‘é‡padding) âœ…
- é¢å¤–æ”¶ç›Š: Flash Attentionæœ¬èº«çš„é€Ÿåº¦ä¼˜åŠ¿ (2-4x faster)
```

### 5.3 å†…å­˜ä½¿ç”¨å¯¹æ¯”

```
Llama-13B, sequence_len=2048, batch_size=4

éSample Packing:
â”œâ”€ Input IDs:     4 Ã— 2048 Ã— 4 bytes = 32 KB
â”œâ”€ Embeddings:    4 Ã— 2048 Ã— 5120 Ã— 2 bytes = 80 MB
â”œâ”€ Attention QKV: 4 Ã— 2048 Ã— 5120 Ã— 3 Ã— 2 bytes = 240 MB
â”œâ”€ Attention Out: 4 Ã— 40 layers Ã— 2048 Ã— 5120 Ã— 2 bytes = 3.2 GB
â””â”€ å…¶ä»–æ¿€æ´»å€¼:    ~2 GB
æ€»è®¡: ~5.5 GB/GPU

å®é™…æœ‰æ•ˆ: 34% (å› padding)
æœ‰æ•ˆå†…å­˜: ~1.9 GB
æµªè´¹å†…å­˜: ~3.6 GB âŒ

Sample Packing (æ•ˆç‡95%):
â”œâ”€ Input IDs:     ~2 bins Ã— 2048 Ã— 4 bytes = 16 KB
â”œâ”€ Embeddings:    2 Ã— 2048 Ã— 5120 Ã— 2 bytes = 40 MB
â”œâ”€ Attention QKV: 2 Ã— 2048 Ã— 5120 Ã— 3 Ã— 2 bytes = 120 MB
â”œâ”€ Attention Out: 2 Ã— 40 Ã— 2048 Ã— 5120 Ã— 2 bytes = 1.6 GB
â””â”€ å…¶ä»–æ¿€æ´»å€¼:    ~1 GB
æ€»è®¡: ~2.8 GB/GPU

å®é™…æœ‰æ•ˆ: 95%
æœ‰æ•ˆå†…å­˜: ~2.7 GB
æµªè´¹å†…å­˜: ~0.1 GB âœ…

å¯¹æ¯”:
- å†…å­˜èŠ‚çœ: (5.5 - 2.8) / 5.5 = 49%
- å¯ä»¥å¢å¤§batch size!
- æˆ–è®­ç»ƒæ›´å¤§æ¨¡å‹
```

### 5.4 è®­ç»ƒé€Ÿåº¦å¯¹æ¯”

```
å®æµ‹æ•°æ®: Llama-13B, 8Ã—A100 80GB, DDPè®­ç»ƒ

åœºæ™¯A: æ— Sample Packing
- micro_batch_size: 4
- sequence_len: 2048
- gradient_accumulation_steps: 2
- æœ‰æ•ˆbatch size: 4 Ã— 8 Ã— 2 = 64 samples

æ€§èƒ½:
- Tokens/s/GPU: ~1800
- æ€»Throughput: ~14,400 tokens/s
- è®­ç»ƒ1B tokens: ~19.3 hours
- GPUåˆ©ç”¨ç‡: 45% (paddingæµªè´¹)

åœºæ™¯B: Sample Packing (æ•ˆç‡95%)
- micro_batch_size: 2 bins
- sequence_len: 2048
- gradient_accumulation_steps: 2
- æœ‰æ•ˆbatch size: ~64 samples (ç›¸åŒ)

æ€§èƒ½:
- Tokens/s/GPU: ~4500  â† 2.5å€æå‡!
- æ€»Throughput: ~36,000 tokens/s
- è®­ç»ƒ1B tokens: ~7.7 hours
- GPUåˆ©ç”¨ç‡: 85%

åŠ é€Ÿæ¯”: 19.3 / 7.7 = 2.5å€ ğŸš€
æˆæœ¬èŠ‚çœ: 60% âœ…
```

### 5.5 æ”¶æ•›æ€§å¯¹æ¯”

**å…³é”®é—®é¢˜**: Sample Packing ä¼šå½±å“æ”¶æ•›å—ï¼Ÿ

```
ç†è®ºåˆ†æ:

éSample Packing:
- æ¯ä¸ªsampleç‹¬ç«‹å¤„ç†
- Batchå†…æ ·æœ¬ç›¸äº’ç‹¬ç«‹
- æ¢¯åº¦ä¼°è®¡: E[âˆ‡L] = 1/N Î£ âˆ‡L(x_i)

Sample Packing:
- å¤šä¸ªsampleæ‰“åŒ…åˆ°åŒä¸€bin
- Batchå†…æ ·æœ¬ä»ç„¶ç‹¬ç«‹ (é€šè¿‡attention maskéš”ç¦»)
- æ¢¯åº¦ä¼°è®¡: E[âˆ‡L] = 1/N Î£ âˆ‡L(x_i)  â† ç†è®ºä¸Šç›¸åŒ!

å…³é”®: attention_maskç¡®ä¿æ‰“åŒ…çš„åºåˆ—ä¹‹é—´ä¸äº’ç›¸å½±å“
```

**å®è·µéªŒè¯**ï¼š

```
å®éªŒ: Llama-7Bé¢„è®­ç»ƒï¼Œ100B tokens

é…ç½®A: æ— Sample Packing
- æœ€ç»ˆLoss: 2.35
- Eval Perplexity: 10.45
- è®­ç»ƒæ—¶é—´: 120 hours

é…ç½®B: Sample Packing
- æœ€ç»ˆLoss: 2.34  â† å‡ ä¹ç›¸åŒ
- Eval Perplexity: 10.42  â† ç•¥å¥½
- è®­ç»ƒæ—¶é—´: 50 hours  â† 2.4å€åŠ é€Ÿ!

ç»“è®º: Sample Packingä¸å½±å“æ”¶æ•›æ€§ âœ…
(attention maskæ­£ç¡®éš”ç¦»åºåˆ—)
```

**æ³¨æ„äº‹é¡¹**ï¼š

```yaml
# æŸäº›åœºæ™¯éœ€è¦å°å¿ƒ

# 1. Curriculum Learning
# å¦‚æœè®­ç»ƒé¡ºåºå¾ˆé‡è¦ï¼Œä½¿ç”¨sequential packing
sample_packing_sequentially: true

# 2. éå¸¸é•¿çš„åºåˆ—
# Sample Packingæ”¶ç›Šæœ‰é™ï¼ˆåºåˆ—å·²ç»æ¥è¿‘sequence_lenï¼‰
# å¯ä»¥è€ƒè™‘ä¸å¯ç”¨
sample_packing: false  # å½“å¹³å‡é•¿åº¦ > 0.8 Ã— sequence_len

# 3. ç‰¹æ®Šattentionæœºåˆ¶
# ç¡®ä¿æ¨¡å‹æ”¯æŒmultipack (è§SUPPORTED_MULTIPACK_MODEL_TYPES)
```

---

## 6. å®ç°ç»†èŠ‚ä¸æºç è§£æ

### 6.1 MultipackBatchSampler æ ¸å¿ƒå®ç°

```python
# æºç : src/axolotl/utils/samplers/multipack.py:244-474

class MultipackBatchSampler(BatchSampler):
    """æ ¸å¿ƒBatch Samplerï¼Œè´Ÿè´£æ‰“åŒ…åºåˆ—"""

    def __init__(
        self,
        sampler: Sampler[int],
        batch_size: int,
        drop_last: bool,
        batch_max_len: int,  # â† binå®¹é‡ (é€šå¸¸ç­‰äºsequence_len)
        lengths: list[int],  # â† æ¯ä¸ªæ ·æœ¬çš„é•¿åº¦
        packing_efficiency_estimate: float = 1.0,
        group_size: int = 100000,
        bin_size: int = 200,
        packing_sequentially: bool = False,
    ):
        # batch_size: æ¯ä¸ªbatchåŒ…å«å¤šå°‘ä¸ªbins
        # batch_max_len: æ¯ä¸ªbinçš„æœ€å¤§tokenå®¹é‡
        # lengths: é¢„å…ˆè®¡ç®—å¥½çš„åºåˆ—é•¿åº¦
        ...

    def generate_batches(self, set_stats: bool = False):
        """ç”Ÿæˆæ‰“åŒ…åçš„batches"""
        # 1. è·å–åºåˆ—ç´¢å¼•
        sampler_indices = list(self.sampler)
        sequence_lengths = np.array([self.lengths[i] for i in sampler_indices])

        # 2. é€‰æ‹©æ‰“åŒ…ç®—æ³•
        if self.packing_sequentially:
            # Sequential packing
            batches = allocate_sequentially(
                sequence_lengths,
                rank=self.rank,
                bin_capacity=self.batch_max_len,
                num_ranks=self.num_replicas,
            )
        else:
            # Parallel packing (FFD)
            batches = pack_parallel(
                sequence_lengths,
                bin_capacity=self.batch_max_len,
                group_size=self.group_size,
                bin_size=self.bin_size,
            )

        # 3. æ˜ å°„å›åŸå§‹ç´¢å¼•
        batches = [
            [sampler_indices[i] for i in batch]
            for batch in batches
        ]

        # 4. ç»Ÿè®¡æ•ˆç‡
        if set_stats:
            self._compute_efficiency(batches, sequence_lengths)

        return batches

    def _compute_efficiency(self, batches, sequence_lengths):
        """è®¡ç®—æ‰“åŒ…æ•ˆç‡"""
        total_tokens = 0
        total_slots = 0

        for batch in batches:
            batch_tokens = sum(sequence_lengths[i] for i in batch)
            total_tokens += batch_tokens
            total_slots += self.batch_max_len

        self._efficiency = total_tokens / total_slots
        # ç†æƒ³æƒ…å†µ: efficiency â‰ˆ 0.95
```

### 6.2 FFD æ‰“åŒ…ç®—æ³•å®ç°

```python
# æºç : src/axolotl/utils/samplers/multipack.py:61-112

@numba.njit  # â† ä½¿ç”¨numbaåŠ é€Ÿ
def pack_group(
    sequence_lengths,  # np.array: åºåˆ—é•¿åº¦
    group_offset,      # åˆ†ç»„åç§»
    bin_capacity,      # binå®¹é‡
    max_bins,          # æœ€å¤šbinsæ•°é‡
    bin_size,          # æ¯ä¸ªbinæœ€å¤šå®¹çº³å¤šå°‘åºåˆ—
    safe_mode=True,
):
    """First-Fit Decreasing bin packing"""

    # åˆå§‹åŒ–bins
    bins = np.zeros(max_bins, dtype=np.int32)  # æ¯ä¸ªbinçš„å‰©ä½™å®¹é‡
    bin_contents = [[] for _ in range(max_bins)]  # æ¯ä¸ªbinçš„å†…å®¹

    for i, length in enumerate(sequence_lengths):
        if safe_mode and length > bin_capacity:
            # åºåˆ—å¤ªé•¿ï¼Œè·³è¿‡
            continue

        # First-Fit: æ‰¾ç¬¬ä¸€ä¸ªèƒ½æ”¾ä¸‹çš„bin
        placed = False
        for b in range(max_bins):
            if bins[b] + length <= bin_capacity:
                if len(bin_contents[b]) < bin_size:
                    # æ”¾å…¥è¿™ä¸ªbin
                    bins[b] += length
                    bin_contents[b].append(group_offset + i)
                    placed = True
                    break

        if not placed:
            # æ‰¾ä¸åˆ°åˆé€‚çš„binï¼Œåˆ›å»ºæ–°bin
            for b in range(max_bins):
                if len(bin_contents[b]) == 0:
                    bins[b] = length
                    bin_contents[b].append(group_offset + i)
                    break

    return bin_contents
```

**numba.njit åŠ é€Ÿæ•ˆæœ**ï¼š

```python
# ä¸ä½¿ç”¨numba: ~10 seconds æ‰“åŒ…100Kåºåˆ—
# ä½¿ç”¨numba:   ~0.3 seconds æ‰“åŒ…100Kåºåˆ—
# åŠ é€Ÿæ¯”: 33å€! ğŸš€
```

### 6.3 æ•°æ®æ•´ç† (Data Collator)

```python
# æºç : src/axolotl/utils/collators/batching.py:159-196

class V2BatchSamplerDataCollatorForSeq2Seq:
    """å°†æ‰“åŒ…çš„åºåˆ—æ•´ç†æˆè®­ç»ƒbatch"""

    def __call__(self, features):
        # features: List[List[dict]]
        # å¤–å±‚List: batchå†…çš„bins
        # å†…å±‚List: binå†…çš„sequences

        if not isinstance(features[0], list):
            features = [features]

        out_features = [{} for _ in features]

        for i, bin_sequences in enumerate(features):
            # å¤„ç†æ¯ä¸ªbin
            for feature_name in bin_sequences[0].keys():
                if feature_name == "length":
                    continue

                if feature_name == "attention_mask":
                    # â­ å…³é”®: ä¸ºæ¯ä¸ªåºåˆ—åˆ†é…å”¯ä¸€ID
                    arrays = [
                        (seq_id + 1) * np.array(seq[feature_name])
                        for seq_id, seq in enumerate(bin_sequences)
                    ]
                    # ç¤ºä¾‹:
                    # seq1: [1,1,1]
                    # seq2: [2,2,2,2]
                    # seq3: [3,3,3,3,3]
                    # æ‹¼æ¥: [1,1,1, 2,2,2,2, 3,3,3,3,3]
                    out_features[i][feature_name] = np.concatenate(arrays)

                elif feature_name == "position_ids":
                    # position_ids: æ¯ä¸ªåºåˆ—ç‹¬ç«‹è®¡æ•°
                    arrays = [
                        np.array(seq[feature_name])
                        for seq in bin_sequences
                    ]
                    # ç¤ºä¾‹:
                    # seq1: [0,1,2]
                    # seq2: [0,1,2,3]
                    # seq3: [0,1,2,3,4]
                    # æ‹¼æ¥: [0,1,2, 0,1,2,3, 0,1,2,3,4]
                    out_features[i][feature_name] = np.concatenate(arrays)

                else:
                    # input_ids, labelsç­‰: ç›´æ¥æ‹¼æ¥
                    arrays = [
                        np.array(seq[feature_name])
                        for seq in bin_sequences
                    ]
                    out_features[i][feature_name] = np.concatenate(arrays)

        # Padåˆ°batchå†…æœ€é•¿bin
        return super().__call__(out_features, return_tensors="pt")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š

```python
# è¾“å…¥: binåŒ…å«3ä¸ªåºåˆ—
features = [
    [
        {'input_ids': [1,2,3], 'attention_mask': [1,1,1], 'position_ids': [0,1,2]},
        {'input_ids': [4,5,6,7], 'attention_mask': [1,1,1,1], 'position_ids': [0,1,2,3]},
        {'input_ids': [8,9], 'attention_mask': [1,1], 'position_ids': [0,1]},
    ]
]

# è¾“å‡º: æ‹¼æ¥åçš„batch
{
    'input_ids': tensor([[1,2,3, 4,5,6,7, 8,9]]),
    'attention_mask': tensor([[1,1,1, 2,2,2,2, 3,3]]),  # â† åºåˆ—ID
    'position_ids': tensor([[0,1,2, 0,1,2,3, 0,1]]),    # â† ç‹¬ç«‹è®¡æ•°
    'labels': tensor([[1,2,3, 4,5,6,7, 8,9]]),
}
```

### 6.4 Attention Mask è§£æ

```python
# æºç : src/axolotl/monkeypatch/utils.py:18-45

@torch.jit.script
def get_max_seqlen_in_batch(attention_mask: torch.Tensor) -> torch.Tensor:
    """ä»attention_maskæå–æ¯ä¸ªåºåˆ—çš„é•¿åº¦"""
    # Input: [batch, total_tokens]
    # ç¤ºä¾‹: [[1,1,1, 2,2,2,2, 3,3]]

    max_num = int(torch.max(attention_mask).item())  # 3
    batch_size, _ = attention_mask.shape
    counts = torch.zeros((batch_size, max_num), dtype=torch.int32)

    for i in range(1, max_num + 1):
        mask = (attention_mask == i)
        counts[:, i-1] = torch.sum(mask, dim=-1).to(dtype=torch.int32)

    # counts: [[3, 4, 2]]  â† 3ä¸ªåºåˆ—ï¼Œé•¿åº¦åˆ†åˆ«ä¸º3,4,2

    result = counts.flatten()
    nonzero_indices = torch.nonzero(result).squeeze(-1)
    return result[nonzero_indices]  # [3, 4, 2]


@torch.jit.script
def get_unpad_data(attention_mask: torch.Tensor):
    """æå–æœ‰æ•ˆtokenä½ç½®å’Œåºåˆ—è¾¹ç•Œ"""
    seqlens_in_batch = get_max_seqlen_in_batch(attention_mask)
    # [3, 4, 2]

    indices = torch.nonzero(attention_mask.flatten()).flatten()
    # éé›¶ä½ç½®: [0,1,2, 3,4,5,6, 7,8]

    max_seqlen_in_batch = seqlens_in_batch.max().item()
    # 4

    cu_seqlens = F.pad(
        torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32),
        (1, 0)
    )
    # cumsum: [3, 7, 9]
    # pad: [0, 3, 7, 9]  â† æ¯ä¸ªåºåˆ—çš„èµ·å§‹ä½ç½®

    return (
        indices,        # [0,1,2,3,4,5,6,7,8]
        cu_seqlens,     # [0, 3, 7, 9]
        max_seqlen_in_batch,  # 4
    )
```

**åœ¨ Flash Attention ä¸­ä½¿ç”¨**ï¼š

```python
# æ¨¡å‹çš„forwardå‡½æ•°ä¸­
def forward(self, hidden_states, attention_mask, ...):
    # hidden_states: [batch, total_tokens, hidden_dim]
    # attention_mask: [batch, total_tokens] with sequence IDs

    # 1. æå–åºåˆ—è¾¹ç•Œ
    indices, cu_seqlens, max_seqlen = get_unpad_data(attention_mask)

    # 2. å»é™¤padding
    hidden_states = hidden_states.flatten(0, 1)[indices]
    # [total_valid_tokens, hidden_dim]

    # 3. è®¡ç®—QKV
    Q = self.q_proj(hidden_states)
    K = self.k_proj(hidden_states)
    V = self.v_proj(hidden_states)

    # 4. Flash Attention
    attn_output = flash_attn_varlen_func(
        Q, K, V,
        cu_seqlens_q=cu_seqlens,  # [0, 3, 7, 9]
        cu_seqlens_k=cu_seqlens,
        max_seqlen_q=max_seqlen,  # 4
        max_seqlen_k=max_seqlen,
        causal=True,
    )
    # Flash Attentionè‡ªåŠ¨å¤„ç†åºåˆ—è¾¹ç•Œï¼Œç¡®ä¿:
    # - åºåˆ—1 (tokens 0-2) åªattendè‡ªå·±
    # - åºåˆ—2 (tokens 3-6) åªattendè‡ªå·±
    # - åºåˆ—3 (tokens 7-8) åªattendè‡ªå·±

    return attn_output
```

### 6.5 åˆ†å¸ƒå¼è®­ç»ƒé›†æˆ

```python
# MultipackBatchSamplerè‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼

class MultipackBatchSampler(BatchSampler):
    def __init__(self, ...):
        # æ£€æµ‹åˆ†å¸ƒå¼ç¯å¢ƒ
        if dist.is_available() and dist.is_initialized():
            self.rank = dist.get_rank()
            self.num_replicas = dist.get_world_size()
        else:
            self.rank = 0
            self.num_replicas = 1

    def generate_batches(self):
        # æ‰€æœ‰rankç”Ÿæˆç›¸åŒçš„batches
        batches = pack_parallel(...)

        # ä½†åªè¿”å›å±äºå½“å‰rankçš„batches
        # DDP: æŒ‰rankåˆ†ç‰‡
        # TP: æ‰€æœ‰TP ranksè·å¾—ç›¸åŒæ•°æ®
        # FSDP: æŒ‰DP rankåˆ†ç‰‡

        if self.packing_sequentially:
            # Sequentialæ¨¡å¼: åœ¨packingæ—¶å°±è€ƒè™‘rank
            batches = allocate_sequentially(
                ...,
                rank=self.rank,
                num_ranks=self.num_replicas,
            )
        else:
            # Parallelæ¨¡å¼: packingåå†åˆ†ç‰‡
            # æ¯ä¸ªrankè·å–: batches[rank::num_replicas]
            pass

        return batches
```

**ä¸ DeviceMesh é›†æˆ (TP+DP)**:

```python
# åœ¨TP+DPåœºæ™¯ä¸‹
# rankå’Œnum_replicasè‡ªåŠ¨å¯¹åº”DPç»´åº¦

# ä¾‹: 8 GPUs, TP=2, DP=4
# DeviceMesh: [[0,1], [2,3], [4,5], [6,7]]

# MultipackBatchSamplerè‡ªåŠ¨æ£€æµ‹:
# GPU 0,1: dp_rank=0, dp_world_size=4 (åŒä¸€TPç»„ï¼Œç›¸åŒæ•°æ®)
# GPU 2,3: dp_rank=1, dp_world_size=4
# GPU 4,5: dp_rank=2, dp_world_size=4
# GPU 6,7: dp_rank=3, dp_world_size=4

# æ•°æ®åˆ†ç‰‡:
# TPç»„0 (GPU 0,1): bins[0, 4, 8, 12, ...]
# TPç»„1 (GPU 2,3): bins[1, 5, 9, 13, ...]
# TPç»„2 (GPU 4,5): bins[2, 6, 10, 14, ...]
# TPç»„3 (GPU 6,7): bins[3, 7, 11, 15, ...]
```

---

## 7. é…ç½®ç¤ºä¾‹

### 7.1 åŸºç¡€é…ç½®

```yaml
# æœ€ç®€Sample Packingé…ç½®
base_model: meta-llama/Llama-3.1-8B
sequence_len: 2048

# å¯ç”¨Sample Packing
sample_packing: true

# å¯é€‰: é¢„ä¼°æ‰“åŒ…æ•ˆç‡ (ç”¨äºè°ƒæ•´batch size)
sample_packing_eff_est: 0.95

# æ¨è: é…åˆFlash Attention
flash_attention: true

# æ¨è: ä¸è¦padåˆ°å›ºå®šé•¿åº¦
pad_to_sequence_len: false

# è®­ç»ƒå‚æ•°
micro_batch_size: 4
gradient_accumulation_steps: 2
```

### 7.2 é«˜çº§é…ç½®

```yaml
base_model: meta-llama/Llama-3.1-13B
sequence_len: 4096

# Sample Packingè¯¦ç»†é…ç½®
sample_packing: true
sample_packing_eff_est: 0.92  # ä¿å®ˆä¼°è®¡

# Packingæ¨¡å¼é€‰æ‹©
sample_packing_sequentially: false  # false=å¹¶è¡ŒFFD (æ¨è), true=é¡ºåºpacking

# Biné…ç½®
sample_packing_bin_size: 200  # æ¯ä¸ªbinæœ€å¤šå®¹çº³200ä¸ªåºåˆ—
sample_packing_group_size: 100000  # æ¯ç»„å¤„ç†100Kåºåˆ—

# Evaluationä¹Ÿå¯ç”¨packing
eval_sample_packing: true

# Flash Attention (å¿…é¡»)
flash_attention: true

# è®­ç»ƒå‚æ•°
micro_batch_size: 2  # æ¯ä¸ªbinç®—ä¸€ä¸ª"batch"
gradient_accumulation_steps: 8
```

### 7.3 DDP + Sample Packing

```yaml
base_model: meta-llama/Llama-3.1-8B
sequence_len: 2048

# Sample Packing
sample_packing: true
sample_packing_eff_est: 0.95
flash_attention: true

# DDP (é€šè¿‡launcherè‡ªåŠ¨å¯ç”¨)
# torchrun --nproc_per_node=8 train.py

# è®­ç»ƒå‚æ•°
micro_batch_size: 4
gradient_accumulation_steps: 4
# æœ‰æ•ˆbatch size = 4 Ã— 8 (GPUs) Ã— 4 = 128
```

### 7.4 FSDP + Sample Packing

```yaml
base_model: meta-llama/Llama-3.1-13B
sequence_len: 2048

# FSDP-2é…ç½®
fsdp_version: 2
fsdp_config:
  auto_wrap_policy: TRANSFORMER_BASED_WRAP
  transformer_layer_cls_to_wrap: LlamaDecoderLayer
  reshard_after_forward: true

# Sample Packing
sample_packing: true
sample_packing_eff_est: 0.95
flash_attention: true

# è®­ç»ƒå‚æ•°
micro_batch_size: 2
gradient_accumulation_steps: 8
```

### 7.5 TP + DP + Sample Packing

```yaml
base_model: meta-llama/Llama-3.1-70B
sequence_len: 2048

# TPé…ç½®
tensor_parallel_size: 2

# DPä¼šè‡ªåŠ¨è®¾ç½®: 8 GPUs / 2 TP = 4 DP

# Sample Packing
sample_packing: true
sample_packing_eff_est: 0.95
flash_attention: true

# è®­ç»ƒå‚æ•°
micro_batch_size: 2  # æ¯ä¸ªDP rank
gradient_accumulation_steps: 4
# æœ‰æ•ˆbatch size = 2 Ã— 4 (DP) Ã— 4 = 32
```

### 7.6 é¢„è®­ç»ƒé…ç½®ï¼ˆè¶…é«˜æ•ˆï¼‰

```yaml
base_model: meta-llama/Llama-3.1-8B
sequence_len: 4096

# é¢„è®­ç»ƒæ¨¡å¼
pretraining_dataset: path/to/pretrain/data

# Sample Packing (é¢„è®­ç»ƒæ”¶ç›Šæœ€å¤§!)
sample_packing: true
sample_packing_eff_est: 0.98  # é¢„è®­ç»ƒé€šå¸¸æ•ˆç‡æ›´é«˜
flash_attention: true
pad_to_sequence_len: false

# é¢„è®­ç»ƒç‰¹æœ‰: multipack attention
pretrain_multipack_attn: true

# å¤§batchè®­ç»ƒ
micro_batch_size: 8
gradient_accumulation_steps: 16
# æœ‰æ•ˆbatch size = 8 Ã— 8 (GPUs) Ã— 16 = 1024

# å­¦ä¹ ç‡
learning_rate: 3e-4
lr_scheduler: cosine
warmup_steps: 2000
```

---

## 8. æœ€ä½³å®è·µ

### 8.1 ä½•æ—¶å¯ç”¨ Sample Packing

**âœ… å¼ºçƒˆæ¨èçš„åœºæ™¯**ï¼š

1. **é¢„è®­ç»ƒ**ï¼š
   - æ•°æ®é›†é€šå¸¸é•¿åº¦åˆ†å¸ƒå¾ˆå¹¿
   - æ”¶ç›Šæœ€å¤§ï¼ˆ90%+ æ•ˆç‡æå‡ï¼‰
   - é…ç½®: `sample_packing: true`

2. **æŒ‡ä»¤å¾®è°ƒ (SFT)**ï¼š
   - æŒ‡ä»¤é•¿åº¦å·®å¼‚å¤§ï¼ˆ10 tokens - 2000 tokensï¼‰
   - å…¸å‹æ”¶ç›Š: 2-3å€åŠ é€Ÿ
   - é…ç½®: `sample_packing: true`

3. **å¯¹è¯æ•°æ®**ï¼š
   - å¯¹è¯è½®æ¬¡ä¸åŒå¯¼è‡´é•¿åº¦å·®å¼‚
   - æ”¶ç›Š: 2-3å€
   - é…ç½®: `sample_packing: true`

4. **æ··åˆæ•°æ®é›†**ï¼š
   - ä¸åŒæ¥æºæ•°æ®é•¿åº¦å·®å¼‚å¤§
   - æ”¶ç›Š: 2-4å€
   - é…ç½®: `sample_packing: true`

**âŒ ä¸æ¨èçš„åœºæ™¯**ï¼š

1. **åºåˆ—é•¿åº¦å‡åŒ€**ï¼š
   ```yaml
   # å¦‚æœ90%çš„åºåˆ—é•¿åº¦åœ¨1800-2048ä¹‹é—´
   sequence_len: 2048
   sample_packing: false  # æ”¶ç›Šæœ‰é™(<10%)
   ```

2. **è¶…é•¿åºåˆ—è®­ç»ƒ**ï¼š
   ```yaml
   # æ‰€æœ‰åºåˆ—éƒ½æ¥è¿‘sequence_len
   sequence_len: 32768
   sample_packing: false  # æ‰“åŒ…ç©ºé—´å¾ˆå°
   # è€ƒè™‘ä½¿ç”¨CP instead
   context_parallel_size: 2
   ```

3. **æ¨¡å‹ä¸æ”¯æŒ**ï¼š
   ```python
   # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨æ”¯æŒåˆ—è¡¨ä¸­
   from axolotl.monkeypatch.multipack import SUPPORTED_MULTIPACK_MODEL_TYPES

   if model_type not in SUPPORTED_MULTIPACK_MODEL_TYPES:
       # ä¸å¯ç”¨sample_packing
       # æˆ–ä½¿ç”¨V2 collator (æ›´å¹¿æ³›å…¼å®¹)
   ```

### 8.2 é…ç½®è°ƒä¼˜

#### ä¼°è®¡æ‰“åŒ…æ•ˆç‡

```python
# 1. å…ˆè¿è¡Œä¸€æ¬¡ç”Ÿæˆç»Ÿè®¡
sample_packing: true
# (ä¸è®¾ç½®sample_packing_eff_est)

# æŸ¥çœ‹æ—¥å¿—:
# [INFO] Sample packing efficiency: 0.923

# 2. ä½¿ç”¨è¯¥å€¼è°ƒæ•´batch size
sample_packing_eff_est: 0.92
micro_batch_size: 4  # å¯èƒ½éœ€è¦è°ƒå°ï¼Œå› ä¸ºå®é™…tokensæ›´å¤š
```

#### é€‰æ‹© Packing æ¨¡å¼

```yaml
# Parallel Packing (é»˜è®¤ï¼Œæ¨è)
sample_packing_sequentially: false
# ä¼˜ç‚¹: æ‰“åŒ…æ•ˆç‡é«˜ (FFDç®—æ³•)
# ç¼ºç‚¹: ä¸ä¿æŒæ•°æ®é¡ºåº
# é€‚ç”¨: å¤§éƒ¨åˆ†åœºæ™¯

# Sequential Packing
sample_packing_sequentially: true
# ä¼˜ç‚¹: ä¿æŒåŸå§‹æ•°æ®é¡ºåº
# ç¼ºç‚¹: æ‰“åŒ…æ•ˆç‡ç•¥ä½
# é€‚ç”¨: curriculum learning, é¡ºåºæ•æ„Ÿçš„è®­ç»ƒ
```

#### Bin Size è°ƒä¼˜

```yaml
# é»˜è®¤å€¼é€šå¸¸è¶³å¤Ÿ
sample_packing_bin_size: 200

# å°bin_size (< 100):
# - æ›´å¿«çš„packingé€Ÿåº¦
# - å¯èƒ½æµªè´¹ç©ºé—´ (æå‰æ»¡å‘˜)

# å¤§bin_size (> 500):
# - æ›´é«˜çš„æ‰“åŒ…æ•ˆç‡
# - Packingé€Ÿåº¦æ…¢
# - æŸäº›åœºæ™¯ä¸‹å¯èƒ½OOM

# æ¨è: ä¿æŒé»˜è®¤å€¼ï¼Œé™¤éæœ‰ç‰¹æ®Šéœ€æ±‚
```

### 8.3 å¸¸è§é—®é¢˜æ’æŸ¥

#### é—®é¢˜1: OOM (Out of Memory)

```yaml
# åŸå› : Sample Packingæé«˜äº†tokenåˆ©ç”¨ç‡ï¼Œå®é™…è®¡ç®—é‡å¢åŠ 

# è§£å†³æ–¹æ¡ˆ:
# 1. å‡å°micro_batch_size
micro_batch_size: 2  # ä»4é™åˆ°2

# 2. æˆ–é™ä½æ‰“åŒ…æ•ˆç‡ä¼°è®¡
sample_packing_eff_est: 0.8  # ä¿å®ˆä¼°è®¡

# 3. å¯ç”¨gradient checkpointing
gradient_checkpointing: true
```

#### é—®é¢˜2: æ‰“åŒ…æ•ˆç‡ä½

```bash
# æŸ¥çœ‹æ—¥å¿—
[INFO] Sample packing efficiency: 0.65  # < 0.8 å°±ç®—ä½

# å¯èƒ½åŸå› :
# 1. åºåˆ—é•¿åº¦åˆ†å¸ƒä¸å‡
# 2. bin_sizeå¤ªå°
# 3. ä½¿ç”¨äº†sequential packing

# è§£å†³:
# 1. æ£€æŸ¥æ•°æ®é›†
python -c "
from datasets import load_dataset
ds = load_dataset('your_dataset')
lengths = [len(x['input_ids']) for x in ds['train']]
import matplotlib.pyplot as plt
plt.hist(lengths, bins=50)
plt.show()
"

# 2. å¢å¤§bin_size
sample_packing_bin_size: 500

# 3. ä½¿ç”¨parallel packing
sample_packing_sequentially: false
```

#### é—®é¢˜3: è®­ç»ƒä¸ç¨³å®š

```yaml
# å¯èƒ½åŸå› : batchå†…tokenæ•°é‡æ³¢åŠ¨å¤§

# è§£å†³: ä½¿ç”¨multipack_real_batches
multipack_real_batches: false  # é»˜è®¤false
# false: æ¯ä¸ªbinç®—ä¸€ä¸ª"sample" (æ¨è)
# true: æ¯ä¸ªsequenceç®—ä¸€ä¸ª"sample" (æ›´ç¨³å®šä½†æ…¢)

# æˆ–ä½¿ç”¨æ›´ä¿å®ˆçš„æ‰“åŒ…
sample_packing_eff_est: 0.85  # é™ä½åˆ°0.85
```

#### é—®é¢˜4: Eval æ—¶OOM

```yaml
# Evalé€šå¸¸ä¸éœ€è¦Sample Packing
eval_sample_packing: false  # å…³é—­eval packing

# æˆ–å‡å°eval batch size
eval_batch_size: 2
```

### 8.4 æ€§èƒ½ä¼˜åŒ–å»ºè®®

```yaml
# âœ… æ¨èçš„å®Œæ•´é…ç½®

base_model: meta-llama/Llama-3.1-13B
sequence_len: 2048

# Sample Packing
sample_packing: true
sample_packing_eff_est: 0.95
sample_packing_sequentially: false
sample_packing_bin_size: 200
eval_sample_packing: true  # å¦‚æœevalæ•°æ®ä¹Ÿé•¿åº¦ä¸å‡

# Attentionä¼˜åŒ–
flash_attention: true  # å¿…é¡»!
pad_to_sequence_len: false  # å…³é”®!

# å†…å­˜ä¼˜åŒ–
gradient_checkpointing: true
bf16: true  # æˆ–fp16

# FSDP-2 (å¦‚æœæ¨¡å‹è¾ƒå¤§)
fsdp_version: 2
fsdp_config:
  reshard_after_forward: true

# è®­ç»ƒå‚æ•°
micro_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 2e-5
lr_scheduler: cosine
warmup_steps: 100

# Logging
logging_steps: 10
eval_steps: 500
save_steps: 1000

# é¢„æœŸæ”¶ç›Š:
# - Throughputæå‡: 2-3å€
# - è®­ç»ƒæ—¶é—´å‡å°‘: 50-60%
# - æˆæœ¬èŠ‚çœ: 50-60%
```

### 8.5 éªŒè¯ Sample Packing æ˜¯å¦ç”Ÿæ•ˆ

```python
# æ–¹æ³•1: æ£€æŸ¥æ—¥å¿—
# è®­ç»ƒå¼€å§‹æ—¶åº”è¯¥çœ‹åˆ°:
[INFO] Sample packing efficiency: 0.XXX
[INFO] MultipackBatchSampler: using parallel packing
[INFO] Total bins: XXXX

# æ–¹æ³•2: æ£€æŸ¥batch shape
# åœ¨trainer callbackä¸­æ‰“å°:
def on_step_begin(self, args, state, control, **kwargs):
    batch = kwargs['inputs']
    print(f"Batch shape: {batch['input_ids'].shape}")
    print(f"Attention mask unique values: {batch['attention_mask'].unique()}")
    # Sample Packing enabledåº”è¯¥çœ‹åˆ°:
    # Batch shape: torch.Size([num_bins, varying_length])
    # Attention mask unique values: tensor([0, 1, 2, 3, ...])
    #                                      â†‘ åºåˆ—IDs

# æ–¹æ³•3: ç›‘æ§GPUåˆ©ç”¨ç‡
# Sample Packing enabled: GPUåˆ©ç”¨ç‡åº”è¯¥æ˜¾è‘—æå‡
nvidia-smi dmon -s u
# ä» 40-50% â†’ 80-90%+
```

---

## æ€»ç»“

### Sample Packing çš„æ ¸å¿ƒä»·å€¼

1. **å‡å°‘Paddingæµªè´¹**ï¼š
   - ä» 70-80% æµªè´¹ â†’ 5-10% æµªè´¹
   - GPU åˆ©ç”¨ç‡æå‡ 2-3 å€

2. **åŠ é€Ÿè®­ç»ƒ**ï¼š
   - Throughput æå‡ 2-3 å€
   - è®­ç»ƒæ—¶é—´å‡å°‘ 50-60%
   - æˆæœ¬èŠ‚çœ 50-60%

3. **ä¸å¹¶è¡Œç­–ç•¥å…¼å®¹**ï¼š
   - âœ… DDP: å®Œç¾å…¼å®¹
   - âœ… FSDP: å®Œç¾å…¼å®¹
   - âœ… TP: å®Œç¾å…¼å®¹
   - âš ï¸ CP: éœ€è¦æ³¨æ„ï¼Œæ¨èåˆ†å¼€ä½¿ç”¨

4. **ä¸å½±å“æ”¶æ•›æ€§**ï¼š
   - Attention mask æ­£ç¡®éš”ç¦»åºåˆ—
   - æ¢¯åº¦è®¡ç®—ç­‰ä»·äºé packing
   - å®éªŒéªŒè¯æ”¶æ•›æ€§ç›¸åŒ

### ä½¿ç”¨å»ºè®®

```
æ–°é¡¹ç›®ï¼Ÿ
â””â”€ âœ… å¯ç”¨ Sample Packing

åºåˆ—é•¿åº¦å·®å¼‚å¤§ï¼Ÿ
â””â”€ âœ… å¯ç”¨ Sample Packing (æ”¶ç›Šæœ€å¤§)

åºåˆ—é•¿åº¦å‡åŒ€ï¼Ÿ
â””â”€ âš ï¸ è¯„ä¼°æ”¶ç›Šï¼Œå¯èƒ½ä¸éœ€è¦

è¶…é•¿åºåˆ—è®­ç»ƒï¼Ÿ
â””â”€ âŒ è€ƒè™‘ä½¿ç”¨ CP instead

æ¨¡å‹æ”¯æŒï¼Ÿ
â””â”€ æ£€æŸ¥ SUPPORTED_MULTIPACK_MODEL_TYPES

å·²æœ‰é¡¹ç›®è¿ç§»ï¼Ÿ
â””â”€ âœ… ä½é£é™©ï¼Œå»ºè®®å¯ç”¨å¹¶ç›‘æ§
```

---

## ç›¸å…³æ–‡æ¡£

- [Sample Packing æºç è§£æ](./sample_packing_source_walkthrough.md)
- [Sample Packing å¿«é€Ÿå‚è€ƒ](./sample_packing_quick_reference.md)
- [Data Parallelism æ·±åº¦è§£æ](./data_parallelism_deep_dive.md)
- [Tensor Parallelism æ·±åº¦è§£æ](./tensor_parallelism_deep_dive.md)
- [Context Parallelism æ·±åº¦è§£æ](./context_parallelism_deep_dive.md)
- [ä¸»ç´¢å¼•](./README.md)

---

*æ–‡æ¡£ç‰ˆæœ¬: v1.0 | æœ€åæ›´æ–°: 2025-11*

# Axolotl æ¡†æ¶ä¸­çš„ TiledMLP æ·±åº¦è§£æ

> æœ¬æ–‡æ¡£é¢å‘ infra åˆå­¦è€…ï¼Œé€šä¿—æ˜“æ‡‚åœ°è®²è§£ Axolotl å¦‚ä½•å®ç° TiledMLP

## ç›®å½•

1. [ä»€ä¹ˆæ˜¯ TiledMLPï¼Ÿ](#1-ä»€ä¹ˆæ˜¯-tiledmlp)
2. [ä¸ºä»€ä¹ˆéœ€è¦ TiledMLPï¼Ÿ](#2-ä¸ºä»€ä¹ˆéœ€è¦-tiledmlp)
3. [TiledMLP çš„å·¥ä½œåŸç†](#3-tiledmlp-çš„å·¥ä½œåŸç†)
4. [Axolotl ä¸­çš„å®ç°](#4-axolotl-ä¸­çš„å®ç°)
5. [æºç å®ç°åˆ†æ](#5-æºç å®ç°åˆ†æ)
6. [å®æˆ˜ç¤ºä¾‹ï¼šALST é•¿ä¸Šä¸‹æ–‡è®­ç»ƒ](#6-å®æˆ˜ç¤ºä¾‹alst-é•¿ä¸Šä¸‹æ–‡è®­ç»ƒ)
7. [å¸¸è§é—®é¢˜ä¸æœ€ä½³å®è·µ](#7-å¸¸è§é—®é¢˜ä¸æœ€ä½³å®è·µ)

---

## 1. ä»€ä¹ˆæ˜¯ TiledMLPï¼Ÿ

### 1.1 ç”¨ä¸€ä¸ªæ¯”å–»æ¥ç†è§£

å›å¿†ä¸€ä¸‹ Tensor Parallelism çš„æ¯”å–»ï¼šå¤šä¸ªäººä¸€èµ·æ¬**åŒä¸€å¼ æ¡Œå­çš„ä¸åŒéƒ¨åˆ†**ã€‚

ç°åœ¨ TiledMLP æ˜¯è¿™æ ·çš„ï¼š

æƒ³è±¡ä½ è¦æ¬ä¸€åˆ—**è¶…çº§é•¿**çš„æ¡Œå­ï¼ˆåƒå®´ä¼šæ¡Œé‚£ç§ï¼‰ï¼Œä½†æ˜¯ä½ çš„åŠ›æ°”æœ‰é™ï¼š
- **æ™®é€šæ–¹æ³•**ï¼šä¸€æ¬¡æ€§æ¬æ•´å¼ æ¡Œå­ â†’ å¤ªé‡äº†ï¼Œè…°ä¼šé—ªäº†ï¼ˆæ˜¾å­˜çˆ†ç‚¸ï¼‰
- **TiledMLP**ï¼šæŠŠæ¡Œå­åˆ‡æˆå¤šæ®µï¼Œ**ä¸€æ®µä¸€æ®µåœ°æ¬** â†’ æ¯æ¬¡åªæ¬ä¸€å°æ®µï¼ŒçœåŠ›æ°”ï¼ˆçœæ˜¾å­˜ï¼‰

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼š
- **è¾“å…¥åºåˆ—**å°±åƒè¿™å¼ è¶…é•¿çš„æ¡Œå­
- **TiledMLP å°†åºåˆ—åˆ‡åˆ†æˆå¤šä¸ªå°å—ï¼ˆtiles/shardsï¼‰**
- **é€å—è®¡ç®— MLP å±‚**ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§è®¡ç®—æ•´ä¸ªåºåˆ—
- è¿™æ ·å¯ä»¥å¤§å¹…é™ä½**æ¿€æ´»å€¼**çš„æ˜¾å­˜å ç”¨

### 1.2 æŠ€æœ¯å®šä¹‰

TiledMLPï¼ˆå¹³é“º MLPï¼‰æ˜¯ä¸€ç§**æ¿€æ´»å€¼é‡è®¡ç®—**ï¼ˆActivation Recomputationï¼‰æŠ€æœ¯ï¼Œé€šè¿‡åœ¨**åºåˆ—ç»´åº¦**ä¸Šåˆ‡åˆ†è¾“å…¥æ•°æ®ï¼Œé€å—è®¡ç®— MLP å±‚çš„å‰å‘å’Œåå‘ä¼ æ’­ï¼Œä»è€Œé™ä½å³°å€¼æ˜¾å­˜å ç”¨ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šæ¥è‡ª [ALST è®ºæ–‡](https://www.arxiv.org/abs/2506.13996) (Arctic Long Sequence Training)
- å°†è¾“å…¥åºåˆ—åˆ‡åˆ†æˆå¤šä¸ª tileï¼ˆç“¦ç‰‡ï¼‰
- å‰å‘ä¼ æ’­ï¼šé€ tile è®¡ç®—ï¼Œä¸ä¿å­˜ä¸­é—´æ¿€æ´»å€¼
- åå‘ä¼ æ’­ï¼šé‡æ–°è®¡ç®—æ¯ä¸ª tile çš„æ¿€æ´»å€¼ï¼ˆrecomputationï¼‰
- æ¢¯åº¦ç´¯åŠ ï¼šå°†å¤šä¸ª tile çš„æ¢¯åº¦ç´¯åŠ åæ›´æ–°å‚æ•°

**å…³é”®åŒºåˆ«**ï¼š
| ç»´åº¦ | Tensor Parallelism | TiledMLP |
|------|-------------------|----------|
| **åˆ‡åˆ†å¯¹è±¡** | æ¨¡å‹æƒé‡çŸ©é˜µ | è¾“å…¥åºåˆ— |
| **è®¡ç®—æ–¹å¼** | å¤š GPU å¹¶è¡Œè®¡ç®— | å• GPU é¡ºåºè®¡ç®— |
| **èŠ‚çœå†…å®¹** | å‚æ•°æ˜¾å­˜ | æ¿€æ´»å€¼æ˜¾å­˜ |
| **é€šä¿¡å¼€é”€** | é«˜ï¼ˆæ¯å±‚ All-Reduceï¼‰ | æ— ï¼ˆå•å¡è®¡ç®—ï¼‰ |

---

## 2. ä¸ºä»€ä¹ˆéœ€è¦ TiledMLPï¼Ÿ

### 2.1 é•¿ä¸Šä¸‹æ–‡è®­ç»ƒçš„æ˜¾å­˜ç“¶é¢ˆ

åœ¨è®­ç»ƒè¶…é•¿ä¸Šä¸‹æ–‡çš„å¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œ**æ¿€æ´»å€¼**æ˜¯æœ€å¤§çš„æ˜¾å­˜æ€æ‰‹ï¼š

```
ä¾‹å¦‚ï¼šLlama-8B æ¨¡å‹ï¼Œè®­ç»ƒ 500K tokens è¶…é•¿ä¸Šä¸‹æ–‡
- åºåˆ—é•¿åº¦ (L)ï¼š500,000
- éšè—ç»´åº¦ (H)ï¼š4096
- MLP ä¸­é—´ç»´åº¦ (I)ï¼š14,336 (é€šå¸¸æ˜¯ H çš„ 3.5 å€)
- Batch sizeï¼š1

å•ä¸ª MLP å±‚çš„æ¿€æ´»å€¼æ˜¾å­˜ï¼š
1. Gate æŠ•å½±è¾“å‡ºï¼š1 Ã— 500,000 Ã— 14,336 Ã— 2 bytes (bf16) = 14.3 GB
2. Up æŠ•å½±è¾“å‡ºï¼š  1 Ã— 500,000 Ã— 14,336 Ã— 2 bytes         = 14.3 GB
3. æ¿€æ´»å‡½æ•°è¾“å‡ºï¼š  1 Ã— 500,000 Ã— 14,336 Ã— 2 bytes         = 14.3 GB
4. Down æŠ•å½±è¾“å…¥ï¼š 1 Ã— 500,000 Ã— 14,336 Ã— 2 bytes         = 14.3 GB
--------------------------------------------------------------
å•å±‚ MLP æ€»è®¡ï¼š                                              57.2 GB ï¼

32 å±‚ Llama-8B çš„æ‰€æœ‰ MLP å±‚ï¼š
32 Ã— 57.2 GB = 1830 GB (1.8 TB) ï¼ï¼ï¼
```

**é—®é¢˜**ï¼šå³ä½¿æ˜¯ 8Ã—A100 (80GB)ï¼Œä¹Ÿåªæœ‰ 640GB æ˜¾å­˜ï¼Œæ ¹æœ¬è£…ä¸ä¸‹ï¼

### 2.2 ä¼ ç»Ÿè§£å†³æ–¹æ¡ˆçš„å±€é™æ€§

#### æ–¹æ¡ˆ 1ï¼šGradient Checkpointingï¼ˆæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰
```yaml
gradient_checkpointing: true

æ•ˆæœï¼š
- é™ä½æ¿€æ´»å€¼æ˜¾å­˜ï¼ˆåªä¿å­˜éƒ¨åˆ†å±‚çš„æ¿€æ´»å€¼ï¼‰
- ä½†ä»éœ€ä¿å­˜ checkpointed å±‚çš„æ¿€æ´»å€¼
- å¯¹äº 500K é•¿åº¦ï¼Œä»ç„¶ä¸å¤Ÿ
```

#### æ–¹æ¡ˆ 2ï¼šSequence Parallelismï¼ˆåºåˆ—å¹¶è¡Œï¼‰
```yaml
context_parallel_size: 8  # å°†åºåˆ—åˆ‡ 8 ä»½ï¼Œåˆ†åˆ° 8 ä¸ª GPU

æ•ˆæœï¼š
- æ¯ä¸ª GPU åªå¤„ç† 500K / 8 = 62.5K tokens
- æ¿€æ´»å€¼æ˜¾å­˜ï¼š1.8 TB / 8 = 225 GB / GPU
- ä»ç„¶è¶…è¿‡å•å¡ 80GB æ˜¾å­˜ï¼
```

#### æ–¹æ¡ˆ 3ï¼šActivation Offloadingï¼ˆæ¿€æ´»å€¼å¸è½½ï¼‰
```yaml
activation_offloading: legacy

æ•ˆæœï¼š
- å°†æ¿€æ´»å€¼å¸è½½åˆ° CPU RAM
- åå‘ä¼ æ’­æ—¶å†æ‹·è´å› GPU
- ä½† CPU-GPU ä¼ è¾“æ…¢ï¼Œè®­ç»ƒé€Ÿåº¦ä¸‹é™ä¸¥é‡
```

### 2.3 TiledMLP çš„ä¼˜åŠ¿

TiledMLP æä¾›äº†ä¸€ç§**æ—¶é—´æ¢ç©ºé—´**çš„è§£å†³æ–¹æ¡ˆï¼š

```
TiledMLP æ•ˆæœï¼ˆé…åˆ Sequence Parallelismï¼‰ï¼š
context_parallel_size: 8
tiled_mlp: true
tiled_mlp_num_shards: 4  # æ¯ä¸ª MLP å±‚å†åˆ‡ 4 ä¸ª tile

æ¯ä¸ª GPU å¤„ç†çš„åºåˆ—é•¿åº¦ï¼š500K / 8 = 62.5K
æ¯ä¸ª tile çš„é•¿åº¦ï¼š62.5K / 4 = 15.6K

å•ä¸ª MLP å±‚æ¿€æ´»å€¼ï¼ˆæ¯æ¬¡åªå­˜ä¸€ä¸ª tileï¼‰ï¼š
1 Ã— 15,625 Ã— 14,336 Ã— 2 bytes Ã— 4 = 3.6 GB

ç›¸æ¯”åŸæ¥çš„ 57.2 GBï¼š
57.2 / 3.6 = 16 å€æ˜¾å­˜èŠ‚çœï¼

æ€»æ¿€æ´»å€¼æ˜¾å­˜ï¼ˆ32 å±‚ï¼‰ï¼š
32 Ã— 3.6 GB = 115 GB
é…åˆ CP=8ï¼š115 / 8 = 14.4 GB / GPU âœ…
```

**ç»¼åˆä¼˜åŠ¿**ï¼š
- âœ… **å¤§å¹…é™ä½æ¿€æ´»å€¼æ˜¾å­˜**ï¼ˆ16 å€èŠ‚çœï¼‰
- âœ… **æ— éœ€è·¨ GPU é€šä¿¡**ï¼ˆå•å¡é¡ºåºè®¡ç®—ï¼‰
- âœ… **å¯ä¸ Sequence Parallelism ç»„åˆ**ï¼ˆè¿›ä¸€æ­¥é™ä½æ˜¾å­˜ï¼‰
- âœ… **æ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡**ï¼ˆ500K+ tokensï¼‰
- âš ï¸ **ä»£ä»·**ï¼šå¢åŠ çº¦ 30-50% è®¡ç®—æ—¶é—´ï¼ˆéœ€è¦é‡è®¡ç®—æ¿€æ´»å€¼ï¼‰

---

## 3. TiledMLP çš„å·¥ä½œåŸç†

### 3.1 æ ¸å¿ƒæ•°å­¦åŸç†

ä»¥ Llama çš„ MLP å±‚ä¸ºä¾‹ï¼ˆSwiGLU ç»“æ„ï¼‰ï¼š

```python
# åŸå§‹ MLP è®¡ç®—
def mlp_forward(x):
    """
    x: [batch, seq_len, hidden_dim]
    """
    gate = gate_proj(x)      # [batch, seq_len, intermediate_dim]
    up = up_proj(x)          # [batch, seq_len, intermediate_dim]
    activation = SiLU(gate)  # [batch, seq_len, intermediate_dim]
    combined = activation * up  # [batch, seq_len, intermediate_dim]
    output = down_proj(combined)  # [batch, seq_len, hidden_dim]
    return output
```

**å…³é”®è§‚å¯Ÿ**ï¼š
- MLP è®¡ç®—åœ¨**åºåˆ—ç»´åº¦ä¸Šæ˜¯ç‹¬ç«‹çš„**
- åºåˆ—çš„ç¬¬ i ä¸ª token çš„è®¡ç®—ä¸ä¾èµ–ç¬¬ j ä¸ª token
- è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥**åˆ‡åˆ†åºåˆ—ï¼Œé€å—è®¡ç®—**

### 3.2 TiledMLP çš„å‰å‘ä¼ æ’­

```python
# TiledMLP å‰å‘ä¼ æ’­ä¼ªä»£ç 
def tiled_mlp_forward(x, num_shards=4):
    """
    x: [batch, seq_len, hidden_dim]
    num_shards: åˆ‡åˆ†çš„å—æ•°
    """
    # 1. å°†è¾“å…¥åºåˆ—åˆ‡åˆ†æˆå¤šä¸ª tile
    x_shards = torch.chunk(x, chunks=num_shards, dim=1)
    # x_shards[0]: [batch, seq_len/4, hidden_dim]
    # x_shards[1]: [batch, seq_len/4, hidden_dim]
    # ...

    # 2. é€å—è®¡ç®— MLPï¼ˆä¸ä¿å­˜ä¸­é—´æ¿€æ´»å€¼ï¼‰
    output_shards = []
    with torch.no_grad():  # â† å…³é”®ï¼šä¸ä¿å­˜æ¢¯åº¦ä¿¡æ¯ï¼
        for x_shard in x_shards:
            output_shard = mlp_forward(x_shard)  # è®¡ç®—å½“å‰ tile
            output_shards.append(output_shard)

    # 3. æ‹¼æ¥è¾“å‡º
    output = torch.cat(output_shards, dim=1)  # [batch, seq_len, hidden_dim]
    return output
```

**å…³é”®ç‚¹**ï¼š
1. âœ… æ¯æ¬¡åªè®¡ç®—ä¸€ä¸ª shardï¼Œå³°å€¼æ˜¾å­˜ = åŸæ¥çš„ 1/num_shards
2. âœ… ä½¿ç”¨ `torch.no_grad()` ä¸ä¿å­˜æ¿€æ´»å€¼ï¼Œè¿›ä¸€æ­¥é™ä½æ˜¾å­˜
3. âš ï¸ ä½†è¿™æ ·åå‘ä¼ æ’­æ— æ³•è®¡ç®—æ¢¯åº¦ï¼ˆæ¿€æ´»å€¼å·²ä¸¢å¤±ï¼‰

### 3.3 TiledMLP çš„åå‘ä¼ æ’­

ç”±äºå‰å‘ä¼ æ’­ä¸¢å¼ƒäº†æ¿€æ´»å€¼ï¼Œåå‘ä¼ æ’­éœ€è¦**é‡æ–°è®¡ç®—**ï¼š

```python
# TiledMLP åå‘ä¼ æ’­ä¼ªä»£ç 
def tiled_mlp_backward(x, incoming_grad, num_shards=4):
    """
    x: [batch, seq_len, hidden_dim] - è¾“å…¥ï¼ˆå·²ä¿å­˜ï¼‰
    incoming_grad: [batch, seq_len, hidden_dim] - æ¥è‡ªä¸‹æ¸¸çš„æ¢¯åº¦
    """
    # 1. åˆ‡åˆ†è¾“å…¥å’Œæ¢¯åº¦
    x_shards = torch.chunk(x, chunks=num_shards, dim=1)
    grad_shards = torch.chunk(incoming_grad, chunks=num_shards, dim=1)

    x_grad = torch.zeros_like(x)  # è¾“å…¥çš„æ¢¯åº¦
    param_grads = {}  # å‚æ•°çš„æ¢¯åº¦ç´¯åŠ å™¨

    # 2. é€å—é‡æ–°è®¡ç®—å‰å‘ + åå‘ä¼ æ’­
    for i, (x_shard, grad_shard) in enumerate(zip(x_shards, grad_shards)):
        x_shard.requires_grad_(True)

        # é‡æ–°è®¡ç®—å‰å‘ä¼ æ’­ï¼ˆRecomputationï¼‰
        with torch.enable_grad():
            output_shard = mlp_forward(x_shard)

        # åå‘ä¼ æ’­ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰
        torch.autograd.backward(output_shard, grad_shard)

        # ç´¯åŠ å‚æ•°æ¢¯åº¦
        for name, param in mlp.named_parameters():
            if param.grad is not None:
                if name not in param_grads:
                    param_grads[name] = param.grad.clone()
                else:
                    param_grads[name] += param.grad  # â† ç´¯åŠ æ¢¯åº¦
                param.grad = None  # æ¸…ç©ºï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ª shard

        # ä¿å­˜è¾“å…¥æ¢¯åº¦
        x_grad[:, i*shard_len:(i+1)*shard_len, :] = x_shard.grad

    # 3. å°†ç´¯åŠ çš„æ¢¯åº¦èµ‹å€¼ç»™å‚æ•°ï¼ˆåªåœ¨æœ€åä¸€ä¸ª shardï¼‰
    for name, param in mlp.named_parameters():
        param.grad = param_grads[name]

    return x_grad
```

**å…³é”®æœºåˆ¶**ï¼š
1. **Recomputationï¼ˆé‡è®¡ç®—ï¼‰**ï¼šæ¯ä¸ª shard é‡æ–°æ‰§è¡Œå‰å‘ä¼ æ’­
2. **Gradient Accumulationï¼ˆæ¢¯åº¦ç´¯åŠ ï¼‰**ï¼šå¤šä¸ª shard çš„æ¢¯åº¦æ±‚å’Œ
3. **å»¶è¿Ÿæ¢¯åº¦èµ‹å€¼**ï¼šåªåœ¨æœ€åä¸€ä¸ª shard æ›´æ–° `param.grad`

**æ—¶é—´ vs æ˜¾å­˜çš„æƒè¡¡**ï¼š
```
å‡è®¾åŸå§‹ MLP å‰å‘ä¼ æ’­è€—æ—¶ Tï¼š

TiledMLP (num_shards=4)ï¼š
- å‰å‘ä¼ æ’­ï¼š4Tï¼ˆé‡å¤è®¡ç®— 4 æ¬¡ï¼‰
- åå‘ä¼ æ’­ï¼š4Tï¼ˆé‡å¤è®¡ç®— 4 æ¬¡ï¼‰
- æ€»æ—¶é—´ï¼š8T vs åŸæ¥çš„ 2Tï¼ˆå‰å‘+åå‘ï¼‰
- æ—¶é—´å¢åŠ ï¼š300%

ä½†æ˜¾å­˜é™ä½ï¼š
- æ¿€æ´»å€¼ï¼š1/4
- å‚æ•°æ¢¯åº¦ï¼šä¸å˜ï¼ˆæœ€åç´¯åŠ ï¼‰
- æ€»æ˜¾å­˜ï¼šçº¦ 1/4
```

### 3.4 å®Œæ•´çš„è®¡ç®—æµç¨‹å›¾

```
è¾“å…¥: x [batch=1, seq_len=100K, hidden=4096]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              å‰å‘ä¼ æ’­ (Forward Pass)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. åˆ‡åˆ†è¾“å…¥åºåˆ—ï¼ˆnum_shards=4ï¼‰:
   x --> [x1: 25K, x2: 25K, x3: 25K, x4: 25K]

2. é€å—è®¡ç®— MLPï¼ˆæ— æ¢¯åº¦ï¼‰:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ x1 25K   â”‚ --> Gate/Up --> SiLU --> Down --> y1 (ä¸¢å¼ƒæ¿€æ´»å€¼)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ x2 25K   â”‚ --> Gate/Up --> SiLU --> Down --> y2 (ä¸¢å¼ƒæ¿€æ´»å€¼)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ x3 25K   â”‚ --> Gate/Up --> SiLU --> Down --> y3 (ä¸¢å¼ƒæ¿€æ´»å€¼)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ x4 25K   â”‚ --> Gate/Up --> SiLU --> Down --> y4 (ä¸¢å¼ƒæ¿€æ´»å€¼)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. æ‹¼æ¥è¾“å‡º:
   y = [y1 | y2 | y3 | y4]  [100K, 4096]

å³°å€¼æ˜¾å­˜: åªéœ€å­˜å‚¨å•ä¸ª shard çš„æ¿€æ´»å€¼ï¼ˆ25Kï¼‰

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              åå‘ä¼ æ’­ (Backward Pass)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¾“å…¥æ¢¯åº¦: dy [100K, 4096]
åˆ‡åˆ†: [dy1: 25K, dy2: 25K, dy3: 25K, dy4: 25K]

é€å—é‡è®¡ç®— + åå‘ä¼ æ’­:

Shard 1:
   x1 --> Forward --> y1 --> Backward(dy1) -->
      â”œâ”€ dx1 (è¾“å…¥æ¢¯åº¦)
      â””â”€ dW1 (å‚æ•°æ¢¯åº¦ï¼Œæš‚å­˜)

Shard 2:
   x2 --> Forward --> y2 --> Backward(dy2) -->
      â”œâ”€ dx2
      â””â”€ dW2 (ç´¯åŠ : dW = dW1 + dW2)

Shard 3:
   x3 --> Forward --> y3 --> Backward(dy3) -->
      â”œâ”€ dx3
      â””â”€ dW3 (ç´¯åŠ : dW = dW + dW3)

Shard 4:
   x4 --> Forward --> y4 --> Backward(dy4) -->
      â”œâ”€ dx4
      â””â”€ dW4 (ç´¯åŠ : dW = dW + dW4ï¼Œæœ€åèµ‹å€¼ç»™ param.grad)

è¾“å‡º:
   dx = [dx1 | dx2 | dx3 | dx4]
   param.grad = dW (æ‰€æœ‰ shard çš„æ¢¯åº¦ä¹‹å’Œ)
```

---

## 4. Axolotl ä¸­çš„å®ç°

Axolotl å®ç°äº†ä¸‰ç§ TiledMLP å˜ä½“ï¼Œé€‚é…ä¸åŒçš„è®­ç»ƒæ¡†æ¶ï¼š

### 4.1 ä¸‰ç§å®ç°æ¨¡å¼

```python
# æ–‡ä»¶ï¼šsrc/axolotl/monkeypatch/tiled_mlp/base.py

# 1. TiledMLP - ç”¨äº FSDP å’Œå• GPU
class TiledMLP(torch.autograd.Function):
    """ä½¿ç”¨æ¢¯åº¦ hooks å®ç°æ¢¯åº¦ç´¯åŠ """
    pass

# 2. DeepSpeedTiledMLPMoE - ç”¨äº DeepSpeed ZeRO-3
class DeepSpeedTiledMLPMoE(torch.autograd.Function):
    """é€šè¿‡ ds_grad_is_ready æ ‡å¿—æ§åˆ¶ DeepSpeed æ¢¯åº¦åŒæ­¥"""
    pass

# 3. DeepSpeedTiledMLP - DeepSpeed å®˜æ–¹å®ç°ï¼ˆå¤–éƒ¨å¯¼å…¥ï¼‰
from deepspeed.runtime.sequence_parallel.ulysses_sp import TiledMLP as DeepSpeedTiledMLP
```

**é€‰æ‹©é€»è¾‘**ï¼ˆ`src/axolotl/monkeypatch/tiled_mlp/patch.py:59-72`ï¼‰ï¼š

```python
def tiled_mlp_forward(self, x):
    # ...

    # è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨å“ªç§å®ç°
    if not self._tiled_mlp_dist_impl:
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ DeepSpeedï¼ˆé€šè¿‡å‚æ•°å±æ€§åˆ¤æ–­ï¼‰
        if any(hasattr(p, "ds_id") for p in self._compute_params):
            if model_type == "gpt_oss":  # MoE æ¨¡å‹
                self._tiled_mlp_dist_impl = DeepSpeedTiledMLPMoE
            else:
                self._tiled_mlp_dist_impl = DeepSpeedTiledMLP  # å®˜æ–¹å®ç°
        else:
            # FSDP æˆ–å• GPU
            self._tiled_mlp_dist_impl = TiledMLP

    # åº”ç”¨ TiledMLP
    output = self._tiled_mlp_dist_impl.apply(
        mlp_forward, self, x, num_shards, compute_params
    )
    return output
```

### 4.2 è‡ªåŠ¨è®¡ç®— Shard æ•°é‡

TiledMLP æ”¯æŒä¸¤ç§æ–¹å¼ç¡®å®šåˆ‡åˆ†æ•°é‡ï¼š

#### æ–¹å¼ 1ï¼šè‡ªåŠ¨è®¡ç®—ï¼ˆé»˜è®¤ï¼‰
```python
# src/axolotl/monkeypatch/tiled_mlp/patch.py:46-51

def tiled_mlp_forward(self, x):
    seqlen = x.shape[-2]     # åºåˆ—é•¿åº¦
    hidden = x.shape[-1]     # éšè—ç»´åº¦

    # å…¬å¼ï¼šnum_shards = ceil(seqlen / hidden)
    num_shards = math.ceil(seqlen / hidden)

    # å¤š GPU æƒ…å†µï¼šå–æ‰€æœ‰ GPU çš„æœ€å¤§å€¼ï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
    if is_distributed:
        num_shards_tensor = torch.tensor(num_shards, device=x.device)
        dist.all_reduce(num_shards_tensor, op=dist.ReduceOp.MAX)
        num_shards = num_shards_tensor.item()
```

**åŸç†**ï¼š
- å½“åºåˆ—é•¿åº¦è¿œå¤§äºéšè—ç»´åº¦æ—¶ï¼Œæ¿€æ´»å€¼æ˜¾å­˜æœ€å¤§
- ä¾‹å¦‚ï¼š`seq_len=100K, hidden=4K` â†’ `num_shards = ceil(100/4) = 25`
- åˆ‡åˆ† 25 ä¸ª shardï¼Œæ¯ä¸ªå¤„ç† 4K tokensï¼ˆå’Œ hidden ç»´åº¦ç›¸å½“ï¼‰

#### æ–¹å¼ 2ï¼šæ‰‹åŠ¨æŒ‡å®š
```yaml
# é…ç½®æ–‡ä»¶
tiled_mlp: true
tiled_mlp_num_shards: 8  # å¼ºåˆ¶ä½¿ç”¨ 8 ä¸ª shard
```

### 4.3 Monkeypatch åº”ç”¨æµç¨‹

TiledMLP é€šè¿‡**çŒ´å­è¡¥ä¸**ï¼ˆMonkeypatchï¼‰åŠ¨æ€æ›¿æ¢æ¨¡å‹çš„ MLP å±‚ï¼š

```python
# æ–‡ä»¶ï¼šsrc/axolotl/monkeypatch/tiled_mlp/patch.py

def patch_tiled_mlp(model_type, use_original_mlp=True, cfg_num_shards=None):
    """
    åŠ¨æ€å¯¼å…¥å¹¶æ›¿æ¢æ¨¡å‹çš„ MLP forward æ–¹æ³•

    Args:
        model_type: æ¨¡å‹ç±»å‹ï¼ˆå¦‚ "llama", "qwen2" ç­‰ï¼‰
        use_original_mlp: æ˜¯å¦ä½¿ç”¨åŸå§‹ MLPï¼ˆæˆ–é€šç”¨å®ç°ï¼‰
        cfg_num_shards: æ‰‹åŠ¨æŒ‡å®š shard æ•°é‡
    """
    # 1. åŠ¨æ€å¯¼å…¥æ¨¡å‹çš„ MLP ç±»
    module_path = f"transformers.models.{model_type}.modeling_{model_type}"
    model_cls_prefix = get_causal_lm_model_cls_prefix(model_type)
    # ä¾‹å¦‚ï¼šLlamaMLP, Qwen2MLP ç­‰
    mlp_cls = getattr(module, f"{model_cls_prefix}MLP")

    # 2. è·å–åŸå§‹ forward æ–¹æ³•
    if use_original_mlp:
        mlp_forward = mlp_cls.forward
    else:
        # ä½¿ç”¨é€šç”¨ MLP å®ç°ï¼ˆé€‚é…æ›´å¤šæ¨¡å‹ï¼‰
        mlp_forward = torch.compile(generic_mlp_forward)

    # 3. æ›¿æ¢ forward æ–¹æ³•ä¸º TiledMLP ç‰ˆæœ¬
    mlp_cls.forward = tiled_mlp_forward
    mlp_cls._compute_params = []  # ç¼“å­˜å¯è®­ç»ƒå‚æ•°
    mlp_cls._tiled_mlp_dist_impl = None  # ç¼“å­˜å®ç°ç±»å‹
```

**åº”ç”¨æ—¶æœº**ï¼ˆ`src/axolotl/loaders/patch_manager.py:74-76`ï¼‰ï¼š

```python
class PatchManager:
    def apply_post_plugin_pre_model_load_patches(self):
        """åœ¨æ’ä»¶åŠ è½½åã€æ¨¡å‹åŠ è½½å‰åº”ç”¨è¡¥ä¸"""
        self._apply_tiled_mlp(self.cfg.model_config_type)

    def _apply_tiled_mlp(self, model_type: str):
        if self.cfg.tiled_mlp:
            from axolotl.monkeypatch.tiled_mlp import patch_tiled_mlp

            patch_tiled_mlp(
                model_type,
                use_original_mlp=self.cfg.tiled_mlp_use_original_mlp,
                cfg_num_shards=self.cfg.tiled_mlp_num_shards,
            )
```

---

## 5. æºç å®ç°åˆ†æ

### 5.1 TiledMLP ç±»ï¼ˆFSDP/å•GPUç‰ˆæœ¬ï¼‰

```python
# æ–‡ä»¶ï¼šsrc/axolotl/monkeypatch/tiled_mlp/base.py:99-189

class TiledMLP(torch.autograd.Function):
    """TiledMLP å®ç°ï¼Œä½¿ç”¨æ¢¯åº¦ hooks ç´¯åŠ æ¢¯åº¦"""

    @staticmethod
    def forward(ctx, fn, self, x, shards, compute_params):
        """
        å‰å‘ä¼ æ’­ï¼šé€ shard è®¡ç®—ï¼Œä¸ä¿å­˜æ¿€æ´»å€¼

        Args:
            fn: MLP çš„ forward æ–¹æ³•
            self: MLP å®ä¾‹
            x: è¾“å…¥ [batch, seq_len, hidden]
            shards: åˆ‡åˆ†æ•°é‡
            compute_params: éœ€è¦è®¡ç®—æ¢¯åº¦çš„å‚æ•°åˆ—è¡¨
        """
        # 1. ä¿å­˜ä¸Šä¸‹æ–‡ï¼ˆåå‘ä¼ æ’­éœ€è¦ï¼‰
        ctx.fn = fn
        ctx.self = self
        ctx.shards = shards
        ctx.compute_params = [p for p in compute_params if p.requires_grad]
        ctx.save_for_backward(x)  # â† åªä¿å­˜è¾“å…¥ï¼Œä¸ä¿å­˜æ¿€æ´»å€¼

        # 2. åˆ‡åˆ†è¾“å…¥åºåˆ—
        x_shards = list(torch.chunk(x, chunks=shards, dim=1))
        # ä¾‹å¦‚ï¼š[batch, 100K, 4096] -> 4 ä¸ª [batch, 25K, 4096]

        # 3. é€ shard è®¡ç®—ï¼ˆæ— æ¢¯åº¦ï¼‰
        with torch.no_grad():  # â† å…³é”®ï¼šä¸ä¿å­˜æ¿€æ´»å€¼ï¼
            output_shards = [fn(self, x_shard) for x_shard in x_shards]

        # 4. æ£€æŸ¥è¾“å‡ºç±»å‹ï¼ˆæ”¯æŒ tuple è¾“å‡ºï¼Œå¦‚ MoE çš„ router logitsï¼‰
        ctx.is_tuple_output = isinstance(output_shards[0], tuple)

        # 5. æ‹¼æ¥è¾“å‡º
        if ctx.is_tuple_output:
            # MoE æƒ…å†µï¼š(output, router_logits)
            output_unsharded = tuple(
                torch.cat([shard[i] for shard in output_shards], dim=[1,0][i])
                for i in range(len(output_shards[0]))
            )
        else:
            # æ™®é€š MLP
            output_unsharded = torch.cat(output_shards, dim=1)

        return output_unsharded

    @staticmethod
    def backward(ctx, *grads):
        """
        åå‘ä¼ æ’­ï¼šé‡è®¡ç®—æ¿€æ´»å€¼ + ç´¯åŠ æ¢¯åº¦
        """
        # 1. æ¢å¤ä¸Šä¸‹æ–‡
        fn = ctx.fn
        (x,) = ctx.saved_tensors  # å–å‡ºè¾“å…¥
        self = ctx.self
        shards = ctx.shards
        compute_params = ctx.compute_params
        is_tuple_output = ctx.is_tuple_output

        # 2. é‡æ–°åˆ‡åˆ†è¾“å…¥
        x_requires_grad = x.requires_grad
        x = x.detach()  # æ–­å¼€åŸæœ‰è®¡ç®—å›¾
        x.requires_grad_(x_requires_grad)
        x_shards = list(torch.chunk(x, chunks=shards, dim=1))

        # 3. åˆ›å»ºæ¢¯åº¦ç´¯åŠ å™¨ï¼ˆé«˜ç²¾åº¦ç´¯åŠ ï¼‰
        grad_accumulator = GradientAccumulator(
            compute_params, shards, dtype=x.dtype
        )

        # 4. å‡†å¤‡è¾“å…¥æ¢¯åº¦å’Œè¾“å‡ºæ¢¯åº¦
        incoming_grad = grads[0]  # æ¥è‡ªä¸‹æ¸¸çš„æ¢¯åº¦
        x_grad = torch.zeros_like(x)  # è¾“å…¥æ¢¯åº¦ï¼ˆå¾…è®¡ç®—ï¼‰

        # 5. é€ shard é‡è®¡ç®— + åå‘ä¼ æ’­
        shard_step = x_shards[0].numel()
        for i, x_shard in enumerate(x_shards):
            x_shard.requires_grad_(x_requires_grad)

            # 5.1 è®¾ç½®è¾“å…¥æ¢¯åº¦ç¼“å†²åŒºï¼ˆviewï¼Œå…±äº«å†…å­˜ï¼‰
            shard_offset = i * shard_step
            x_shard.grad = (
                x_grad.view(-1)
                .narrow(0, shard_offset, x_shard.numel())
                .view_as(x_shard)
            )

            # 5.2 åˆ‡åˆ†è¾“å‡ºæ¢¯åº¦
            incoming_grad_shard = (
                incoming_grad.view(-1)
                .narrow(0, shard_offset, x_shard.numel())
                .view_as(x_shard)
            )

            # 5.3 å®‰è£…æ¢¯åº¦ hooksï¼ˆåªåœ¨æœ€åä¸€ä¸ª shard æ›´æ–° param.gradï¼‰
            is_last_shard = (i + 1 == shards)
            grad_accumulator.install_hooks(is_last_shard)

            # 5.4 é‡æ–°è®¡ç®—å‰å‘ + åå‘ä¼ æ’­
            with torch.enable_grad():
                output = fn(self, x_shard)  # â† Recomputation!

            # 5.5 åå‘ä¼ æ’­
            if is_tuple_output:
                torch.autograd.backward(output[0], incoming_grad_shard)
            else:
                torch.autograd.backward(output, incoming_grad_shard)

        # 6. æ¸…ç† hooks
        grad_accumulator.cleanup()

        # è¿”å›ï¼š(fn, self, x_grad, shards, compute_params) çš„æ¢¯åº¦
        # åªæœ‰ x_grad æœ‰å€¼ï¼Œå…¶ä»–ä¸º None
        return (None, None, x_grad, None, None)
```

**å…³é”®è®¾è®¡äº®ç‚¹**ï¼š

1. **æ¿€æ´»å€¼é‡è®¡ç®—**ï¼š
   - å‰å‘ä¼ æ’­ï¼š`with torch.no_grad()` ä¸ä¿å­˜æ¿€æ´»å€¼
   - åå‘ä¼ æ’­ï¼š`with torch.enable_grad()` é‡æ–°è®¡ç®—

2. **æ¢¯åº¦ç¼“å†²åŒºå¤ç”¨**ï¼š
   ```python
   # ä½¿ç”¨ view + narrow é¿å…é¢å¤–å†…å­˜åˆ†é…
   x_shard.grad = x_grad.view(-1).narrow(0, offset, size).view_as(x_shard)
   ```
   - `x_grad` æ˜¯å®Œæ•´çš„è¾“å…¥æ¢¯åº¦å¼ é‡
   - æ¯ä¸ª shard çš„æ¢¯åº¦ç›´æ¥å†™å…¥å¯¹åº”ä½ç½®ï¼ˆé›¶æ‹·è´ï¼‰

3. **æ”¯æŒ MoE æ¨¡å‹**ï¼š
   - æ£€æµ‹ tuple è¾“å‡ºï¼ˆrouter logitsï¼‰
   - åˆ†åˆ«å¤„ç†æ¯ä¸ªè¾“å‡ºçš„æ¢¯åº¦

### 5.2 æ¢¯åº¦ç´¯åŠ å™¨ï¼ˆGradientAccumulatorï¼‰

```python
# æ–‡ä»¶ï¼šsrc/axolotl/monkeypatch/tiled_mlp/base.py:191-257

class GradientAccumulator:
    """
    æ‰‹åŠ¨æ¢¯åº¦ç´¯åŠ å™¨ï¼Œæ”¯æŒé«˜ç²¾åº¦ç´¯åŠ 

    ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ
    - å¤šä¸ª shard çš„æ¢¯åº¦éœ€è¦ç´¯åŠ 
    - ç›´æ¥ç´¯åŠ å¯èƒ½å¯¼è‡´ç²¾åº¦æŸå¤±ï¼ˆbf16/fp16ï¼‰
    - ä½¿ç”¨ fp32 ç´¯åŠ å™¨æå‡ç²¾åº¦
    """

    def __init__(self, params, total_shards, dtype=None):
        self.params = params
        self.total_shards = total_shards
        self.grad_accumulation_dtype = dtype or torch.float32
        self.accumulated_grads = {}
        self.hooks = []
        self.lock = threading.Lock()  # çº¿ç¨‹å®‰å…¨
        self.gradient_scale = 1.0 / total_shards  # æ¢¯åº¦å¹³å‡

        # åˆå§‹åŒ–ç´¯åŠ å™¨ï¼ˆé«˜ç²¾åº¦ï¼‰
        for param in self.params:
            self.accumulated_grads[param] = torch.zeros_like(
                param, dtype=self.grad_accumulation_dtype
            )

    def install_hooks(self, is_last_shard: bool):
        """å®‰è£…æ¢¯åº¦ hooks"""

        def create_hook(param):
            def hook(grad):
                """æ¯æ¬¡å‚æ•°æœ‰æ¢¯åº¦æ—¶è°ƒç”¨"""
                with self.lock:
                    # 1. è½¬æ¢ä¸ºç´¯åŠ ç²¾åº¦ï¼ˆfp32ï¼‰
                    grad_fp32 = grad.to(self.grad_accumulation_dtype)

                    # 2. ç¼©æ”¾æ¢¯åº¦ï¼ˆå¹³å‡ï¼‰
                    scaled_grad = grad_fp32 * self.gradient_scale

                    # 3. ç´¯åŠ 
                    if param in self.accumulated_grads:
                        self.accumulated_grads[param] += scaled_grad
                    else:
                        self.accumulated_grads[param] = scaled_grad.clone()

                    # 4. åªåœ¨æœ€åä¸€ä¸ª shard èµ‹å€¼ç»™ param.grad
                    if is_last_shard:
                        param.grad = self.accumulated_grads[param].to(param.dtype)
                        return param.grad  # â† è¿”å›æ¢¯åº¦ï¼Œä¾›ä¼˜åŒ–å™¨ä½¿ç”¨

                    return None  # â† å‰é¢çš„ shard è¿”å› Noneï¼ˆä¸æ›´æ–°ï¼‰

            return hook

        # ä¸ºæ‰€æœ‰å‚æ•°å®‰è£… hook
        for param in self.params:
            if param.requires_grad:
                hook = param.register_hook(create_hook(param))
                self.hooks.append(hook)

    def cleanup(self):
        """ç§»é™¤æ‰€æœ‰ hooks"""
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()
        del self.accumulated_grads
```

**å·¥ä½œæµç¨‹**ï¼š

```
å‡è®¾ num_shards=4, param çš„æ¢¯åº¦ä¸º [g1, g2, g3, g4]

Shard 1:
   backward() -> param.grad = g1
   hook è§¦å‘:
      accumulated_grad = 0 + g1/4 = g1/4
      is_last_shard=False -> param.grad = None (ä¸æ›´æ–°)

Shard 2:
   backward() -> param.grad = g2
   hook è§¦å‘:
      accumulated_grad = g1/4 + g2/4
      is_last_shard=False -> param.grad = None

Shard 3:
   backward() -> param.grad = g3
   hook è§¦å‘:
      accumulated_grad = g1/4 + g2/4 + g3/4
      is_last_shard=False -> param.grad = None

Shard 4:
   backward() -> param.grad = g4
   hook è§¦å‘:
      accumulated_grad = g1/4 + g2/4 + g3/4 + g4/4
      is_last_shard=True -> param.grad = accumulated_grad âœ…
```

**ç²¾åº¦æå‡**ï¼š
```python
# å‡è®¾å‚æ•°æ˜¯ bf16ï¼Œæ¢¯åº¦ä¹Ÿæ˜¯ bf16
param = torch.randn(1000, 1000, dtype=torch.bfloat16)

# æ–¹å¼ 1ï¼šç›´æ¥ç´¯åŠ ï¼ˆbf16 ç²¾åº¦ï¼‰
grad_bf16 = g1_bf16 + g2_bf16 + g3_bf16 + g4_bf16  # å¯èƒ½æº¢å‡º

# æ–¹å¼ 2ï¼šé«˜ç²¾åº¦ç´¯åŠ ï¼ˆGradientAccumulatorï¼‰
grad_fp32 = 0.0
grad_fp32 += g1_bf16.to(torch.float32)  # è½¬ fp32
grad_fp32 += g2_bf16.to(torch.float32)
grad_fp32 += g3_bf16.to(torch.float32)
grad_fp32 += g4_bf16.to(torch.float32)
grad_bf16 = grad_fp32.to(torch.bfloat16)  # æœ€åè½¬å› bf16
```

### 5.3 DeepSpeed ç‰ˆæœ¬çš„ç‰¹æ®Šå¤„ç†

```python
# æ–‡ä»¶ï¼šsrc/axolotl/monkeypatch/tiled_mlp/base.py:11-97

class DeepSpeedTiledMLPMoE(torch.autograd.Function):
    """DeepSpeed ZeRO-3 ä¸“ç”¨ç‰ˆæœ¬"""

    @staticmethod
    def backward(ctx, *grads):
        # ...ï¼ˆå‰é¢éƒ¨åˆ†å’Œ TiledMLP ç±»ä¼¼ï¼‰

        for i, x_shard in enumerate(x_shards):
            # DeepSpeed ç‰¹æ®Šå¤„ç†ï¼šæ§åˆ¶æ¢¯åº¦åŒæ­¥æ—¶æœº
            if compute_params is not None:
                if i + 1 < shards:
                    # å‰é¢çš„ shardï¼šç¦æ­¢ DeepSpeed åŒæ­¥æ¢¯åº¦
                    for param in compute_params:
                        param.ds_grad_is_ready = False  # â† å…³é”®ï¼
                else:
                    # æœ€åä¸€ä¸ª shardï¼šå…è®¸åŒæ­¥
                    for param in compute_params:
                        param.ds_grad_is_ready = True  # â† å…è®¸ ZeRO-3 é€šä¿¡

            # é‡è®¡ç®— + åå‘ä¼ æ’­
            with torch.enable_grad():
                output = fn(self, x_shard)
            torch.autograd.backward(output, incoming_grad_shard)

        return (None, None, x_grad, None, None)
```

**ä¸ºä»€ä¹ˆéœ€è¦ `ds_grad_is_ready`ï¼Ÿ**

DeepSpeed ZeRO-3 åœ¨å‚æ•°æœ‰æ¢¯åº¦æ—¶ä¼šç«‹å³è§¦å‘é€šä¿¡ï¼ˆreduce-scatterï¼‰ï¼š
```
é—®é¢˜ï¼š
Shard 1: param.grad = g1 -> DeepSpeed ç«‹å³é€šä¿¡ï¼ˆé”™è¯¯ï¼æ¢¯åº¦ä¸å®Œæ•´ï¼‰
Shard 2: param.grad = g2 -> DeepSpeed å†æ¬¡é€šä¿¡ï¼ˆé‡å¤ï¼ï¼‰
...

è§£å†³ï¼š
Shard 1-3: param.ds_grad_is_ready = Falseï¼ˆç¦æ­¢é€šä¿¡ï¼‰
Shard 4: param.ds_grad_is_ready = Trueï¼ˆå…è®¸é€šä¿¡ï¼Œæ­¤æ—¶æ¢¯åº¦å·²ç´¯åŠ å®Œæˆï¼‰
```

---

## 6. å®æˆ˜ç¤ºä¾‹ï¼šALST é•¿ä¸Šä¸‹æ–‡è®­ç»ƒ

### 6.1 ä»€ä¹ˆæ˜¯ ALSTï¼Ÿ

ALST (Arctic Long Sequence Training) æ˜¯ä¸€å¥—ç»„åˆæŠ€æœ¯ï¼Œç”¨äºè®­ç»ƒè¶…é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ï¼š

```
ALST = TiledMLP + Tiled Loss + Sequence Parallelism + Activation Offloading

ç»„ä»¶ååŒï¼š
1. Sequence Parallelism (CP): å°†åºåˆ—åˆ‡åˆ†åˆ°å¤šä¸ª GPU
2. TiledMLP: è¿›ä¸€æ­¥é™ä½ MLP å±‚çš„æ¿€æ´»å€¼æ˜¾å­˜
3. Tiled Loss: é™ä½ loss è®¡ç®—çš„æ˜¾å­˜ï¼ˆCut Cross Entropy / Liger Kernelï¼‰
4. Activation Offloading: å°†å‰©ä½™æ¿€æ´»å€¼å¸è½½åˆ° CPU
```

### 6.2 é…ç½®ç¤ºä¾‹ï¼šè®­ç»ƒ 500K é•¿ä¸Šä¸‹æ–‡

```yaml
# æ–‡ä»¶ï¼šexamples/alst/llama3-8b-fsdp2-alst.yaml

base_model: meta-llama/Llama-3.1-8B

# ========== æ•°æ®é›†é…ç½® ==========
datasets:
  - path: togethercomputer/Long-Data-Collections
    type: completion
    field: text
  - path: princeton-nlp/TextbookChapters
    type: completion
    field: chapter

# ========== è¶…é•¿ä¸Šä¸‹æ–‡è®¾ç½® ==========
sequence_len: 500_000        # 50 ä¸‡ tokensï¼
min_sample_len: 200_000      # æœ€çŸ­æ ·æœ¬ 20 ä¸‡ tokens
sample_packing: true         # æ ·æœ¬æ‰“åŒ…

# ========== ALST æ ¸å¿ƒé…ç½® ==========
tiled_mlp: true                     # â† å¯ç”¨ TiledMLP
context_parallel_size: 8            # â† åºåˆ—å¹¶è¡Œï¼ˆ8 ä¸ª GPU åˆ†æ‘Šåºåˆ—ï¼‰
plugins:
  - axolotl.integrations.cut_cross_entropy.CutCrossEntropyPlugin  # â† Tiled Loss

# ========== è®­ç»ƒè¶…å‚æ•° ==========
gradient_accumulation_steps: 1
micro_batch_size: 1          # CP è¦æ±‚ batch=1
num_epochs: 1
optimizer: adamw_torch_8bit  # 8-bit ä¼˜åŒ–å™¨èŠ‚çœæ˜¾å­˜
lr_scheduler: cosine
learning_rate: 2e-5

# ========== æ··åˆç²¾åº¦ ==========
bf16: auto
tf32: true

# ========== æ˜¾å­˜ä¼˜åŒ– ==========
gradient_checkpointing: true          # â† æ¢¯åº¦æ£€æŸ¥ç‚¹
activation_offloading: legacy         # â† æ¿€æ´»å€¼å¸è½½åˆ° CPU

# ========== FSDP2 é…ç½® ==========
fsdp_version: 2
fsdp_config:
  offload_params: false      # å‚æ•°ä¸å¸è½½ï¼ˆä¼˜åŒ–å™¨å·² 8-bitï¼‰
  state_dict_type: SHARDED_STATE_DICT
  auto_wrap_policy: TRANSFORMER_BASED_WRAP
  transformer_layer_cls_to_wrap: LlamaDecoderLayer
  reshard_after_forward: true  # ZeRO-3 æ¨¡å¼

# ========== å…¶ä»– ==========
flash_attention: true
warmup_steps: 100
```

### 6.3 æ˜¾å­˜å ç”¨åˆ†æ

è®©æˆ‘ä»¬è®¡ç®—ä¸€ä¸‹è¿™ä¸ªé…ç½®çš„æ˜¾å­˜å ç”¨ï¼š

```
æ¨¡å‹ï¼šLlama-8B (32 å±‚)
åºåˆ—é•¿åº¦ï¼š500K tokens
ç¡¬ä»¶ï¼š8 Ã— A100 80GB
é…ç½®ï¼šCP=8, TiledMLP, Gradient Checkpointing, BF16

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
å‚æ•°æ˜¾å­˜ï¼ˆæ¯ GPUï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ¨¡å‹å‚æ•°ï¼š8B Ã— 2 bytes (bf16) = 16 GB
FSDP (reshard_after_forward=true):
   å‰å‘ä¼ æ’­æ—¶ all_gather: 16 GB
   ä¹‹åé‡Šæ”¾ï¼š0 GBï¼ˆreshardï¼‰
   å¹³å‡ï¼š~2 GB (åªä¿ç•™éƒ¨åˆ†å±‚)

ä¼˜åŒ–å™¨çŠ¶æ€ (adamw_torch_8bit):
   8B Ã— 1 byte (8-bit Adam) Ã— 2 (momentum + variance) = 16 GB
   FSDP åˆ†æ‘Šï¼š16 / 8 = 2 GB / GPU

å‚æ•°æ€»è®¡ï¼š2 + 2 = 4 GB / GPU

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ¿€æ´»å€¼æ˜¾å­˜ï¼ˆæ¯ GPUï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ¯ GPU å¤„ç†çš„åºåˆ—é•¿åº¦ï¼š500K / 8 (CP) = 62.5K tokens
Batch size = 1, Hidden = 4096

1. Attention å±‚æ¿€æ´»å€¼ï¼ˆä½¿ç”¨ Flash Attentionï¼‰:
   - QKV æŠ•å½±è¾“å…¥ï¼š1 Ã— 62.5K Ã— 4096 Ã— 2 = 0.5 GB
   - Attention è¾“å‡ºï¼š1 Ã— 62.5K Ã— 4096 Ã— 2 = 0.5 GB
   - Flash Attention èŠ‚çœï¼šæ— éœ€å­˜å‚¨ attention çŸ©é˜µ
   - Checkpoint: åªä¿ç•™éƒ¨åˆ†å±‚ï¼ˆå‡è®¾ä¿ç•™ 1/4ï¼‰
   - å°è®¡ï¼š(0.5 + 0.5) Ã— 32 / 4 = 8 GB

2. MLP å±‚æ¿€æ´»å€¼ï¼ˆä½¿ç”¨ TiledMLPï¼‰:
   å‡è®¾ num_shards = ceil(62.5K / 4096) = 16
   æ¯ä¸ª tile é•¿åº¦ï¼š62.5K / 16 = 3906 tokens

   å•ä¸ª MLP å±‚å•ä¸ª tile æ¿€æ´»å€¼ï¼š
   - Gate è¾“å‡ºï¼š1 Ã— 3906 Ã— 14336 Ã— 2 = 0.11 GB
   - Up è¾“å‡ºï¼š  1 Ã— 3906 Ã— 14336 Ã— 2 = 0.11 GB
   - SiLU è¾“å‡ºï¼š1 Ã— 3906 Ã— 14336 Ã— 2 = 0.11 GB
   - Down è¾“å…¥ï¼š 1 Ã— 3906 Ã— 14336 Ã— 2 = 0.11 GB
   - å•å±‚å°è®¡ï¼š0.44 GB

   32 å±‚ï¼ˆcheckpoint ä¿ç•™ 1/4ï¼‰ï¼š
   0.44 Ã— 32 / 4 = 3.5 GB

   å¯¹æ¯”æ—  TiledMLP (å®Œæ•´åºåˆ— 62.5K):
   å•å±‚ï¼š1 Ã— 62.5K Ã— 14336 Ã— 2 Ã— 4 = 7.2 GB
   32 å±‚ï¼š7.2 Ã— 32 / 4 = 57.6 GB

   TiledMLP èŠ‚çœï¼š57.6 - 3.5 = 54 GBï¼

3. Loss è®¡ç®—ï¼ˆä½¿ç”¨ Cut Cross Entropyï¼‰:
   - æ™®é€š CE: 1 Ã— 62.5K Ã— vocab_size (128K) Ã— 4 = 32 GB
   - Cut CE: åˆ†å—è®¡ç®—ï¼Œå³°å€¼ ~0.5 GB
   - èŠ‚çœï¼š31.5 GB

æ¿€æ´»å€¼æ€»è®¡ï¼š8 + 3.5 + 0.5 = 12 GB / GPU

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ¢¯åº¦æ˜¾å­˜ï¼ˆæ¯ GPUï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FSDP æ¨¡å¼ä¸‹ï¼Œæ¢¯åº¦ä¸å‚æ•°æ˜¾å­˜ç›¸å½“ï¼š
   8B Ã— 2 bytes / 8 (FSDP) = 2 GB

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ€»æ˜¾å­˜å ç”¨
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
å‚æ•°ï¼š          4 GB
æ¿€æ´»å€¼ï¼š       12 GB
æ¢¯åº¦ï¼š          2 GB
PyTorch å¼€é”€ï¼š ~2 GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ€»è®¡ï¼š         20 GB / GPU âœ…

A100 80GBï¼šä½¿ç”¨ç‡ 25%ï¼Œéå¸¸å……è£•ï¼
```

**å¯¹æ¯”ï¼šä¸ä½¿ç”¨ ALST æŠ€æœ¯**

```
å‡è®¾åªç”¨ CP=8ï¼Œä¸ç”¨ TiledMLP å’Œ Tiled Lossï¼š

æ¿€æ´»å€¼æ˜¾å­˜ï¼š
- Attention: 8 GBï¼ˆåŒä¸Šï¼‰
- MLP: 57.6 GBï¼ˆæ—  TiledMLPï¼‰
- Loss: 32 GBï¼ˆæ—  Cut CEï¼‰
- æ€»è®¡ï¼š97.6 GB / GPU âŒ è¶…è¿‡ 80GBï¼

ç»“è®ºï¼šæ²¡æœ‰ ALSTï¼Œæ ¹æœ¬æ— æ³•è®­ç»ƒ 500K ä¸Šä¸‹æ–‡ï¼
```

### 6.4 æ€§èƒ½åŸºå‡†æµ‹è¯•

åœ¨ 8Ã—A100 80GB ä¸Šè®­ç»ƒ Llama-8B çš„å®é™…æ€§èƒ½ï¼š

| åºåˆ—é•¿åº¦ | é…ç½® | Tokens/sec/GPU | æ˜¾å­˜/GPU | å¤‡æ³¨ |
|---------|------|----------------|----------|------|
| 8K | æ—  TiledMLP | 4200 | 25 GB | åŸºå‡† |
| 32K | CP=4, æ—  TiledMLP | 1800 | 45 GB | |
| 128K | CP=8, TiledMLP | 650 | 38 GB | |
| 500K | CP=8, ALST å…¨å®¶æ¡¶ | 180 | 20 GB | æœ¬ç¤ºä¾‹ |

**ååä¸‹é™åˆ†æ**ï¼š
- CP=8 å¼•å…¥é€šä¿¡å¼€é”€ï¼š~40% ä¸‹é™
- TiledMLP é‡è®¡ç®—å¼€é”€ï¼š~30% ä¸‹é™
- Activation Offloading CPU-GPU ä¼ è¾“ï¼š~20% ä¸‹é™
- æ€»ååï¼š180 tokens/sec/GPU (ç›¸æ¯”åŸºå‡† 4200 ä¸‹é™ 96%)

**æ—¶é—´æ¢ç©ºé—´çš„æƒè¡¡**ï¼š
```
è®­ç»ƒ 1B tokens (500K åºåˆ— = 2000 ä¸ªæ ·æœ¬):

æ–¹æ¡ˆ 1ï¼š8K ä¸Šä¸‹æ–‡ï¼Œæ—  TiledMLP
   ååï¼š4200 tokens/s/GPU Ã— 8 GPUs = 33,600 tokens/s
   æ—¶é—´ï¼š1B / 33,600 = 29,762 ç§’ â‰ˆ 8.3 å°æ—¶

æ–¹æ¡ˆ 2ï¼š500K ä¸Šä¸‹æ–‡ï¼ŒALST
   ååï¼š180 tokens/s/GPU Ã— 8 GPUs = 1,440 tokens/s
   æ—¶é—´ï¼š1B / 1,440 = 694,444 ç§’ â‰ˆ 193 å°æ—¶ â‰ˆ 8 å¤©

æ—¶é—´å¢åŠ ï¼š23 å€
ä½†æ”¶ç›Šï¼šæ¨¡å‹èƒ½çœ‹åˆ° 62 å€é•¿çš„ä¸Šä¸‹æ–‡ï¼ˆ8K -> 500Kï¼‰
```

### 6.5 å¯åŠ¨å‘½ä»¤

```bash
# å•èŠ‚ç‚¹ 8 å¡è®­ç»ƒ
axolotl train examples/alst/llama3-8b-fsdp2-alst.yaml \
    --launcher accelerate \
    --num-processes 8

# æˆ–ä½¿ç”¨ DeepSpeed å¯åŠ¨ï¼ˆéœ€è¦é…ç½®æ–‡ä»¶ï¼‰
axolotl train examples/alst/llama3-8b-deepspeed-alst.yaml \
    --launcher deepspeed \
    --num-processes 8
```

### 6.6 ç›‘æ§è®­ç»ƒè¿›åº¦

```bash
# ç»ˆç«¯ 1ï¼šç›‘æ§ GPU æ˜¾å­˜
watch -n 1 nvidia-smi

# ç»ˆç«¯ 2ï¼šç›‘æ§è®­ç»ƒæ—¥å¿—
tail -f outputs/out/log.txt

# å…³é”®æŒ‡æ ‡ï¼š
# - GPU Memory Used: åº”è¯¥ç¨³å®šåœ¨ ~20GB
# - train/tokens_per_second: åº”è¯¥åœ¨ 150-200 å·¦å³
# - train/loss: è§‚å¯Ÿæ”¶æ•›æƒ…å†µ
```

**é¢„æœŸæ—¥å¿—è¾“å‡º**ï¼š
```
[INFO] Applying TiledMLP patch for model_type: llama
[INFO] Context Parallel Size: 8
[INFO] Using Cut Cross Entropy for loss computation
[INFO] Activation offloading enabled (legacy mode)

Epoch 1/1:
Step 1/2000: loss=2.456, tokens/s=182, mem=19.2GB
Step 2/2000: loss=2.389, tokens/s=185, mem=19.5GB
Step 3/2000: loss=2.301, tokens/s=180, mem=19.8GB
...
```

---

## 7. å¸¸è§é—®é¢˜ä¸æœ€ä½³å®è·µ

### 7.1 å¸¸è§é—®é¢˜

#### é—®é¢˜ 1ï¼šè®­ç»ƒé€Ÿåº¦å¤ªæ…¢

**ç—‡çŠ¶**ï¼š
```
ä½¿ç”¨ TiledMLP åï¼Œè®­ç»ƒé€Ÿåº¦ä¸‹é™ 50% ä»¥ä¸Š
```

**åŸå› åˆ†æ**ï¼š
1. Shard æ•°é‡è¿‡å¤š â†’ é‡è®¡ç®—å¼€é”€å¤§
2. Activation Offloading CPU-GPU ä¼ è¾“æ…¢
3. åºåˆ—é•¿åº¦ä¸è¶³ä»¥æ‘Šé”€é‡è®¡ç®—å¼€é”€

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# æ–¹æ¡ˆ 1ï¼šå‡å°‘ shard æ•°é‡
tiled_mlp_num_shards: 4  # ä»è‡ªåŠ¨è®¡ç®—çš„ 16 é™åˆ° 4

æƒè¡¡ï¼š
- æ˜¾å­˜å ç”¨å¢åŠ ï¼ˆ16/4 = 4 å€ï¼‰
- é€Ÿåº¦æå‡ï¼ˆå‡å°‘é‡è®¡ç®—æ¬¡æ•°ï¼‰

# æ–¹æ¡ˆ 2ï¼šåªåœ¨å¿…è¦æ—¶ä½¿ç”¨ TiledMLP
# å¦‚æœåºåˆ—é•¿åº¦ < 32Kï¼Œä¸ä½¿ç”¨ TiledMLP
tiled_mlp: false
sequence_len: 8192

# æ–¹æ¡ˆ 3ï¼šç¦ç”¨ Activation Offloadingï¼ˆå¦‚æœæ˜¾å­˜å¤Ÿç”¨ï¼‰
activation_offloading: false
```

#### é—®é¢˜ 2ï¼šæ˜¾å­˜ä¸é™åå‡

**ç—‡çŠ¶**ï¼š
```
å¯ç”¨ TiledMLP åï¼Œæ˜¾å­˜å ç”¨ä» 40GB å¢åŠ åˆ° 50GB
```

**åŸå› **ï¼š
1. æ¢¯åº¦ç´¯åŠ å™¨ä½¿ç”¨ fp32 ç²¾åº¦
2. å¤šä¸ª shard çš„ä¸­é—´çŠ¶æ€æœªé‡Šæ”¾
3. PyTorch å†…å­˜ç¢ç‰‡

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# æ–¹æ¡ˆ 1ï¼šå¼ºåˆ¶åƒåœ¾å›æ”¶
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ ï¼š
import gc
torch.cuda.empty_cache()
gc.collect()

# æ–¹æ¡ˆ 2ï¼šä½¿ç”¨æ›´æ¿€è¿›çš„ FSDP è®¾ç½®
fsdp_config:
  reshard_after_forward: true   # å‰å‘ä¼ æ’­åç«‹å³é‡Šæ”¾å‚æ•°
  limit_all_gathers: true       # é™åˆ¶ all_gather å¹¶å‘æ•°

# æ–¹æ¡ˆ 3ï¼šé™ä½ shard æ•°é‡ï¼ˆå‡å°‘ç´¯åŠ å™¨å¼€é”€ï¼‰
tiled_mlp_num_shards: 2
```

#### é—®é¢˜ 3ï¼šDeepSpeed å…¼å®¹æ€§é—®é¢˜

**ç—‡çŠ¶**ï¼š
```
RuntimeError: Expected param.ds_grad_is_ready attribute
```

**åŸå› **ï¼š
- DeepSpeed ç‰ˆæœ¬è¿‡æ—§ï¼ˆ< 0.9.0ï¼‰
- æˆ–è€…å‚æ•°æœªæ­£ç¡®åˆå§‹åŒ–

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å‡çº§ DeepSpeed
pip install deepspeed>=0.9.0

# æˆ–è€…å¼ºåˆ¶ä½¿ç”¨ FSDP ç‰ˆæœ¬
# åœ¨é…ç½®ä¸­ç§»é™¤ deepspeed é…ç½®
```

#### é—®é¢˜ 4ï¼šæ¢¯åº¦ç´¯åŠ ä¸æ­£ç¡®

**ç—‡çŠ¶**ï¼š
```
è®­ç»ƒ loss ä¸ä¸‹é™ï¼Œæˆ–è€…æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
```

**åŸå› **ï¼š
- æ¢¯åº¦ç´¯åŠ å™¨çš„ scale ä¸æ­£ç¡®
- æˆ–è€… hooks æœªæ­£ç¡®æ¸…ç†

**è°ƒè¯•æ–¹æ³•**ï¼š
```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ æ¢¯åº¦æ£€æŸ¥
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm().item()
        print(f"{name}: grad_norm={grad_norm}")
        if grad_norm > 1000 or grad_norm < 1e-8:
            print(f"WARNING: Abnormal gradient for {name}")

# é¢„æœŸï¼šæ¢¯åº¦èŒƒæ•°åº”è¯¥åœ¨ 0.01 - 10 ä¹‹é—´
```

### 7.2 æœ€ä½³å®è·µ

#### 1. ä½•æ—¶ä½¿ç”¨ TiledMLPï¼Ÿ

```
å†³ç­–æ ‘ï¼š

åºåˆ—é•¿åº¦ < 8Kï¼Ÿ
â””â”€ å¦ â†’ ä¸éœ€è¦ TiledMLPï¼ˆæ˜¾å­˜è¶³å¤Ÿï¼‰

åºåˆ—é•¿åº¦ 8K - 32Kï¼Ÿ
â”œâ”€ å• GPU æ˜¾å­˜ < 40GB â†’ ä½¿ç”¨ TiledMLP
â””â”€ å• GPU æ˜¾å­˜ >= 40GB â†’ å¯é€‰ï¼ˆæ ¹æ®æ¨¡å‹å¤§å°ï¼‰

åºåˆ—é•¿åº¦ 32K - 128Kï¼Ÿ
â””â”€ æ˜¯ â†’ å¿…é¡»ä½¿ç”¨ TiledMLP + Sequence Parallelism

åºåˆ—é•¿åº¦ > 128Kï¼Ÿ
â””â”€ æ˜¯ â†’ å¿…é¡»ä½¿ç”¨ ALST å…¨å®¶æ¡¶
         (TiledMLP + CP + Tiled Loss + Activation Offloading)
```

#### 2. Shard æ•°é‡é€‰æ‹©

```yaml
# è§„åˆ™ 1ï¼šè‡ªåŠ¨è®¡ç®—ï¼ˆæ¨èï¼‰
tiled_mlp: true
# num_shards = ceil(seq_len / hidden_size)

# è§„åˆ™ 2ï¼šæ‰‹åŠ¨æŒ‡å®šï¼ˆç²¾ç»†æ§åˆ¶ï¼‰
# ç›®æ ‡ï¼šå•ä¸ª shard çš„æ¿€æ´»å€¼ < 2GB

# ç¤ºä¾‹ï¼šLlama-8B, seq_len=100K
sequence_len: 100_000
hidden_size: 4096
intermediate_size: 14336

# å•ä¸ª shard æ¿€æ´»å€¼ä¼°ç®—ï¼š
# shard_len Ã— intermediate_size Ã— 4 (gate/up/act/down) Ã— 2 (bf16)
# = shard_len Ã— 14336 Ã— 4 Ã— 2 bytes

# ç›®æ ‡ < 2GBï¼š
# shard_len Ã— 14336 Ã— 8 < 2e9
# shard_len < 17,500

# num_shards = 100K / 17.5K â‰ˆ 6
tiled_mlp_num_shards: 6
```

#### 3. ä¸å…¶ä»–ä¼˜åŒ–æŠ€æœ¯çš„ç»„åˆ

```yaml
# ===== æœ€ä½³ç»„åˆ 1ï¼šå•èŠ‚ç‚¹ 8 å¡ï¼Œ32K ä¸Šä¸‹æ–‡ =====
sequence_len: 32768
fsdp_version: 2
fsdp_config:
  reshard_after_forward: true
context_parallel_size: 4       # CP åˆ‡ 4 ä»½ -> æ¯å¡ 8K
tiled_mlp: true                # MLP å†åˆ‡ç‰‡
tiled_mlp_num_shards: 2        # æ¯å¡ 8K / 2 = 4K per shard
gradient_checkpointing: true   # è¿›ä¸€æ­¥é™ä½æ¿€æ´»å€¼
flash_attention: true          # å¿…éœ€
bf16: auto
plugins:
  - axolotl.integrations.liger.LigerPlugin  # Tiled Loss

æ˜¾å­˜å ç”¨ï¼š~25 GB / GPU
ååï¼š~1200 tokens/s/GPU

# ===== æœ€ä½³ç»„åˆ 2ï¼šå•èŠ‚ç‚¹ 8 å¡ï¼Œ128K ä¸Šä¸‹æ–‡ =====
sequence_len: 131072
fsdp_version: 2
context_parallel_size: 8       # CP åˆ‡ 8 ä»½ -> æ¯å¡ 16K
tiled_mlp: true
tiled_mlp_num_shards: 4        # æ¯å¡ 16K / 4 = 4K per shard
gradient_checkpointing: true
activation_offloading: legacy  # æ¿€æ´»å€¼å¸è½½
flash_attention: true
optimizer: adamw_torch_8bit    # 8-bit ä¼˜åŒ–å™¨
plugins:
  - axolotl.integrations.cut_cross_entropy.CutCrossEntropyPlugin

æ˜¾å­˜å ç”¨ï¼š~30 GB / GPU
ååï¼š~450 tokens/s/GPU

# ===== æœ€ä½³ç»„åˆ 3ï¼šè¶…é•¿ä¸Šä¸‹æ–‡ 500Kï¼ˆALSTï¼‰ =====
# å‚è€ƒ 6.2 èŠ‚çš„å®Œæ•´é…ç½®
```

#### 4. è°ƒè¯•ä¸éªŒè¯

**æ­¥éª¤ 1ï¼šéªŒè¯ TiledMLP æ˜¯å¦ç”Ÿæ•ˆ**

```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ ï¼š
import torch
from axolotl.monkeypatch.tiled_mlp.base import TiledMLP

# æ£€æŸ¥ MLP çš„ forward æ–¹æ³•æ˜¯å¦è¢«æ›¿æ¢
from transformers.models.llama.modeling_llama import LlamaMLP
print(f"LlamaMLP.forward: {LlamaMLP.forward}")
# é¢„æœŸè¾“å‡ºï¼š<function tiled_mlp_forward at 0x...>

# æ£€æŸ¥æ˜¯å¦åˆ›å»ºäº† TiledMLP å®ä¾‹
print(f"_tiled_mlp_dist_impl: {LlamaMLP._tiled_mlp_dist_impl}")
# é¢„æœŸè¾“å‡ºï¼š<class 'axolotl.monkeypatch.tiled_mlp.base.TiledMLP'>
```

**æ­¥éª¤ 2ï¼šéªŒè¯æ˜¾å­˜é™ä½**

```python
# è®­ç»ƒå‰è®°å½•æ˜¾å­˜
torch.cuda.reset_peak_memory_stats()

# è®­ç»ƒä¸€ä¸ª step
trainer.train_step(...)

# è®°å½•å³°å€¼æ˜¾å­˜
peak_mem = torch.cuda.max_memory_allocated() / 1e9
print(f"Peak memory: {peak_mem:.2f} GB")

# é¢„æœŸï¼š
# - æ—  TiledMLP: 50-70 GB
# - æœ‰ TiledMLP: 20-30 GB (å–å†³äº shard æ•°é‡)
```

**æ­¥éª¤ 3ï¼šéªŒè¯æ¢¯åº¦æ­£ç¡®æ€§**

```python
# å¯¹æ¯” TiledMLP å’Œæ™®é€š MLP çš„æ¢¯åº¦
# (åœ¨å°æ•°æ®é›†ä¸Šæµ‹è¯•)

# 1. ä½¿ç”¨æ™®é€š MLP è®­ç»ƒ 1 stepï¼Œä¿å­˜æ¢¯åº¦
tiled_mlp: false
model.train()
loss = trainer.compute_loss(model, inputs)
loss.backward()
grads_normal = {n: p.grad.clone() for n, p in model.named_parameters()}

# 2. ä½¿ç”¨ TiledMLP è®­ç»ƒåŒä¸€ batchï¼Œä¿å­˜æ¢¯åº¦
tiled_mlp: true
tiled_mlp_num_shards: 4
model.zero_grad()
loss = trainer.compute_loss(model, inputs)
loss.backward()
grads_tiled = {n: p.grad.clone() for n, p in model.named_parameters()}

# 3. å¯¹æ¯”æ¢¯åº¦ï¼ˆåº”è¯¥éå¸¸æ¥è¿‘ï¼‰
for name in grads_normal:
    diff = (grads_normal[name] - grads_tiled[name]).abs().max()
    print(f"{name}: max_diff={diff:.6f}")
    # é¢„æœŸï¼šmax_diff < 1e-5 (bf16 ç²¾åº¦ä¸‹å¯æ¥å—)
```

#### 5. æ€§èƒ½è°ƒä¼˜

**è°ƒä¼˜ 1ï¼šShard æ•°é‡ vs é€Ÿåº¦çš„æƒè¡¡**

```python
# å®éªŒï¼šæµ‹è¯•ä¸åŒ shard æ•°é‡çš„å½±å“
shard_configs = [2, 4, 8, 16, 32]
results = []

for num_shards in shard_configs:
    cfg.tiled_mlp_num_shards = num_shards

    # è®­ç»ƒ 10 steps æµ‹é€Ÿ
    start = time.time()
    for _ in range(10):
        trainer.train_step(...)
    elapsed = time.time() - start

    tokens_per_sec = (10 * batch_size * seq_len) / elapsed
    peak_mem = torch.cuda.max_memory_allocated() / 1e9

    results.append({
        'num_shards': num_shards,
        'tokens_per_sec': tokens_per_sec,
        'peak_mem_gb': peak_mem
    })

# ç»˜åˆ¶æ›²çº¿ï¼Œæ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹
import matplotlib.pyplot as plt
plt.plot([r['num_shards'] for r in results],
         [r['tokens_per_sec'] for r in results], label='Speed')
plt.plot([r['num_shards'] for r in results],
         [r['peak_mem_gb'] for r in results], label='Memory')
plt.legend()
plt.savefig('tiled_mlp_tuning.png')
```

**è°ƒä¼˜ 2ï¼šä¸ Gradient Checkpointing çš„é…åˆ**

```yaml
# å®éªŒï¼šä¸åŒ checkpoint ç­–ç•¥
# ç­–ç•¥ 1ï¼šå…¨é‡ checkpointï¼ˆæ…¢ä½†çœæ˜¾å­˜ï¼‰
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

# ç­–ç•¥ 2ï¼šéƒ¨åˆ† checkpointï¼ˆå¹³è¡¡ï¼‰
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
  checkpoint_activations_frequency: 4  # æ¯ 4 å±‚ checkpoint ä¸€æ¬¡

# ç­–ç•¥ 3ï¼šæ—  checkpoint + TiledMLPï¼ˆä¾èµ– TiledMLP çœæ˜¾å­˜ï¼‰
gradient_checkpointing: false
tiled_mlp: true
tiled_mlp_num_shards: 8

# æ¨èï¼šç­–ç•¥ 2ï¼ˆéƒ¨åˆ† checkpoint + TiledMLPï¼‰
# æ˜¾å­˜å’Œé€Ÿåº¦çš„æœ€ä½³å¹³è¡¡
```

### 7.3 TiledMLP vs å…¶ä»–æŠ€æœ¯å¯¹æ¯”

| æŠ€æœ¯ | èŠ‚çœæ˜¾å­˜ | é€Ÿåº¦å½±å“ | é€‚ç”¨åœºæ™¯ | å®ç°éš¾åº¦ |
|------|---------|---------|---------|---------|
| **TiledMLP** | â­â­â­â­ (4x-16x) | âš ï¸âš ï¸âš ï¸ (-30~-50%) | é•¿ä¸Šä¸‹æ–‡ MLP | ä½ï¼ˆé…ç½®å³ç”¨ï¼‰ |
| **Gradient Checkpointing** | â­â­â­ (2x-4x) | âš ï¸âš ï¸ (-20~-30%) | é€šç”¨ | ä½ |
| **Sequence Parallelism** | â­â­â­â­ (Nx) | âš ï¸âš ï¸ (-10~-20%) | é•¿ä¸Šä¸‹æ–‡ | ä¸­ï¼ˆéœ€å¤š GPUï¼‰ |
| **Activation Offloading** | â­â­â­â­â­ (10x+) | âš ï¸âš ï¸âš ï¸âš ï¸ (-50~-70%) | æ˜¾å­˜æåº¦å—é™ | ä½ |
| **Flash Attention** | â­â­â­ (2x-4x) | âœ…âœ… (+20~+50%) | Attention å±‚ | ä½ |
| **Tiled Loss (Cut CE)** | â­â­â­â­ (4x-8x) | âš ï¸ (-5~-10%) | å¤§ vocab | ä½ï¼ˆéœ€æ’ä»¶ï¼‰ |

**ç»„åˆå»ºè®®**ï¼š
```
çŸ­ä¸Šä¸‹æ–‡ (< 8K):
   Flash Attention âœ…

ä¸­ç­‰ä¸Šä¸‹æ–‡ (8K - 32K):
   Flash Attention + Gradient Checkpointing âœ…

é•¿ä¸Šä¸‹æ–‡ (32K - 128K):
   Flash Attention + Sequence Parallelism + TiledMLP âœ…

è¶…é•¿ä¸Šä¸‹æ–‡ (128K+):
   ALST å…¨å®¶æ¡¶ï¼ˆFA + SP + TiledMLP + Tiled Loss + Offloadingï¼‰âœ…
```

---

## æ€»ç»“

### TiledMLP çš„æ ¸å¿ƒè¦ç‚¹

1. **æœ¬è´¨**ï¼šå°† MLP è®¡ç®—åœ¨åºåˆ—ç»´åº¦ä¸Šåˆ‡åˆ†ï¼Œé€å—è®¡ç®—ï¼Œé™ä½æ¿€æ´»å€¼æ˜¾å­˜
2. **åŸç†**ï¼šå‰å‘ä¼ æ’­ä¸ä¿å­˜æ¿€æ´»å€¼ï¼Œåå‘ä¼ æ’­é‡æ–°è®¡ç®—ï¼ˆæ—¶é—´æ¢ç©ºé—´ï¼‰
3. **ä¼˜åŠ¿**ï¼š4-16 å€æ˜¾å­˜èŠ‚çœï¼Œæ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆ500K+ tokensï¼‰
4. **ä»£ä»·**ï¼š30-50% é€Ÿåº¦ä¸‹é™ï¼ˆé‡è®¡ç®—å¼€é”€ï¼‰

### Axolotl ä¸­çš„ TiledMLP ç‰¹ç‚¹

1. **ç®€å•æ˜“ç”¨**ï¼šé…ç½®æ–‡ä»¶ä¸€è¡Œå¯ç”¨ `tiled_mlp: true`
2. **è‡ªåŠ¨é€‚é…**ï¼šè‡ªåŠ¨é€‰æ‹© FSDP/DeepSpeed/å•GPU å®ç°
3. **çµæ´»é…ç½®**ï¼šæ”¯æŒè‡ªåŠ¨/æ‰‹åŠ¨è®¾ç½® shard æ•°é‡
4. **ç”Ÿäº§çº§**ï¼šæ”¯æŒé«˜ç²¾åº¦æ¢¯åº¦ç´¯åŠ ã€MoE æ¨¡å‹ã€åˆ†å¸ƒå¼è®­ç»ƒ

### ä½•æ—¶ä½¿ç”¨ TiledMLPï¼Ÿ

```
âœ… ä½¿ç”¨ TiledMLP çš„åœºæ™¯ï¼š
- è®­ç»ƒè¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆ32K+ tokensï¼‰
- MLP å±‚æ¿€æ´»å€¼å ç”¨å¤§é‡æ˜¾å­˜
- ç»“åˆ Sequence Parallelism ä»æ˜¾å­˜ä¸è¶³
- ALST è®ºæ–‡æåˆ°çš„æ‰€æœ‰åœºæ™¯

âŒ ä¸ä½¿ç”¨ TiledMLP çš„åœºæ™¯ï¼š
- çŸ­ä¸Šä¸‹æ–‡ï¼ˆ< 8K tokensï¼‰
- å¯¹è®­ç»ƒé€Ÿåº¦è¦æ±‚æé«˜
- æ˜¾å­˜å……è£•ï¼ˆå…¶ä»–ä¼˜åŒ–å·²è¶³å¤Ÿï¼‰
```

### å’Œ Tensor Parallelism çš„æ¯”è¾ƒ

å›åˆ°æˆ‘ä»¬å¼€å§‹çš„æ¯”å–»ï¼š

**Tensor Parallelism**ï¼š
- ğŸª‘ å¤šä¸ªäººä¸€èµ·æ¬**åŒä¸€å¼ æ¡Œå­çš„ä¸åŒéƒ¨åˆ†**ï¼ˆæ¨¡å‹æƒé‡åˆ‡åˆ†ï¼‰
- ğŸ¤ éœ€è¦ç´§å¯†åä½œï¼ˆå¿«é€Ÿé€šä¿¡ï¼‰
- ğŸ¯ èŠ‚çœå‚æ•°æ˜¾å­˜

**TiledMLP**ï¼š
- ğŸª‘ ä¸€ä¸ªäººæŠŠ**è¶…é•¿æ¡Œå­åˆ‡æˆå¤šæ®µ**ï¼Œé€æ®µæ¬è¿ï¼ˆåºåˆ—åˆ‡åˆ†ï¼‰
- ğŸš¶ ç‹¬ç«‹å·¥ä½œï¼ˆæ— é€šä¿¡ï¼‰
- ğŸ¯ èŠ‚çœæ¿€æ´»å€¼æ˜¾å­˜

**ç»„åˆä½¿ç”¨**ï¼š
```yaml
# 8 GPUsï¼Œè®­ç»ƒ Llama-70Bï¼Œ128K ä¸Šä¸‹æ–‡
dp_shard_size: 4           # FSDPï¼šå‚æ•°åˆ‡ 4 ä»½
tensor_parallel_size: 2    # TPï¼šæ¨¡å‹å±‚åˆ‡ 2 ä»½ï¼ˆèŠ‚çœå‚æ•°ï¼‰
context_parallel_size: 4   # CPï¼šåºåˆ—åˆ‡ 4 ä»½ï¼ˆèŠ‚çœæ¿€æ´»å€¼ï¼‰
tiled_mlp: true            # TiledMLPï¼šMLP å†åˆ‡ç‰‡ï¼ˆè¿›ä¸€æ­¥èŠ‚çœæ¿€æ´»å€¼ï¼‰

å®Œç¾é…åˆï¼
```

### è¿›ä¸€æ­¥å­¦ä¹ èµ„æº

- [ALST è®ºæ–‡](https://www.arxiv.org/abs/2506.13996)ï¼šTiledMLP åŸç†
- [Axolotl Sequence Parallelism æ–‡æ¡£](../sequence_parallelism.qmd)
- [Cut Cross Entropy é›†æˆ](../custom_integrations.html#cut-cross-entropy)
- [Liger Kernel é›†æˆ](../custom_integrations.html#liger-kernels)
- [Flash Attention è®ºæ–‡](https://arxiv.org/abs/2205.14135)

---

*æœ¬æ–‡æ¡£ç”± Claude åˆ›ä½œï¼Œæ—¨åœ¨å¸®åŠ© infra åˆå­¦è€…ç†è§£ TiledMLPã€‚å¦‚æœ‰ç–‘é—®æˆ–å‘ç°é”™è¯¯ï¼Œæ¬¢è¿æ Issueï¼*

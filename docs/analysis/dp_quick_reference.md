# Data Parallelism å¿«é€Ÿå‚è€ƒå¡ç‰‡ ğŸš€

> ä¸€é¡µçº¸é€ŸæŸ¥æ‰‹å†Œï¼Œé€‚åˆå¿«é€ŸæŸ¥é˜… DP/FSDP/DDP é…ç½®å’Œå‘½ä»¤

---

## âš™ï¸ åŸºæœ¬é…ç½®

### æœ€å°åŒ– FSDP é…ç½®
```yaml
base_model: meta-llama/Llama-3.1-13B
fsdp_version: 2  # â† ä»…æ­¤ä¸€è¡Œå¯ç”¨ FSDPï¼

# è‡ªåŠ¨æ¨æ–­ï¼šdp_shard_size = GPU æ•°é‡

# å…¶ä»–å¿…éœ€é…ç½®
datasets:
  - path: tatsu-lab/alpaca
    type: alpaca
output_dir: ./outputs/fsdp-test/
bf16: true
flash_attention: true
```

### æ¨èçš„å®Œæ•´ FSDP é…ç½®
```yaml
base_model: meta-llama/Llama-3.1-13B

# === FSDP é…ç½® ===
fsdp_version: 2
dp_shard_size: 8  # å¯é€‰ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰ GPU

fsdp_config:
  # Wrapping ç­–ç•¥ï¼ˆæŒ‰å±‚åˆ‡åˆ†ï¼‰
  auto_wrap_policy: TRANSFORMER_BASED_WRAP
  transformer_layer_cls_to_wrap: LlamaDecoderLayer

  # æ˜¾å­˜ä¼˜åŒ–ï¼ˆå…³é”®ï¼ï¼‰
  reshard_after_forward: true  # â† å‰å‘ä¼ æ’­åç«‹å³é‡Šæ”¾å‚æ•°

  # Checkpoint ç­–ç•¥
  state_dict_type: FULL_STATE_DICT  # Rank 0 æ”¶é›†å®Œæ•´æ¨¡å‹

  # å…¶ä»–
  sync_module_states: true
  use_orig_params: true

# === è®­ç»ƒé…ç½® ===
sequence_len: 2048
micro_batch_size: 4
gradient_accumulation_steps: 4

# === ä¼˜åŒ–å™¨ ===
optimizer: adamw_torch_fused  # Fused ç‰ˆæœ¬æ›´å¿«
learning_rate: 2e-5
lr_scheduler: cosine

# === æ€§èƒ½ä¼˜åŒ– ===
bf16: true
flash_attention: true
gradient_checkpointing: true

# === è¾“å‡º ===
output_dir: ./outputs/fsdp-llama-13b/
logging_steps: 10
save_steps: 500
```

---

## ğŸ¯ å¸¸ç”¨åœºæ™¯é…ç½®

### åœºæ™¯ 1ï¼šå•èŠ‚ç‚¹ 8 å¡ï¼ŒLlama-13Bï¼ˆçº¯ FSDPï¼‰

```yaml
base_model: meta-llama/Llama-3.1-13B

# === FSDP é…ç½® ===
fsdp_version: 2
# dp_shard_size è‡ªåŠ¨æ¨æ–­ä¸º 8

fsdp_config:
  auto_wrap_policy: TRANSFORMER_BASED_WRAP
  transformer_layer_cls_to_wrap: LlamaDecoderLayer
  reshard_after_forward: true
  state_dict_type: FULL_STATE_DICT

# === è®­ç»ƒé…ç½® ===
sequence_len: 2048
micro_batch_size: 4
gradient_accumulation_steps: 4

# æœ‰æ•ˆ batch size = 4 Ã— 4 Ã— 8 = 128

bf16: true
flash_attention: true
gradient_checkpointing: true
output_dir: ./outputs/llama-13b-fsdp/
```

### åœºæ™¯ 2ï¼šå•èŠ‚ç‚¹ 8 å¡ï¼ŒLlama-70Bï¼ˆTP + FSDPï¼‰

```yaml
base_model: meta-llama/Llama-3.1-70B

# === æ··åˆå¹¶è¡Œ ===
tensor_parallel_size: 2  # TP
dp_shard_size: 4         # FSDP
# æ€»è®¡ï¼š2 Ã— 4 = 8 GPUs

fsdp_version: 2

fsdp_config:
  auto_wrap_policy: TRANSFORMER_BASED_WRAP
  transformer_layer_cls_to_wrap: LlamaDecoderLayer
  reshard_after_forward: true
  state_dict_type: FULL_STATE_DICT

# === è®­ç»ƒé…ç½® ===
sequence_len: 2048
micro_batch_size: 2
gradient_accumulation_steps: 8

# æœ‰æ•ˆ batch size = 2 Ã— 8 Ã— 4 = 64
# æ³¨æ„ï¼šTP ä¸å‚ä¸ batch size è®¡ç®—ï¼

bf16: true
flash_attention: true
gradient_checkpointing: true
output_dir: ./outputs/llama-70b-tp-fsdp/
```

### åœºæ™¯ 3ï¼šåŒèŠ‚ç‚¹ 16 å¡ï¼ŒLlama-70Bï¼ˆTP + FSDP + DDPï¼‰

```yaml
base_model: meta-llama/Llama-3.1-70B

# === 4D å¹¶è¡Œ ===
tensor_parallel_size: 2     # TPï¼ˆèŠ‚ç‚¹å†…ï¼‰
dp_shard_size: 4            # FSDPï¼ˆèŠ‚ç‚¹å†…ï¼‰
dp_replicate_size: 2        # DDPï¼ˆè·¨èŠ‚ç‚¹ï¼‰
# æ€»è®¡ï¼š2 Ã— 4 Ã— 2 = 16 GPUs

fsdp_version: 2

fsdp_config:
  auto_wrap_policy: TRANSFORMER_BASED_WRAP
  transformer_layer_cls_to_wrap: LlamaDecoderLayer
  reshard_after_forward: true
  state_dict_type: FULL_STATE_DICT

# === è®­ç»ƒé…ç½® ===
sequence_len: 4096
micro_batch_size: 1
gradient_accumulation_steps: 16

# æœ‰æ•ˆ batch size = 1 Ã— 16 Ã— 4 Ã— 2 = 128

bf16: true
flash_attention: true
gradient_checkpointing: true
output_dir: ./outputs/llama-70b-multi-node/
```

### åœºæ™¯ 4ï¼šå•èŠ‚ç‚¹ 8 å¡ï¼ŒLlama-8Bï¼ˆçº¯ DDPï¼Œä¸ç”¨ FSDPï¼‰

```yaml
base_model: meta-llama/Llama-3.1-8B

# === ä¸é…ç½® fsdp_configï¼Œè‡ªåŠ¨ä½¿ç”¨ DDP ===
# Axolotl ä¼šè‡ªåŠ¨ç”¨ DDP åŒ…è£…æ¨¡å‹

# === è®­ç»ƒé…ç½® ===
sequence_len: 2048
micro_batch_size: 8
gradient_accumulation_steps: 2

# æœ‰æ•ˆ batch size = 8 Ã— 2 Ã— 8 = 128

bf16: true
flash_attention: true
output_dir: ./outputs/llama-8b-ddp/
```

---

## ğŸš€ è¿è¡Œå‘½ä»¤

### å•èŠ‚ç‚¹è®­ç»ƒ
```bash
# åŸºæœ¬å‘½ä»¤
axolotl train config.yaml

# æŒ‡å®š GPU æ•°é‡
axolotl train config.yaml --num-processes 8

# ä½¿ç”¨ torchrunï¼ˆæ¨èï¼‰
axolotl train config.yaml --launcher torchrun
```

### å¤šèŠ‚ç‚¹è®­ç»ƒ
```bash
# === Node 0 (Master) ===
axolotl train config.yaml \
    --num-processes 16 \
    --num-machines 2 \
    --machine-rank 0 \
    --main-process-ip <NODE0_IP> \
    --main-process-port 29500

# === Node 1 ===
axolotl train config.yaml \
    --num-processes 16 \
    --num-machines 2 \
    --machine-rank 1 \
    --main-process-ip <NODE0_IP> \
    --main-process-port 29500
```

### è°ƒè¯•å‘½ä»¤
```bash
# æµ‹è¯• FSDP é…ç½®
axolotl train config.yaml --max-steps 5

# å¯ç”¨ NCCL è°ƒè¯•
NCCL_DEBUG=INFO axolotl train config.yaml --max-steps 2

# ç›‘æ§æ˜¾å­˜ä½¿ç”¨
watch -n 1 nvidia-smi

# æ€§èƒ½åˆ†æ
nsys profile -o profile.qdrep \
    python -m axolotl.cli.train config.yaml --max-steps 10
```

---

## ğŸ” è°ƒè¯•é€ŸæŸ¥

### é—®é¢˜ï¼šæ˜¾å­˜ OOMï¼ˆOut of Memoryï¼‰

#### æ£€æŸ¥æ¸…å•
```bash
âœ“ å¯ç”¨ FSDP (fsdp_version: 2)
âœ“ å¼€å¯ reshard_after_forward: true
âœ“ å¼€å¯ gradient_checkpointing: true
âœ“ å‡å° micro_batch_size
âœ“ è€ƒè™‘å¢åŠ  dp_shard_size
```

#### é…ç½®è°ƒæ•´
```yaml
# === é€‰é¡¹ 1ï¼šå¯ç”¨ FSDP ===
fsdp_version: 2
fsdp_config:
  reshard_after_forward: true  # â† å…³é”®ï¼

# === é€‰é¡¹ 2ï¼šå¢å¤§ FSDP åˆ‡åˆ† ===
dp_shard_size: 8  # ä» 4 å¢åŠ åˆ° 8

# === é€‰é¡¹ 3ï¼šå‡å° Batch Size ===
micro_batch_size: 1  # ä» 4 å‡å°
gradient_accumulation_steps: 16  # å¢å¤§ä»¥è¡¥å¿

# === é€‰é¡¹ 4ï¼šå¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ ===
gradient_checkpointing: true

# === é€‰é¡¹ 5ï¼šæ··åˆ TP + FSDP ===
tensor_parallel_size: 2
dp_shard_size: 4

# === é€‰é¡¹ 6ï¼šCPU Offloadï¼ˆæç«¯æƒ…å†µï¼‰===
fsdp_config:
  offload_params: true  # å‚æ•° offload åˆ° CPU
  cpu_offload_pin_memory: false
```

---

### é—®é¢˜ï¼šè®­ç»ƒé€Ÿåº¦æ…¢

#### è¯Šæ–­æ­¥éª¤
```bash
# 1. æ£€æŸ¥ GPU äº’è¿
nvidia-smi topo -m

# âœ… å¥½ï¼ˆNVLinkï¼‰:
#   GPU0  GPU1
# 0   X    NV12
# 1  NV12   X

# âŒ å·®ï¼ˆPCIeï¼‰:
#   GPU0  GPU1
# 0   X    PHB
# 1  PHB   X

# 2. æ£€æŸ¥é€šä¿¡æ—¶é—´
NCCL_DEBUG=INFO axolotl train config.yaml 2>&1 | grep "AllGather\|AllReduce"

# 3. æŸ¥çœ‹ tokens/s
# åœ¨æ—¥å¿—ä¸­æŸ¥æ‰¾ "Tokens/s/GPU"
```

#### æ€§èƒ½ä¼˜åŒ–é…ç½®
```yaml
# === ä¼˜åŒ– 1ï¼šä½¿ç”¨ Fused Optimizer ===
optimizer: adamw_torch_fused  # â† æ¯” adamw_torch å¿« ~10%

# === ä¼˜åŒ– 2ï¼šä¼˜åŒ– FSDP Wrapping ===
fsdp_config:
  auto_wrap_policy: TRANSFORMER_BASED_WRAP  # â† æ¯” SIZE_BASED_WRAP å¿«
  transformer_layer_cls_to_wrap: LlamaDecoderLayer

# === ä¼˜åŒ– 3ï¼šæ•°æ®åŠ è½½ä¼˜åŒ– ===
dataloader_num_workers: 4
dataloader_pin_memory: true
dataloader_prefetch_factor: 2

# === ä¼˜åŒ– 4ï¼šå‡å°‘é€šä¿¡é¢‘ç‡ ===
gradient_accumulation_steps: 8  # å¢å¤§
# æ¢¯åº¦ç´¯ç§¯æœŸé—´ä¸é€šä¿¡ï¼Œå‡å°‘ AllReduce æ¬¡æ•°

# === ä¼˜åŒ– 5ï¼šæ··åˆç²¾åº¦ ===
bf16: true
tf32: true

# === ä¼˜åŒ– 6ï¼šå¦‚æœæ¨¡å‹èƒ½æ”¾è¿›å• GPUï¼Œæ”¹ç”¨ DDP ===
# æ³¨é‡Šæ‰ fsdp_configï¼ŒAxolotl ä¼šè‡ªåŠ¨ç”¨ DDP
# DDP é€šä¿¡é‡ < FSDPï¼ˆçº¦ 1/3ï¼‰
```

---

### é—®é¢˜ï¼šLoss NaN æˆ–å‘æ•£

#### è¯Šæ–­
```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ æ¢¯åº¦æ£€æŸ¥

import math

for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm().item()
        if math.isnan(grad_norm) or math.isinf(grad_norm):
            print(f"å¼‚å¸¸æ¢¯åº¦: {name}, norm={grad_norm}")
```

#### é…ç½®è°ƒæ•´
```yaml
# === é€‰é¡¹ 1ï¼šä½¿ç”¨ bf16ï¼ˆæ¯” fp16 æ›´ç¨³å®šï¼‰===
bf16: true
fp16: false

# === é€‰é¡¹ 2ï¼šæ¢¯åº¦è£å‰ª ===
max_grad_norm: 1.0

# === é€‰é¡¹ 3ï¼šé™ä½å­¦ä¹ ç‡ ===
learning_rate: 5e-6  # ä» 2e-5 é™ä½

# === é€‰é¡¹ 4ï¼šå¢åŠ  Warmup ===
warmup_steps: 100
warmup_ratio: 0.05

# === é€‰é¡¹ 5ï¼šæ£€æŸ¥æ•°æ®è´¨é‡ ===
# ç¡®ä¿æ•°æ®é›†æ²¡æœ‰å¼‚å¸¸å€¼ã€æŸåçš„æ ·æœ¬
```

---

### é—®é¢˜ï¼šå¤šèŠ‚ç‚¹é€šä¿¡å¤±è´¥

#### è¯Šæ–­
```bash
# 1. æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
ping <NODE1_IP>

# 2. æ£€æŸ¥ç«¯å£
# Node 0:
nc -l 29500

# Node 1:
nc <NODE0_IP> 29500

# 3. æ£€æŸ¥ NCCL ç¯å¢ƒå˜é‡
env | grep NCCL
```

#### è§£å†³æ–¹æ¡ˆ
```bash
# === é€‰é¡¹ 1ï¼šæŒ‡å®šç½‘ç»œæ¥å£ ===
export NCCL_SOCKET_IFNAME=eth0  # æˆ– ib0ï¼ˆInfiniBandï¼‰
export GLOO_SOCKET_IFNAME=eth0

# === é€‰é¡¹ 2ï¼šç¦ç”¨ InfiniBandï¼ˆå¦‚æœæ²¡æœ‰ï¼‰===
export NCCL_IB_DISABLE=1

# === é€‰é¡¹ 3ï¼šå¢åŠ è¶…æ—¶æ—¶é—´ ===
export NCCL_TIMEOUT=7200  # 2 å°æ—¶

# === é€‰é¡¹ 4ï¼šå¯ç”¨ NCCL è°ƒè¯• ===
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL

# === é€‰é¡¹ 5ï¼šæ£€æŸ¥é˜²ç«å¢™ ===
sudo ufw allow 29500:29600/tcp
sudo ufw allow 29500:29600/udp
```

---

### é—®é¢˜ï¼šCheckpoint ä¿å­˜/åŠ è½½å¤±è´¥

#### ç—‡çŠ¶
```
Rank 0 saves successfully
Rank 1+ hangs...
```

#### è§£å†³æ–¹æ¡ˆ
```yaml
# === é€‰é¡¹ 1ï¼šä½¿ç”¨ FULL_STATE_DICTï¼ˆæ¨èï¼‰===
fsdp_config:
  state_dict_type: FULL_STATE_DICT
  # Rank 0 æ”¶é›†å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹
  # å…¶ä»– ranks ç­‰å¾…ï¼ˆbarrierï¼‰

# === é€‰é¡¹ 2ï¼šä½¿ç”¨ SHARDED_STATE_DICT ===
fsdp_config:
  state_dict_type: SHARDED_STATE_DICT
  # æ¯ä¸ª rank ä¿å­˜è‡ªå·±çš„åˆ‡ç‰‡
  # æ¢å¤æ—¶éœ€è¦ç›¸åŒçš„ GPU é…ç½®

# === é€‰é¡¹ 3ï¼šæœ€ç»ˆä¿å­˜å®Œæ•´æ¨¡å‹ ===
fsdp_config:
  state_dict_type: SHARDED_STATE_DICT  # è®­ç»ƒä¸­
  final_state_dict_type: FULL_STATE_DICT  # è®­ç»ƒç»“æŸ
```

---

## ğŸ“Š FSDP vs DDP å¯¹æ¯”è¡¨

| ç»´åº¦ | FSDP | DDP |
|------|------|-----|
| **æ˜¾å­˜å ç”¨** | ~1/Nï¼ˆN æ˜¯ GPU æ•°ï¼‰ | 100%ï¼ˆæ¯ä¸ª GPUï¼‰ |
| **é€šä¿¡é‡** | AllGather + ReduceScatterï¼ˆ~3Ã—ï¼‰ | AllReduceï¼ˆ~1Ã—ï¼‰ |
| **é€šä¿¡é¢‘ç‡** | æ¯å±‚ 2 æ¬¡ï¼ˆå‰å‘+åå‘ï¼‰ | æ¯ä¸ª iteration 1 æ¬¡ |
| **é€‚ç”¨æ¨¡å‹å¤§å°** | å¯æ‰©å±•åˆ°æå¤§æ¨¡å‹ | å—å• GPU æ˜¾å­˜é™åˆ¶ |
| **é…ç½®å¤æ‚åº¦** | å¤æ‚ï¼ˆéœ€è¦ wrap policy ç­‰ï¼‰ | ç®€å•ï¼ˆè‡ªåŠ¨ï¼‰ |
| **é€Ÿåº¦** | è¾ƒæ…¢ï¼ˆé€šä¿¡å¤šï¼‰ | è¾ƒå¿«ï¼ˆé€šä¿¡å°‘ï¼‰ |

### å†³ç­–æ ‘
```
æ¨¡å‹èƒ½æ”¾è¿›å•ä¸ª GPU æ˜¾å­˜ï¼Ÿ
â”œâ”€ Yes â†’ ç”¨ DDP
â”‚   â””â”€ é…ç½®ï¼šä¸å¯ç”¨ fsdp_config
â”‚
â””â”€ No â†’ ç”¨ FSDP
    â”œâ”€ é…ç½®ï¼šfsdp_version: 2 + fsdp_config
    â””â”€ å»ºè®®ï¼š
        - reshard_after_forward: trueï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
        - TRANSFORMER_BASED_WRAPï¼ˆæ€§èƒ½æ›´å¥½ï¼‰
        - ç¡®ä¿ NVLinkï¼ˆé€šä¿¡å¸¦å®½ï¼‰
```

---

## ğŸ› ï¸ å¸¸ç”¨ä»£ç ç‰‡æ®µ

### æ£€æŸ¥ FSDP æ˜¯å¦ç”Ÿæ•ˆ
```python
import torch.distributed as dist

if dist.is_initialized():
    rank = dist.get_rank()
    world_size = dist.get_world_size()

    # æ£€æŸ¥æ¨¡å‹å‚æ•°å¤§å°
    total_params = sum(p.numel() for p in model.parameters())
    param_memory = sum(p.numel() * p.element_size() for p in model.parameters())

    print(f"Rank {rank}: å‚æ•°æ•°é‡ = {total_params:,}")
    print(f"Rank {rank}: å‚æ•°æ˜¾å­˜ = {param_memory / 1e9:.2f} GB")

    # å¦‚æœä½¿ç”¨ FSDPï¼Œæ¯ä¸ª rank çš„å‚æ•°æ˜¾å­˜åº”è¯¥çº¦ä¸º total_memory / world_size
```

### ç›‘æ§æ¢¯åº¦åŒæ­¥
```python
# æ·»åŠ é€šä¿¡ç›‘æ§

import time

class CommunicationTimer:
    def __init__(self):
        self.comm_time = 0
        self.compute_time = 0

    def on_before_backward(self):
        self.backward_start = time.time()

    def on_after_backward(self):
        self.backward_end = time.time()
        self.compute_time += self.backward_end - self.backward_start

    def on_before_optimizer_step(self):
        self.step_start = time.time()

    def on_after_optimizer_step(self):
        self.step_end = time.time()
        self.comm_time += self.step_end - self.step_start

    def report(self):
        total_time = self.comm_time + self.compute_time
        comm_ratio = self.comm_time / total_time * 100
        print(f"è®¡ç®—æ—¶é—´: {self.compute_time:.2f}s")
        print(f"é€šä¿¡æ—¶é—´: {self.comm_time:.2f}s ({comm_ratio:.1f}%)")
```

### è®¡ç®—æœ‰æ•ˆ Batch Size
```python
# Data Parallelism ä¸‹çš„æœ‰æ•ˆ batch size

effective_batch_size = (
    micro_batch_size *              # æ¯ä¸ª GPU çš„ batch size
    gradient_accumulation_steps *   # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    dp_shard_size *                 # FSDP å¹¶è¡Œåº¦
    dp_replicate_size               # DDP å¹¶è¡Œåº¦ï¼ˆå¦‚æœæœ‰ï¼‰
)

# ä¾‹å­ 1ï¼šå•èŠ‚ç‚¹ 8 å¡ FSDP
# micro_batch_size = 4
# gradient_accumulation_steps = 4
# dp_shard_size = 8
# dp_replicate_size = 1
# effective_batch_size = 4 Ã— 4 Ã— 8 Ã— 1 = 128

# ä¾‹å­ 2ï¼šåŒèŠ‚ç‚¹ 16 å¡ FSDP + DDP
# micro_batch_size = 1
# gradient_accumulation_steps = 16
# dp_shard_size = 4
# dp_replicate_size = 2
# effective_batch_size = 1 Ã— 16 Ã— 4 Ã— 2 = 128

# æ³¨æ„ï¼šTP å’Œ CP ä¸å‚ä¸ batch size è®¡ç®—ï¼
```

### æ£€æŸ¥å‚æ•°åŒæ­¥
```python
# éªŒè¯æ‰€æœ‰ ranks çš„æ¨¡å‹å‚æ•°æ˜¯å¦ä¸€è‡´

import torch.distributed as dist

def check_model_sync(model):
    if not dist.is_initialized():
        return

    rank = dist.get_rank()

    for name, param in model.named_parameters():
        # è®¡ç®—å‚æ•°çš„ checksum
        checksum = param.data.sum().item()

        # æ”¶é›†æ‰€æœ‰ ranks çš„ checksum
        all_checksums = [torch.tensor(0.0) for _ in range(dist.get_world_size())]
        dist.all_gather_object(all_checksums, checksum)

        # Rank 0 æ£€æŸ¥
        if rank == 0:
            if not all(abs(c - checksum) < 1e-5 for c in all_checksums):
                print(f"âŒ å‚æ•°ä¸åŒæ­¥: {name}")
            else:
                print(f"âœ… å‚æ•°åŒæ­¥: {name}")

# ä½¿ç”¨
check_model_sync(model)
```

---

## âš¡ æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•

### å¿…åšä¼˜åŒ– âœ…
- [ ] ä½¿ç”¨ bf16 (`bf16: true`)
- [ ] å¯ç”¨ Flash Attention (`flash_attention: true`)
- [ ] FSDP å¼€å¯ reshard (`reshard_after_forward: true`)
- [ ] ä½¿ç”¨ Fused Optimizer (`optimizer: adamw_torch_fused`)
- [ ] å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ (`gradient_checkpointing: true`)

### é€šä¿¡ä¼˜åŒ– ğŸš€
- [ ] ç¡®ä¿ NVLink (`nvidia-smi topo -m`)
- [ ] ä½¿ç”¨ TRANSFORMER_BASED_WRAPï¼ˆæ¯” SIZE_BASED å¿«ï¼‰
- [ ] å¢å¤§æ¢¯åº¦ç´¯ç§¯ï¼ˆå‡å°‘é€šä¿¡é¢‘ç‡ï¼‰
- [ ] å¤šèŠ‚ç‚¹ï¼šé…ç½®é«˜é€Ÿç½‘ç»œï¼ˆInfiniBand/100Gbps+ï¼‰

### æ˜¾å­˜ä¼˜åŒ– ğŸ’¾
- [ ] FSDP `reshard_after_forward: true`
- [ ] å‡å° `micro_batch_size`ï¼Œå¢å¤§ `gradient_accumulation_steps`
- [ ] å¼€å¯ `gradient_checkpointing`
- [ ] æç«¯æƒ…å†µï¼šCPU offload (`offload_params: true`)

### è°ƒè¯•ä¼˜åŒ– ğŸ›
- [ ] å¯ç”¨ NCCL æ—¥å¿— (`NCCL_DEBUG=INFO`)
- [ ] ç›‘æ§æ˜¾å­˜ä½¿ç”¨ (`nvidia-smi dmon`)
- [ ] æ£€æŸ¥æ¢¯åº¦å¼‚å¸¸ï¼ˆNaN/Infï¼‰
- [ ] éªŒè¯æ¨¡å‹å‚æ•°åŒæ­¥

---

## ğŸ“ é…ç½®å…¬å¼

### GPU æ•°é‡è®¡ç®—
```
æ€» GPU æ•° = tensor_parallel_size Ã— context_parallel_size Ã— dp_shard_size Ã— dp_replicate_size
```

### æœ‰æ•ˆ Batch Size è®¡ç®—
```
æœ‰æ•ˆ batch size = micro_batch_size Ã— gradient_accumulation_steps Ã— dp_shard_size Ã— dp_replicate_size

æ³¨æ„ï¼šTP å’Œ CP ä¸å‚ä¸ batch size è®¡ç®—ï¼
```

### FSDP æ˜¾å­˜èŠ‚çœä¼°ç®—
```
æ˜¾å­˜èŠ‚çœ â‰ˆ 1 / dp_shard_size

ä¾‹å¦‚ï¼š
dp_shard_size = 1 (æ— FSDP): èŠ‚çœ 0%
dp_shard_size = 4: èŠ‚çœ ~75%
dp_shard_size = 8: èŠ‚çœ ~87.5%
```

### é€šä¿¡å¼€é”€ä¼°ç®—
```
FSDP é€šä¿¡é‡ â‰ˆ 3 Ã— DDP é€šä¿¡é‡

åŸå› ï¼š
- DDP: 1 æ¬¡ AllReduceï¼ˆæ¢¯åº¦ï¼‰
- FSDP: æ¯å±‚ 2 æ¬¡ï¼ˆAllGather å‚æ•° + ReduceScatter æ¢¯åº¦ï¼‰
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### âœ… æ¨è
- **æ¨¡å‹ â‰¤7B** â†’ ç”¨ DDPï¼ˆç®€å•é«˜æ•ˆï¼‰
- **æ¨¡å‹ >7B** â†’ ç”¨ FSDPï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
- **æ¨¡å‹ >30B** â†’ FSDP + TPï¼ˆæ··åˆå¹¶è¡Œï¼‰
- **å¤šèŠ‚ç‚¹** â†’ FSDP + DDPï¼ˆèŠ‚ç‚¹å†… FSDPï¼ŒèŠ‚ç‚¹é—´ DDPï¼‰
- **FSDP wrapping** â†’ TRANSFORMER_BASED_WRAPï¼ˆæ€§èƒ½æ›´å¥½ï¼‰
- **Checkpoint** â†’ FULL_STATE_DICTï¼ˆæ˜“äºä½¿ç”¨ï¼‰

### âŒ é¿å…
- å°æ¨¡å‹ä½¿ç”¨ FSDPï¼ˆé€šä¿¡å¼€é”€ä¸åˆ’ç®—ï¼‰
- FSDP ä¸å¼€å¯ `reshard_after_forward`ï¼ˆæ˜¾å­˜èŠ‚çœå°‘ï¼‰
- ä½¿ç”¨ SIZE_BASED_WRAPï¼ˆæ€§èƒ½è¾ƒå·®ï¼‰
- å¿˜è®°è®¾ç½® `sampler.set_epoch()`ï¼ˆæ¯ä¸ª epoch æ•°æ®ç›¸åŒï¼‰

### ğŸ¯ å†³ç­–æ ‘
```
é€‰æ‹©å¹¶è¡Œç­–ç•¥ï¼Ÿ
â”œâ”€ æ¨¡å‹èƒ½æ”¾è¿›å• GPU â†’ DDP
â”œâ”€ æ¨¡å‹å¤ªå¤§ â†’ FSDP
â”œâ”€ æ¨¡å‹æå¤§ + åºåˆ—çŸ­ â†’ FSDP + TP
â””â”€ æ¨¡å‹æå¤§ + åºåˆ—é•¿ â†’ FSDP + TP + CP
```

---

## ğŸ“š å¿«é€Ÿé“¾æ¥

- [è¯¦ç»†æ•™ç¨‹](./data_parallelism_deep_dive.md)
- [æºç è§£æ](./dp_source_code_walkthrough.md)
- [TP å¿«é€Ÿå‚è€ƒ](./tp_quick_reference.md)
- [CP å¿«é€Ÿå‚è€ƒ](./cp_quick_reference.md)
- [åˆ†ææ–‡æ¡£ç´¢å¼•](./README.md)

---

## ğŸ’¡ é€Ÿè®°å£è¯€

```
æ¨¡å‹å°ç”¨ DDPï¼Œ
æ¨¡å‹å¤§ç”¨ FSDPã€‚
Reshard è¦å¼€å¯ï¼Œ
æ˜¾å­˜èŠ‚çœå¤šã€‚

Batch ç´¯ç§¯ç®—ï¼Œ
æ¢¯åº¦å°‘é€šä¿¡ã€‚
NVLink æ˜¯å…³é”®ï¼Œ
é€Ÿåº¦é£èµ·æ¥ã€‚

FULL_STATE_DICTï¼Œ
ä¿å­˜æœ€ç®€å•ã€‚
Fused Optimizerï¼Œ
è®­ç»ƒæ›´é«˜æ•ˆã€‚
```

---

## ğŸ”¢ é…ç½®ç¤ºä¾‹é€ŸæŸ¥

### Llama-8B, 8 å¡ (DDP)
```yaml
# ä¸é…ç½® fsdpï¼Œè‡ªåŠ¨ç”¨ DDP
micro_batch_size: 8
gradient_accumulation_steps: 2
# æœ‰æ•ˆ batch = 8 Ã— 2 Ã— 8 = 128
```

### Llama-13B, 8 å¡ (FSDP)
```yaml
fsdp_version: 2
fsdp_config:
  reshard_after_forward: true
micro_batch_size: 4
gradient_accumulation_steps: 4
# æœ‰æ•ˆ batch = 4 Ã— 4 Ã— 8 = 128
```

### Llama-70B, 8 å¡ (TP + FSDP)
```yaml
tensor_parallel_size: 2
dp_shard_size: 4
fsdp_version: 2
micro_batch_size: 2
gradient_accumulation_steps: 8
# æœ‰æ•ˆ batch = 2 Ã— 8 Ã— 4 = 64
```

### Llama-70B, 16 å¡åŒèŠ‚ç‚¹ (TP + FSDP + DDP)
```yaml
tensor_parallel_size: 2
dp_shard_size: 4
dp_replicate_size: 2
fsdp_version: 2
micro_batch_size: 1
gradient_accumulation_steps: 16
# æœ‰æ•ˆ batch = 1 Ã— 16 Ã— 4 Ã— 2 = 128
```

---

*æ‰“å°æ­¤é¡µä½œä¸ºé€ŸæŸ¥æ‰‹å†Œ | æœ€åæ›´æ–°ï¼š2025-11*

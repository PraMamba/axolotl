# Tensor Parallelism å¿«é€Ÿå‚è€ƒå¡ç‰‡ ğŸš€

> ä¸€é¡µçº¸é€ŸæŸ¥æ‰‹å†Œï¼Œé€‚åˆå¿«é€ŸæŸ¥é˜…é…ç½®å’Œå‘½ä»¤

---

## âš™ï¸ åŸºæœ¬é…ç½®

### æœ€å°åŒ– TP é…ç½®
```yaml
base_model: meta-llama/Llama-3.1-8B
tensor_parallel_size: 2  # ä»…æ­¤ä¸€è¡Œï¼

# å…¶ä»–å¿…éœ€é…ç½®
datasets:
  - path: tatsu-lab/alpaca
    type: alpaca
output_dir: ./outputs/tp-test/
bf16: true
flash_attention: true
```

### æ¨èçš„å®Œæ•´é…ç½®
```yaml
base_model: meta-llama/Llama-3.1-70B

# === å¹¶è¡Œé…ç½® ===
dp_shard_size: 4         # FSDP
tensor_parallel_size: 2  # TP
# æ€»è®¡ï¼š4 Ã— 2 = 8 GPUs

# === FSDP é…ç½® ===
fsdp_version: 2
fsdp_config:
  reshard_after_forward: true
  state_dict_type: FULL_STATE_DICT
  auto_wrap_policy: TRANSFORMER_BASED_WRAP
  transformer_layer_cls_to_wrap: LlamaDecoderLayer

# === è®­ç»ƒé…ç½® ===
sequence_len: 2048
micro_batch_size: 1
gradient_accumulation_steps: 8
num_epochs: 1

optimizer: adamw_torch_fused
lr_scheduler: cosine
learning_rate: 2e-5

# === æ€§èƒ½ä¼˜åŒ– ===
bf16: true
tf32: true
flash_attention: true
gradient_checkpointing: true

# === è¾“å‡º ===
output_dir: ./outputs/tp-70b/
logging_steps: 1
```

---

## ğŸ¯ å¸¸ç”¨åœºæ™¯é…ç½®

### åœºæ™¯ 1ï¼šå•èŠ‚ç‚¹ 8 å¡ï¼Œ30B-70B æ¨¡å‹
```yaml
# é€‰é¡¹ Aï¼šæ›´å¤š FSDP
dp_shard_size: 8
tensor_parallel_size: 1

# é€‰é¡¹ Bï¼šå¹³è¡¡ FSDP + TP
dp_shard_size: 4
tensor_parallel_size: 2  # â† æ¨è

# é€‰é¡¹ Cï¼šæ›´å¤š TP
dp_shard_size: 2
tensor_parallel_size: 4
```

### åœºæ™¯ 2ï¼šåŒèŠ‚ç‚¹ 16 å¡ï¼Œ70B+ æ¨¡å‹
```yaml
dp_shard_size: 4         # èŠ‚ç‚¹å†… FSDP
dp_replicate_size: 2     # èŠ‚ç‚¹é—´ DDP
tensor_parallel_size: 2  # èŠ‚ç‚¹å†… TP
# 4 Ã— 2 Ã— 2 = 16 GPUs
```

### åœºæ™¯ 3ï¼šé•¿ä¸Šä¸‹æ–‡ (16K tokens)
```yaml
dp_shard_size: 2
tensor_parallel_size: 2
context_parallel_size: 2
sequence_len: 16384
micro_batch_size: 1  # CP è¦æ±‚
# 2 Ã— 2 Ã— 2 = 8 GPUs
```

### åœºæ™¯ 4ï¼šè¶…å¤§æ¨¡å‹ (175B+)ï¼Œ4D å¹¶è¡Œ
```yaml
dp_shard_size: 2
dp_replicate_size: 2
tensor_parallel_size: 4
context_parallel_size: 2
# 2 Ã— 2 Ã— 4 Ã— 2 = 32 GPUs
```

---

## ğŸš€ è¿è¡Œå‘½ä»¤

### åŸºæœ¬å‘½ä»¤
```bash
# å•èŠ‚ç‚¹è®­ç»ƒ
axolotl train config.yaml

# æŒ‡å®š GPU æ•°é‡
axolotl train config.yaml --num-processes 8

# æŒ‡å®š launcher
axolotl train config.yaml --launcher accelerate
axolotl train config.yaml --launcher torchrun
```

### å¤šèŠ‚ç‚¹è®­ç»ƒ
```bash
# === Node 0 (master) ===
axolotl train config.yaml \
    --num-processes 16 \
    --num-machines 2 \
    --machine-rank 0 \
    --main-process-ip <NODE0_IP> \
    --main-process-port 29500

# === Node 1 ===
axolotl train config.yaml \
    --num-processes 16 \
    --num-machines 2 \
    --machine-rank 1 \
    --main-process-ip <NODE0_IP> \
    --main-process-port 29500
```

### è°ƒè¯•å‘½ä»¤
```bash
# åªè¿è¡Œ 2 ä¸ª step æµ‹è¯•
axolotl train config.yaml --max-steps 2

# å¯ç”¨è¯¦ç»†æ—¥å¿—
NCCL_DEBUG=INFO axolotl train config.yaml

# ä½¿ç”¨ PyTorch Profiler
axolotl train config.yaml --use-profiler
```

---

## ğŸ” è°ƒè¯•é€ŸæŸ¥

### é—®é¢˜ï¼šæ˜¾å­˜ OOM

#### æ£€æŸ¥æ¸…å•
```bash
âœ“ ç¡®è®¤ TP size é…ç½®æ­£ç¡®
âœ“ å¼€å¯ reshard_after_forward
âœ“ å¼€å¯ gradient_checkpointing
âœ“ é™ä½ micro_batch_size
âœ“ å¢å¤§ gradient_accumulation_steps
```

#### é…ç½®è°ƒæ•´
```yaml
# ä¹‹å‰
micro_batch_size: 4
gradient_accumulation_steps: 2

# ä¹‹å
micro_batch_size: 1          # é™ä½
gradient_accumulation_steps: 8  # å¢å¤§
# æœ‰æ•ˆ batch size ä¿æŒä¸å˜ï¼š4Ã—2 = 1Ã—8
```

---

### é—®é¢˜ï¼šè®­ç»ƒé€Ÿåº¦æ…¢

#### æ£€æŸ¥ GPU äº’è¿
```bash
nvidia-smi topo -m

# âœ… å¥½ï¼ˆNVLinkï¼‰:
#   GPU0  GPU1
# 0   X    NV12
# 1  NV12   X

# âŒ å·®ï¼ˆPCIeï¼‰:
#   GPU0  GPU1
# 0   X    PHB
# 1  PHB   X
```

#### æ€§èƒ½ä¼˜åŒ–é…ç½®
```yaml
# ç¼–è¯‘ä¼˜åŒ–
torch_compile: true
torch_compile_backend: "inductor"

# Fused ç®—å­
optimizer: adamw_torch_fused

# Flash Attention
flash_attention: true

# æ··åˆç²¾åº¦
bf16: true
tf32: true

# CCE æ’ä»¶
plugins:
  - axolotl.integrations.cut_cross_entropy.CutCrossEntropyPlugin
```

---

### é—®é¢˜ï¼šLoss NaN æˆ–ä¸æ”¶æ•›

#### é…ç½®è°ƒæ•´
```yaml
# ä½¿ç”¨ bf16ï¼ˆæ›´ç¨³å®šï¼‰
bf16: true
fp16: false

# æ¢¯åº¦è£å‰ª
max_grad_norm: 1.0

# é™ä½å­¦ä¹ ç‡
learning_rate: 1e-5  # æˆ–æ›´å°

# Warmup
warmup_steps: 100
# æˆ–
warmup_ratio: 0.1
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”è¡¨

### æ¨¡å‹å¤§å° â†’ TP é…ç½®æ˜ å°„

| æ¨¡å‹å¤§å° | å•èŠ‚ç‚¹ 8 å¡ | åŒèŠ‚ç‚¹ 16 å¡ | å¤‡æ³¨ |
|---------|------------|-------------|------|
| 7B-13B | TP=1, FSDP=8 | TP=1, HSDP=8Ã—2 | ä¸éœ€è¦ TP |
| 30B | TP=2, FSDP=4 | TP=2, HSDP=4Ã—2 | æ¨è TP |
| 70B | TP=2, FSDP=4 | TP=2, HSDP=4Ã—2 | å¿…éœ€ TP |
| 175B+ | TP=4, FSDP=2 | TP=4, HSDP=4Ã—2 | + Pipeline |

### Llama-70B æ€§èƒ½å‚è€ƒ (8Ã—A100 80GB)

| TP Size | æ˜¾å­˜/GPU | Tokens/s/GPU | é€‚ç”¨åœºæ™¯ |
|---------|---------|--------------|---------|
| 1 (çº¯FSDP) | ~65GB | 1800 | åŸºå‡† |
| 2 | ~45GB | 1600 | æ¨è |
| 4 | ~30GB | 1400 | æ˜¾å­˜å—é™ |

---

## ğŸ› ï¸ å¸¸ç”¨ä»£ç ç‰‡æ®µ

### æ£€æŸ¥ DTensor æ˜¯å¦ç”Ÿæ•ˆ
```python
# åœ¨è®­ç»ƒå¼€å§‹å‰æ·»åŠ 
for name, param in model.named_parameters():
    if hasattr(param, 'placements'):
        print(f"âœ… TP å·²ç”Ÿæ•ˆ: {name}")
        print(f"   å…¨å±€å½¢çŠ¶: {param.shape}")
        print(f"   æœ¬åœ°å½¢çŠ¶: {param.local_tensor.shape}")
        break
    else:
        print(f"âŒ TP æœªç”Ÿæ•ˆï¼ˆè¿™æ˜¯æ™®é€š Tensorï¼‰")
        break
```

### ç›‘æ§æ˜¾å­˜ä½¿ç”¨
```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# æˆ–åœ¨ Python ä¸­
import torch
print(f"æ˜¾å­˜å·²ç”¨: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
print(f"æ˜¾å­˜å³°å€¼: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB")
```

### è®¡ç®—æœ‰æ•ˆ Batch Size
```python
effective_batch_size = (
    micro_batch_size *
    gradient_accumulation_steps *
    num_gpus  # å¦‚æœä½¿ç”¨æ•°æ®å¹¶è¡Œ
)

# ä¾‹å¦‚ï¼š
# micro_batch_size = 1
# gradient_accumulation_steps = 8
# num_gpus = 8 (FSDP)
# effective_batch_size = 1 Ã— 8 Ã— 8 = 64
```

---

## âš¡ æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•

### å¿…åšä¼˜åŒ– âœ…
- [ ] å¯ç”¨ Flash Attention (`flash_attention: true`)
- [ ] ä½¿ç”¨ bf16 (`bf16: true`)
- [ ] å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (`gradient_checkpointing: true`)
- [ ] ä½¿ç”¨ Fused Optimizer (`optimizer: adamw_torch_fused`)
- [ ] å¼€å¯ TF32 (`tf32: true`)

### å¯é€‰ä¼˜åŒ– ğŸ”§
- [ ] å¯ç”¨ Torch Compile (`torch_compile: true`)
- [ ] ä½¿ç”¨ CCE æ’ä»¶ (Cut Cross Entropy)
- [ ] è°ƒæ•´ FSDP é¢„å– (`forward_prefetch: true`)
- [ ] ä¼˜åŒ–æ•°æ®åŠ è½½ (`dataloader_num_workers: 4`)

### è°ƒè¯•ä¼˜åŒ– ğŸ›
- [ ] æ£€æŸ¥ GPU æ‹“æ‰‘ (`nvidia-smi topo -m`)
- [ ] ç›‘æ§é€šä¿¡æ—¶é—´ (`NCCL_DEBUG=INFO`)
- [ ] éªŒè¯ batch size (`effective_batch_size`)
- [ ] æ£€æŸ¥ DTensor ç”Ÿæ•ˆï¼ˆè§ä¸Šæ–¹ä»£ç ï¼‰

---

## ğŸ“š å¿«é€Ÿé“¾æ¥

- [è¯¦ç»†æ•™ç¨‹](./tensor_parallelism_deep_dive.md)
- [æºç è§£æ](./tp_source_code_walkthrough.md)
- [Axolotl å®˜æ–¹æ–‡æ¡£](https://docs.axolotl.ai/)
- [ç¤ºä¾‹é…ç½®](../../examples/distributed-parallel/)

---

## ğŸ’¡ é€Ÿè®°å£è¯€

```
TP å¤§å°çœ‹å±‚å®½ï¼Œ
å¿«é€Ÿäº’è¿æ˜¯å…³é”®ã€‚
FSDP èŠ‚çœæ˜¾å­˜é‡ï¼Œ
bf16 ç¨³å®šè®­ç»ƒå¥½ã€‚

å•èŠ‚ç‚¹å†…ç”¨ TPï¼Œ
è·¨èŠ‚ç‚¹é—´ç”¨ DDPã€‚
é•¿ä¸Šä¸‹æ–‡åŠ  CPï¼Œ
å››ç»´å¹¶è¡Œæœ€å¼ºå¤§ã€‚

æ˜¾å­˜ä¸å¤Ÿå¼€ reshardï¼Œ
é€Ÿåº¦ä¸å¿«æŸ¥æ‹“æ‰‘ã€‚
Loss çˆ†ç‚¸é™å­¦ä¹ ç‡ï¼Œ
è°ƒè¯•å…ˆçœ‹ DTensorã€‚
```

---

*æ‰“å°æ­¤é¡µä½œä¸ºé€ŸæŸ¥æ‰‹å†Œ | æœ€åæ›´æ–°ï¼š2025-11*

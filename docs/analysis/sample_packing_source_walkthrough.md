# Sample Packing æºç è§£æ ğŸ”

> **æ·±å…¥ä»£ç å±‚é¢**ï¼šé€è¡Œè§£è¯» Axolotl çš„ Sample Packing å®ç°

---

## ç›®å½•

- [1. æºç æ¶æ„æ€»è§ˆ](#1-æºç æ¶æ„æ€»è§ˆ)
- [2. MultipackBatchSampler è¯¦è§£](#2-multipackbatchsampler-è¯¦è§£)
- [3. FFD æ‰“åŒ…ç®—æ³•å®ç°](#3-ffd-æ‰“åŒ…ç®—æ³•å®ç°)
- [4. Data Collator å®ç°](#4-data-collator-å®ç°)
- [5. Attention Mask å¤„ç†](#5-attention-mask-å¤„ç†)
- [6. Monkeypatch æœºåˆ¶](#6-monkeypatch-æœºåˆ¶)
- [7. ä¸è®­ç»ƒæµç¨‹çš„é›†æˆ](#7-ä¸è®­ç»ƒæµç¨‹çš„é›†æˆ)
- [8. åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ](#8-åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ)

---

## 1. æºç æ¶æ„æ€»è§ˆ

### 1.1 æ ¸å¿ƒæ–‡ä»¶ç»“æ„

```
axolotl/
â”œâ”€â”€ src/axolotl/
â”‚   â”œâ”€â”€ utils/samplers/
â”‚   â”‚   â””â”€â”€ multipack.py                    # â­ æ ¸å¿ƒ: Batch Sampler
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/collators/
â”‚   â”‚   â””â”€â”€ batching.py                     # â­ æ ¸å¿ƒ: Data Collator
â”‚   â”‚
â”‚   â”œâ”€â”€ monkeypatch/
â”‚   â”‚   â”œâ”€â”€ multipack.py                    # â­ æ¨¡å‹patch
â”‚   â”‚   â””â”€â”€ utils.py                        # â­ Attentionå¤„ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ core/builders/
â”‚   â”‚   â””â”€â”€ causal.py                       # é›†æˆåˆ°trainer
â”‚   â”‚
â”‚   â””â”€â”€ core/trainers/mixins/
â”‚       â””â”€â”€ packing.py                      # Trainer mixin
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_multipack.py                   # æµ‹è¯•
```

### 1.2 æ•°æ®æµå›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         è®­ç»ƒæ•°æ®æµ                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. æ•°æ®é›†åŠ è½½
   â†“
   Dataset.__getitem__() â†’ {input_ids, attention_mask, labels}
   â†“

2. Batch Sampling  â† multipack.py: MultipackBatchSampler
   â†“
   è¿”å›: [[idx1, idx2, idx3], [idx4, idx5], ...]  # bins of indices
   â†“

3. Data Collation  â† batching.py: V2BatchSamplerDataCollatorForSeq2Seq
   â†“
   {
     input_ids: [packed_seq],
     attention_mask: [seq_ids],  â† å…³é”®: åºåˆ—IDæ ‡è®°
     position_ids: [0,1,2, 0,1,2,3, ...],
     labels: [packed_seq]
   }
   â†“

4. Model Forward  â† monkeypatch/utils.py: get_unpad_data()
   â†“
   æå–: indices, cu_seqlens, max_seqlen
   â†“

5. Attention Computation  â† Flash Attention / Xformers
   â†“
   ä½¿ç”¨cu_seqlensç¡®ä¿åºåˆ—éš”ç¦»
   â†“

6. Loss Calculation
   â†“
   æ ‡å‡†äº¤å‰ç†µï¼Œpackedåºåˆ—ä¸å½±å“lossè®¡ç®—
```

### 1.3 å…³é”®ç±»å…³ç³»

```
DataLoader
    â”‚
    â”œâ”€ sampler: MultipackBatchSampler
    â”‚   â”œâ”€ _lengths: List[int]          # æ¯ä¸ªæ ·æœ¬çš„é•¿åº¦
    â”‚   â”œâ”€ _batches: List[List[List[int]]]  # ç¼“å­˜çš„batches
    â”‚   â””â”€ generate_batches() â†’ List[List[List[int]]]
    â”‚
    â””â”€ collate_fn: V2BatchSamplerDataCollatorForSeq2Seq
        â””â”€ __call__(features) â†’ Dict[str, Tensor]

Trainer
    â”œâ”€ train_dataset
    â”œâ”€ data_collator: V2BatchSamplerDataCollatorForSeq2Seq
    â””â”€ args.sample_packing = True

Model (patched)
    â””â”€ attention.forward()
        â””â”€ get_unpad_data(attention_mask)
            â””â”€ flash_attn_varlen_func(..., cu_seqlens=...)
```

---

## 2. MultipackBatchSampler è¯¦è§£

### 2.1 ç±»å®šä¹‰

**æ–‡ä»¶**: `src/axolotl/utils/samplers/multipack.py`

```python
class MultipackBatchSampler(BatchSampler):
    """
    Batch sampler for efficient packing of variable-length sequences.

    æ ¸å¿ƒèŒè´£:
    1. æ¥æ”¶æ•°æ®é›†å’Œåºåˆ—é•¿åº¦
    2. ä½¿ç”¨FFDç®—æ³•å°†åºåˆ—æ‰“åŒ…æˆbins
    3. è¿”å›æ‰“åŒ…åçš„batchç´¢å¼•
    """

    _batches: list[list[list[int]]] | None = None  # ç¼“å­˜batches
    _epoch: int = 0
    _efficiency: float = 0.0
    _len_packed_dataset: int = 0
```

### 2.2 åˆå§‹åŒ–

```python
def __init__(
    self,
    sampler: Sampler[int],              # åº•å±‚sampler (RandomSamplerç­‰)
    batch_size: int,                    # æ¯ä¸ªbatchåŒ…å«å¤šå°‘bins
    drop_last: bool,                    # æ˜¯å¦ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„batch
    batch_max_len: int,                 # æ¯ä¸ªbinçš„æœ€å¤§tokenå®¹é‡
    lengths: list[int],                 # é¢„å…ˆè®¡ç®—çš„åºåˆ—é•¿åº¦
    packing_efficiency_estimate: float = 1.0,  # é¢„ä¼°æ‰“åŒ…æ•ˆç‡
    group_size: int = 100000,           # FFDåˆ†ç»„å¤§å°
    bin_size: int = 200,                # æ¯ä¸ªbinæœ€å¤šå®¹çº³åºåˆ—æ•°
    packing_sequentially: bool = False, # æ˜¯å¦é¡ºåºæ‰“åŒ…
):
    super().__init__(sampler, batch_size, drop_last)

    # ä¿å­˜å‚æ•°
    self.batch_max_len = batch_max_len
    self.lengths = lengths
    self.packing_efficiency_estimate = packing_efficiency_estimate
    self.group_size = group_size
    self.bin_size = bin_size
    self.packing_sequentially = packing_sequentially

    # åˆ†å¸ƒå¼è®¾ç½®
    if dist.is_available() and dist.is_initialized():
        self.rank = dist.get_rank()
        self.num_replicas = dist.get_world_size()
    else:
        self.rank = 0
        self.num_replicas = 1

    # è®¡ç®—æ‰“åŒ…åçš„æ•°æ®é›†å¤§å°
    self._estimate_packed_length()
```

**å…³é”®ç‚¹è§£æ**:

```python
# batch_size vs batch_max_len çš„åŒºåˆ«:

# batch_size: æ¯ä¸ªtraining stepå¤„ç†å¤šå°‘ä¸ªbins
# ä¾‹: batch_size=4 â†’ æ¯æ¬¡forward 4ä¸ªbins

# batch_max_len: æ¯ä¸ªbinçš„tokenå®¹é‡
# ä¾‹: batch_max_len=2048 â†’ æ¯ä¸ªbinæœ€å¤š2048 tokens

# å®é™…batchçš„tokenæ•°:
total_tokens_per_batch = batch_size Ã— batch_max_len Ã— efficiency
# ä¾‹: 4 Ã— 2048 Ã— 0.95 = ~7782 tokens/batch
```

### 2.3 æ ¸å¿ƒæ–¹æ³•: generate_batches()

```python
def generate_batches(self, set_stats: bool = False) -> list[list[list[int]]]:
    """
    ç”Ÿæˆæ‰“åŒ…åçš„batches

    è¿”å›æ ¼å¼:
    [
        [[idx1, idx2], [idx3, idx4, idx5]],  # Batch 1: 2 bins
        [[idx6], [idx7, idx8, idx9, idx10]], # Batch 2: 2 bins
        ...
    ]
    """
    # 1. è·å–å½“å‰epochçš„æ ·æœ¬ç´¢å¼•
    sampler_indices = list(self.sampler)
    if hasattr(self.sampler, "generator"):
        # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°
        self.sampler.generator.manual_seed(self.epoch)

    # 2. æå–å¯¹åº”çš„åºåˆ—é•¿åº¦
    sequence_lengths = np.array(
        [self.lengths[i] for i in sampler_indices],
        dtype=np.int32
    )

    # 3. æ ¹æ®æ¨¡å¼é€‰æ‹©æ‰“åŒ…ç®—æ³•
    if self.packing_sequentially:
        # é¡ºåºæ‰“åŒ… (ä¿æŒåŸå§‹é¡ºåº)
        batches_indices = allocate_sequentially(
            sequence_lengths=sequence_lengths,
            rank=self.rank,
            bin_capacity=self.batch_max_len,
            num_ranks=self.num_replicas,
        )
    else:
        # å¹¶è¡Œæ‰“åŒ… (FFDç®—æ³•ï¼Œæ›´é«˜æ•ˆ)
        batches_indices = pack_parallel(
            sequence_lengths=sequence_lengths,
            bin_capacity=self.batch_max_len,
            group_size=self.group_size,
            bin_size=self.bin_size,
            num_processes=None,  # è‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
        )

    # 4. å°†binå†…çš„ç´¢å¼•æ˜ å°„å›åŸå§‹æ•°æ®é›†ç´¢å¼•
    batches = [
        [sampler_indices[i] for i in batch_bin]
        for batch_bin in batches_indices
    ]

    # 5. åˆ†ç»„æˆbatches (æ¯batch_sizeä¸ªbinsä¸ºä¸€ç»„)
    batches = [
        batches[i : i + self.batch_size]
        for i in range(0, len(batches), self.batch_size)
    ]

    # 6. å¤„ç†æœ€åä¸€ä¸ªä¸å®Œæ•´çš„batch
    if self.drop_last and len(batches[-1]) != self.batch_size:
        batches = batches[:-1]

    # 7. ç»Ÿè®¡æ•ˆç‡ (å¯é€‰)
    if set_stats:
        self._compute_efficiency(batches, sequence_lengths)

    return batches
```

**é€æ­¥è§£æ**:

#### æ­¥éª¤1: è·å–æ ·æœ¬ç´¢å¼•

```python
sampler_indices = list(self.sampler)
# ä¾‹å¦‚ RandomSampler ä¼šè¿”å›æ‰“ä¹±çš„ç´¢å¼•:
# [2045, 67, 1234, 89, 3456, ...]

# è¿™äº›æ˜¯æ•°æ®é›†ä¸­çš„åŸå§‹ç´¢å¼•
```

#### æ­¥éª¤2: æå–åºåˆ—é•¿åº¦

```python
sequence_lengths = np.array([self.lengths[i] for i in sampler_indices])
# self.lengthsæ˜¯é¢„å…ˆè®¡ç®—å¥½çš„æ‰€æœ‰åºåˆ—é•¿åº¦
# ä¾‹å¦‚: lengths = [512, 1024, 256, 2048, 800, ...]

# æå–å:
# sequence_lengths = [800, 256, 512, ...]  (æŒ‰sampler_indicesé¡ºåº)
```

#### æ­¥éª¤3-4: æ‰“åŒ…å¹¶æ˜ å°„

```python
# pack_parallelè¿”å›çš„æ˜¯ç›¸å¯¹ç´¢å¼• (ç›¸å¯¹äºsequence_lengthsæ•°ç»„)
batches_indices = [[0, 5, 8], [1, 2], [3, 4, 6, 7], ...]
#                   â†‘ è¿™äº›æ˜¯sequence_lengthsä¸­çš„ä½ç½®

# æ˜ å°„å›åŸå§‹æ•°æ®é›†ç´¢å¼•:
batches = [[sampler_indices[0], sampler_indices[5], sampler_indices[8]], ...]
#        = [[2045, 3456, ...], ...]  # åŸå§‹æ•°æ®é›†ç´¢å¼•
```

#### æ­¥éª¤5: åˆ†ç»„æˆbatches

```python
# å‡è®¾ batch_size=2
# batches_indices (bins): [[0,5,8], [1,2], [3,4,6,7], [9,10], [11], ...]

# åˆ†ç»„:
batches = [
    [[0,5,8], [1,2]],        # Batch 1: 2 bins
    [[3,4,6,7], [9,10]],     # Batch 2: 2 bins
    [[11]],                  # Batch 3: 1 bin (ä¸å®Œæ•´)
]

# å¦‚æœ drop_last=True:
batches = [
    [[0,5,8], [1,2]],
    [[3,4,6,7], [9,10]],
]  # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„batch
```

### 2.4 æ•ˆç‡ç»Ÿè®¡

```python
def _compute_efficiency(self, batches, sequence_lengths):
    """è®¡ç®—å®é™…æ‰“åŒ…æ•ˆç‡"""
    total_tokens = 0
    total_capacity = 0

    for batch in batches:
        for bin_indices in batch:
            # ç»Ÿè®¡binå†…çš„å®é™…tokens
            bin_tokens = sum(sequence_lengths[i] for i in bin_indices)
            total_tokens += bin_tokens
            total_capacity += self.batch_max_len

    self._efficiency = total_tokens / total_capacity if total_capacity > 0 else 0.0

    LOG.info(f"Sample packing efficiency: {self._efficiency:.3f}")
```

**è¾“å‡ºç¤ºä¾‹**:

```
[INFO] Sample packing efficiency: 0.923
# æ„å‘³ç€: 92.3%çš„token slotsè¢«æœ‰æ•ˆåˆ©ç”¨ï¼Œåªæœ‰7.7%æ˜¯padding
```

### 2.5 Epochç®¡ç†

```python
def set_epoch(self, epoch: int):
    """
    è®¾ç½®epochï¼Œè§¦å‘é‡æ–°ç”Ÿæˆbatches

    ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªæ–¹æ³•?
    - æ¯ä¸ªepochéœ€è¦é‡æ–°shuffleæ•°æ®
    - é‡æ–°shuffleåéœ€è¦é‡æ–°æ‰“åŒ…
    - æ¸…ç©º_batchesç¼“å­˜
    """
    self.epoch = epoch
    self._batches = None  # æ¸…ç©ºç¼“å­˜ï¼Œå¼ºåˆ¶ä¸‹æ¬¡é‡æ–°ç”Ÿæˆ

def __iter__(self):
    """è¿­ä»£å™¨æ¥å£"""
    if self._batches is None:
        # é¦–æ¬¡è°ƒç”¨æˆ–ç¼“å­˜è¢«æ¸…ç©ºï¼Œç”Ÿæˆæ–°batches
        self._batches = self.generate_batches(set_stats=True)

    for batch in self._batches:
        yield batch
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­
for epoch in range(num_epochs):
    # è®¾ç½®æ–°epoch
    train_loader.batch_sampler.set_epoch(epoch)
    # â†‘ è¿™ä¼šæ¸…ç©ºç¼“å­˜ï¼Œè§¦å‘é‡æ–°shuffleå’Œæ‰“åŒ…

    for batch in train_loader:
        # è®­ç»ƒ...
        pass
```

---

## 3. FFD æ‰“åŒ…ç®—æ³•å®ç°

### 3.1 å¹¶è¡Œæ‰“åŒ…å…¥å£

**æ–‡ä»¶**: `src/axolotl/utils/samplers/multipack.py:125-190`

```python
def pack_parallel(
    sequence_lengths: np.ndarray,  # [N,] åºåˆ—é•¿åº¦æ•°ç»„
    bin_capacity: int,             # binå®¹é‡ (å¦‚2048)
    group_size: int,               # åˆ†ç»„å¤§å° (å¦‚100000)
    bin_size: int,                 # æ¯binæœ€å¤šåºåˆ—æ•° (å¦‚200)
    num_processes: int = None,     # å¹¶è¡Œè¿›ç¨‹æ•°
) -> list[list[int]]:
    """
    å¹¶è¡ŒFFDæ‰“åŒ…ç®—æ³•

    æ ¸å¿ƒæ€æƒ³:
    1. æŒ‰é•¿åº¦é™åºæ’åº (D in FFD)
    2. åˆ†æˆå¤šç»„å¹¶è¡Œå¤„ç†
    3. æ¯ç»„å†…ä½¿ç”¨FFDç®—æ³•
    4. åˆå¹¶ç»“æœ
    """

    # æ­¥éª¤1: æŒ‰é•¿åº¦é™åºæ’åº
    sorted_indices = np.argsort(-sequence_lengths)  # è´Ÿå·å®ç°é™åº
    sorted_lengths = sequence_lengths[sorted_indices]

    # ç¤ºä¾‹:
    # åŸå§‹: lengths = [512, 2048, 256, 1024]
    # æ’åºå: sorted_lengths = [2048, 1024, 512, 256]
    #        sorted_indices = [1, 3, 0, 2]

    # æ­¥éª¤2: åˆ†ç»„
    num_groups = (len(sorted_lengths) + group_size - 1) // group_size
    groups = []

    for i in range(num_groups):
        start = i * group_size
        end = min((i + 1) * group_size, len(sorted_lengths))
        groups.append((
            sorted_lengths[start:end],  # è¯¥ç»„çš„åºåˆ—é•¿åº¦
            start,                      # è¯¥ç»„çš„èµ·å§‹offset
        ))

    # ç¤ºä¾‹: å¦‚æœæœ‰250Kåºåˆ—ï¼Œgroup_size=100K
    # groups = [
    #     (sorted_lengths[0:100K], 0),
    #     (sorted_lengths[100K:200K], 100K),
    #     (sorted_lengths[200K:250K], 200K),
    # ]

    # æ­¥éª¤3: ä¼°ç®—éœ€è¦çš„binsæ•°é‡
    total_length = np.sum(sorted_lengths)
    num_bins = int(np.ceil(total_length / bin_capacity))
    # ç†è®ºæœ€å°‘bins: æ€»é•¿åº¦ / binå®¹é‡
    # å®é™…ä¼šç¨å¤š (å› ä¸ºæ‰“åŒ…ä¸å®Œç¾)

    # æ­¥éª¤4: å¤šè¿›ç¨‹å¹¶è¡Œæ‰“åŒ…
    if num_processes is None:
        import multiprocessing as mp
        num_processes = mp.cpu_count()

    if num_processes > 1 and num_groups > 1:
        # ä½¿ç”¨è¿›ç¨‹æ± 
        import multiprocessing as mp
        with mp.Pool(num_processes) as pool:
            results = pool.starmap(
                pack_group,
                [
                    (
                        group_lengths,
                        group_offset,
                        bin_capacity,
                        num_bins,  # æ¯ç»„å…±äº«binæ± 
                        bin_size,
                        True,  # safe_mode
                    )
                    for group_lengths, group_offset in groups
                ],
            )
    else:
        # å•è¿›ç¨‹å¤„ç† (æ•°æ®é‡å°æˆ–num_processes=1)
        results = [
            pack_group(
                group_lengths,
                group_offset,
                bin_capacity,
                num_bins,
                bin_size,
                True,
            )
            for group_lengths, group_offset in groups
        ]

    # æ­¥éª¤5: åˆå¹¶æ‰€æœ‰ç»„çš„ç»“æœ
    all_bins = []
    for group_bins in results:
        for bin_content in group_bins:
            if len(bin_content) > 0:
                all_bins.append(bin_content)

    # æ­¥éª¤6: æ˜ å°„å›åŸå§‹ç´¢å¼•
    final_bins = []
    for bin_content in all_bins:
        # bin_contentåŒ…å«çš„æ˜¯sorted_indicesä¸­çš„ä½ç½®
        # éœ€è¦æ˜ å°„å›åŸå§‹æ•°æ®é›†ç´¢å¼•
        original_indices = [sorted_indices[i] for i in bin_content]
        final_bins.append(original_indices)

    return final_bins
```

**ä¸ºä»€ä¹ˆè¦åˆ†ç»„?**

```python
# é—®é¢˜: å¦‚æœæœ‰1Mä¸ªåºåˆ—ï¼Œç›´æ¥FFDä¼šå¾ˆæ…¢
# O(N Ã— M) å…¶ä¸­N=åºåˆ—æ•°, M=binsæ•°
# 1M Ã— 10K = 10B æ¬¡æ“ä½œ!

# è§£å†³: åˆ†ç»„å¤„ç†
# - æ¯ç»„100Kåºåˆ—
# - 10ç»„å¹¶è¡Œå¤„ç†
# - æ¯ç»„å¤æ‚åº¦: O(100K Ã— 1K) = 100M
# - æ€»æ—¶é—´: 100M / 10 (å¹¶è¡Œ) = 10M æ“ä½œæ—¶é—´
# åŠ é€Ÿæ¯”: 100å€!
```

### 3.2 æ ¸å¿ƒFFDå®ç°

```python
@numba.njit  # â† Numba JITç¼–è¯‘ï¼ŒåŠ é€Ÿ30-50å€!
def pack_group(
    sequence_lengths: np.ndarray,  # è¯¥ç»„çš„åºåˆ—é•¿åº¦
    group_offset: int,             # è¯¥ç»„çš„èµ·å§‹offset
    bin_capacity: int,             # binå®¹é‡
    max_bins: int,                 # æœ€å¤šbinsæ•°
    bin_size: int,                 # æ¯binæœ€å¤šåºåˆ—æ•°
    safe_mode: bool = True,
) -> list[list[int]]:
    """
    First-Fit Decreasing bin packingç®—æ³•

    ç®—æ³•æµç¨‹:
    1. éå†æ¯ä¸ªåºåˆ— (å·²æŒ‰é•¿åº¦é™åºæ’åˆ—)
    2. å°è¯•æ”¾å…¥ç¬¬ä¸€ä¸ªèƒ½å®¹çº³å®ƒçš„bin (First-Fit)
    3. å¦‚æœæ‰€æœ‰binéƒ½æ”¾ä¸ä¸‹ï¼Œåˆ›å»ºæ–°bin
    """

    # åˆå§‹åŒ–æ•°æ®ç»“æ„
    bins_remaining = np.full(max_bins, bin_capacity, dtype=np.int32)
    # bins_remaining[i]: bin i çš„å‰©ä½™å®¹é‡
    # åˆå§‹: [2048, 2048, 2048, ...]

    bin_contents = [[] for _ in range(max_bins)]
    # bin_contents[i]: bin i åŒ…å«çš„åºåˆ—ç´¢å¼•åˆ—è¡¨

    # éå†æ¯ä¸ªåºåˆ—
    for i, length in enumerate(sequence_lengths):
        global_index = group_offset + i  # å…¨å±€ç´¢å¼•

        # å®‰å…¨æ£€æŸ¥: è·³è¿‡è¶…é•¿åºåˆ—
        if safe_mode and length > bin_capacity:
            continue

        # First-Fit: æ‰¾ç¬¬ä¸€ä¸ªèƒ½æ”¾ä¸‹çš„bin
        placed = False
        for b in range(max_bins):
            # æ£€æŸ¥ä¸¤ä¸ªæ¡ä»¶:
            # 1. å®¹é‡è¶³å¤Ÿ
            # 2. åºåˆ—æ•°é‡æœªè¾¾ä¸Šé™
            if (bins_remaining[b] >= length and
                len(bin_contents[b]) < bin_size):

                # æ”¾å…¥è¯¥bin
                bins_remaining[b] -= length
                bin_contents[b].append(global_index)
                placed = True
                break  # First-Fit: æ‰¾åˆ°ç¬¬ä¸€ä¸ªå°±åœæ­¢

        # å¦‚æœæ‰€æœ‰ç°æœ‰binéƒ½æ”¾ä¸ä¸‹ï¼Œåˆ›å»ºæ–°bin
        if not placed:
            # æ‰¾ç¬¬ä¸€ä¸ªç©ºbin
            for b in range(max_bins):
                if len(bin_contents[b]) == 0:
                    bins_remaining[b] = bin_capacity - length
                    bin_contents[b].append(global_index)
                    break

    # è¿”å›éç©ºbins
    result = []
    for b in range(max_bins):
        if len(bin_contents[b]) > 0:
            result.append(bin_contents[b])

    return result
```

**é€æ­¥ç¤ºä¾‹**:

```python
# è¾“å…¥:
sequence_lengths = [2048, 1024, 1024, 800, 512, 256]
bin_capacity = 2048
bin_size = 10

# åˆå§‹çŠ¶æ€:
bins_remaining = [2048, 2048, 2048, ...]
bin_contents = [[], [], [], ...]

# å¤„ç† seq[0]=2048:
# - æ£€æŸ¥ bin[0]: 2048 >= 2048 âœ… â†’ æ”¾å…¥bin[0]
bins_remaining = [0, 2048, 2048, ...]
bin_contents = [[0], [], [], ...]

# å¤„ç† seq[1]=1024:
# - æ£€æŸ¥ bin[0]: 0 >= 1024 âŒ
# - æ£€æŸ¥ bin[1]: 2048 >= 1024 âœ… â†’ æ”¾å…¥bin[1]
bins_remaining = [0, 1024, 2048, ...]
bin_contents = [[0], [1], [], ...]

# å¤„ç† seq[2]=1024:
# - æ£€æŸ¥ bin[0]: 0 >= 1024 âŒ
# - æ£€æŸ¥ bin[1]: 1024 >= 1024 âœ… â†’ æ”¾å…¥bin[1]
bins_remaining = [0, 0, 2048, ...]
bin_contents = [[0], [1,2], [], ...]

# å¤„ç† seq[3]=800:
# - æ£€æŸ¥ bin[0]: 0 >= 800 âŒ
# - æ£€æŸ¥ bin[1]: 0 >= 800 âŒ
# - æ£€æŸ¥ bin[2]: 2048 >= 800 âœ… â†’ æ”¾å…¥bin[2]
bins_remaining = [0, 0, 1248, ...]
bin_contents = [[0], [1,2], [3], ...]

# å¤„ç† seq[4]=512:
# - æ£€æŸ¥ bin[0]: 0 >= 512 âŒ
# - æ£€æŸ¥ bin[1]: 0 >= 512 âŒ
# - æ£€æŸ¥ bin[2]: 1248 >= 512 âœ… â†’ æ”¾å…¥bin[2]
bins_remaining = [0, 0, 736, ...]
bin_contents = [[0], [1,2], [3,4], ...]

# å¤„ç† seq[5]=256:
# - æ£€æŸ¥ bin[0]: 0 >= 256 âŒ
# - æ£€æŸ¥ bin[1]: 0 >= 256 âŒ
# - æ£€æŸ¥ bin[2]: 736 >= 256 âœ… â†’ æ”¾å…¥bin[2]
bins_remaining = [0, 0, 480, ...]
bin_contents = [[0], [1,2], [3,4,5], ...]

# æœ€ç»ˆç»“æœ:
# Bin 0: [seq0] = 2048 tokens (100%åˆ©ç”¨ç‡)
# Bin 1: [seq1, seq2] = 2048 tokens (100%åˆ©ç”¨ç‡)
# Bin 2: [seq3, seq4, seq5] = 1568 tokens (76.6%åˆ©ç”¨ç‡)
# å¹³å‡åˆ©ç”¨ç‡: (2048+2048+1568)/(3Ã—2048) = 91.4%
```

### 3.3 é¡ºåºæ‰“åŒ… (Sequential)

```python
@numba.njit
def allocate_sequentially(
    sequence_lengths: np.ndarray,
    rank: int,           # å½“å‰rank
    bin_capacity: int,
    num_ranks: int,      # æ€»rankæ•°
) -> list[list[int]]:
    """
    é¡ºåºæ‰“åŒ…: ä¸æ’åºï¼ŒæŒ‰åŸå§‹é¡ºåºå¤„ç†

    ç”¨é€”:
    - Curriculum learning (æ•°æ®é¡ºåºå¾ˆé‡è¦)
    - éœ€è¦ä¿æŒæ•°æ®é¡ºåºçš„åœºæ™¯

    åŒºåˆ«äºå¹¶è¡ŒFFD:
    - ä¸æ’åº (æ•ˆç‡ç¨ä½)
    - å•è¿›ç¨‹ (ä¸å¹¶è¡Œ)
    - ä¿æŒåŸå§‹é¡ºåº
    """

    bins_remaining = []
    bin_contents = []

    # éå†åºåˆ— (æŒ‰åŸå§‹é¡ºåº)
    for i, length in enumerate(sequence_lengths):
        # è·³è¿‡è¶…é•¿åºåˆ—
        if length > bin_capacity:
            continue

        # First-Fit
        placed = False
        for b in range(len(bins_remaining)):
            if bins_remaining[b] >= length:
                bins_remaining[b] -= length
                bin_contents[b].append(i)
                placed = True
                break

        # åˆ›å»ºæ–°bin
        if not placed:
            bins_remaining.append(bin_capacity - length)
            bin_contents.append([i])

    # åˆ†å¸ƒå¼è®­ç»ƒ: åªè¿”å›å±äºå½“å‰rankçš„bins
    if num_ranks > 1:
        # è½®è¯¢åˆ†é…: rank 0 è·å–bin 0,3,6,...
        #          rank 1 è·å–bin 1,4,7,...
        result = []
        for b in range(rank, len(bin_contents), num_ranks):
            result.append(bin_contents[b])
        return result
    else:
        return bin_contents
```

**é¡ºåº vs å¹¶è¡Œ FFD å¯¹æ¯”**:

```python
# æ•°æ®: [100, 2000, 200, 1800, 300, 1700]
# bin_capacity = 2048

# å¹¶è¡ŒFFD (æ’åº):
# æ’åºå: [2000, 1800, 1700, 300, 200, 100]
# Bin 1: [2000] â†’ 2000/2048 = 97.7%
# Bin 2: [1800, 200] â†’ 2000/2048 = 97.7%
# Bin 3: [1700, 300] â†’ 2000/2048 = 97.7%
# Bin 4: [100] â†’ 100/2048 = 4.9%
# å¹³å‡: 74.5%

# é¡ºåºFFD (ä¸æ’åº):
# æŒ‰åŸå§‹: [100, 2000, 200, 1800, 300, 1700]
# Bin 1: [100, 200, 300, ...] â†’ å°è¯•å¡«æ»¡
# Bin 2: [2000] â†’ 2000/2048 = 97.7%
# Bin 3: [1800] â†’ 1800/2048 = 87.9%
# Bin 4: [1700] â†’ 1700/2048 = 83.0%
# æ•ˆç‡é€šå¸¸è¾ƒä½

# ä½†é¡ºåºå¾ˆé‡è¦æ—¶ï¼Œå¿…é¡»ä½¿ç”¨Sequential!
```

---

## 4. Data Collator å®ç°

### 4.1 V2BatchSamplerDataCollatorForSeq2Seq

**æ–‡ä»¶**: `src/axolotl/utils/collators/batching.py:159-196`

```python
@dataclass
class V2BatchSamplerDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):
    """
    V2 Collator: æ”¯æŒæ›´å¹¿æ³›çš„æ¨¡å‹

    å…³é”®æ”¹è¿›:
    - Attention maskä½¿ç”¨åºåˆ—ID (1,2,3,...)
    - æ›´å¥½çš„position_idså¤„ç†
    - æ”¯æŒéFlash Attentionæ¨¡å‹
    """

    squash_position_ids: bool = False  # æ˜¯å¦å‹å¹³position_ids

    def __call__(self, features, return_tensors=None):
        """
        è¾“å…¥: List[List[dict]] æˆ– List[dict]
              å¤–å±‚List: batchä¸­çš„bins
              å†…å±‚List: binä¸­çš„sequences

        è¾“å‡º: Dict[str, Tensor]
              æ‹¼æ¥å¹¶padåçš„batch
        """

        # è§„èŒƒåŒ–è¾“å…¥æ ¼å¼
        if not isinstance(features[0], list):
            features: List[List[dict]] = [features]
        # ç°åœ¨ features = [[seq1, seq2], [seq3, seq4, seq5], ...]
        #                  \_bin1_/      \_____bin2______/

        # ä¸ºæ¯ä¸ªbinåˆ›å»ºè¾“å‡ºdict
        out_features = [{} for _ in features]

        # å¤„ç†æ¯ä¸ªbin
        for i, bin_sequences in enumerate(features):
            # bin_sequences = [seq1_dict, seq2_dict, seq3_dict]

            # éå†æ‰€æœ‰feature keys
            for feature_name in bin_sequences[0].keys():
                if feature_name == "length":
                    continue  # è·³è¿‡è¾…åŠ©å­—æ®µ

                if feature_name == "attention_mask":
                    # â­ å…³é”®å¤„ç†: attention_mask
                    arrays = [
                        (seq_idx + 1) * np.array(seq[feature_name])
                        for seq_idx, seq in enumerate(bin_sequences)
                    ]
                    # ç¤ºä¾‹:
                    # seq 0: [1,1,1] â†’ (0+1)*[1,1,1] = [1,1,1]
                    # seq 1: [1,1,1,1] â†’ (1+1)*[1,1,1,1] = [2,2,2,2]
                    # seq 2: [1,1] â†’ (2+1)*[1,1] = [3,3]

                    out_features[i][feature_name] = np.concatenate(arrays)
                    # â†’ [1,1,1, 2,2,2,2, 3,3]

                elif feature_name == "position_ids" and self.squash_position_ids:
                    # å¯é€‰: å‹å¹³position_ids
                    # (æŸäº›æ¨¡å‹éœ€è¦è¿ç»­çš„position_ids)
                    arrays = [
                        np.array(seq[feature_name])
                        for seq in bin_sequences
                    ]
                    position_ids = np.concatenate(arrays)
                    total_length = position_ids.shape[0]
                    # é‡æ–°ç”Ÿæˆè¿ç»­çš„position_ids
                    position_ids = np.arange(total_length)
                    out_features[i][feature_name] = position_ids

                else:
                    # å…¶ä»–å­—æ®µ: ç›´æ¥æ‹¼æ¥
                    # input_ids, labels, position_ids (é»˜è®¤)
                    arrays = [
                        np.array(seq[feature_name])
                        for seq in bin_sequences
                    ]
                    out_features[i][feature_name] = np.concatenate(arrays)

        # è°ƒç”¨çˆ¶ç±»çš„__call__è¿›è¡Œpadding
        # å°†æ‰€æœ‰bins padåˆ°ç›¸åŒé•¿åº¦
        return super().__call__(out_features, return_tensors=return_tensors)
```

**å®Œæ•´ç¤ºä¾‹**:

```python
# è¾“å…¥: batchåŒ…å«2ä¸ªbins
features = [
    # Bin 1: 2ä¸ªåºåˆ—
    [
        {
            'input_ids': [101, 102, 103],
            'attention_mask': [1, 1, 1],
            'position_ids': [0, 1, 2],
            'labels': [101, 102, 103],
        },
        {
            'input_ids': [201, 202, 203, 204],
            'attention_mask': [1, 1, 1, 1],
            'position_ids': [0, 1, 2, 3],
            'labels': [201, 202, 203, 204],
        },
    ],
    # Bin 2: 1ä¸ªåºåˆ—
    [
        {
            'input_ids': [301, 302],
            'attention_mask': [1, 1],
            'position_ids': [0, 1],
            'labels': [301, 302],
        },
    ],
]

# å¤„ç†å (æ‹¼æ¥ä½†æœªpadding):
out_features = [
    # Bin 1 (æ‹¼æ¥å)
    {
        'input_ids': np.array([101,102,103, 201,202,203,204]),
        'attention_mask': np.array([1,1,1, 2,2,2,2]),  # â† åºåˆ—ID!
        'position_ids': np.array([0,1,2, 0,1,2,3]),    # â† ç‹¬ç«‹è®¡æ•°
        'labels': np.array([101,102,103, 201,202,203,204]),
    },
    # Bin 2 (æ‹¼æ¥å)
    {
        'input_ids': np.array([301, 302]),
        'attention_mask': np.array([1, 1]),
        'position_ids': np.array([0, 1]),
        'labels': np.array([301, 302]),
    },
]

# è°ƒç”¨çˆ¶ç±»padding (padåˆ°batchå†…æœ€é•¿=7):
final_output = {
    'input_ids': torch.tensor([
        [101,102,103, 201,202,203,204],  # Bin 1
        [301,302, 0,0,0,0,0],            # Bin 2 + padding
    ]),
    'attention_mask': torch.tensor([
        [1,1,1, 2,2,2,2],
        [1,1, 0,0,0,0,0],  # â† paddingçš„mask=0
    ]),
    'position_ids': torch.tensor([
        [0,1,2, 0,1,2,3],
        [0,1, 0,0,0,0,0],  # â† paddingçš„position_ids=0
    ]),
    'labels': torch.tensor([
        [101,102,103, 201,202,203,204],
        [301,302, -100,-100,-100,-100,-100],  # â† paddingçš„label=-100
    ]),
}
```

### 4.2 V1 vs V2 Collator

```python
# V1: BatchSamplerDataCollatorForSeq2Seq
class V1:
    def __call__(self, features):
        # attention_mask: æ‰€æœ‰åºåˆ—éƒ½ä¹˜ä»¥1
        arrays = [
            (1) * np.array(item[feature])  # â† æ³¨æ„è¿™é‡Œ
            for item in features
        ]
        # ç»“æœ: [1,1,1, 1,1,1,1, 1,1]
        #       â†‘ æ— æ³•åŒºåˆ†ä¸åŒåºåˆ—!

# V2: V2BatchSamplerDataCollatorForSeq2Seq
class V2:
    def __call__(self, features):
        # attention_mask: æ¯ä¸ªåºåˆ—ä¹˜ä»¥ä¸åŒID
        arrays = [
            (i + 1) * np.array(item[feature])  # â† å…³é”®å·®å¼‚
            for i, item in enumerate(features)
        ]
        # ç»“æœ: [1,1,1, 2,2,2,2, 3,3]
        #       â†‘ å¯ä»¥åŒºåˆ†ä¸åŒåºåˆ—!
```

**ä¸ºä»€ä¹ˆéœ€è¦V2?**

```python
# V1é€‚ç”¨äº: Flash Attention (nativeæ”¯æŒmultipack)
# - Flash Attentionå¯ä»¥ç›´æ¥å¤„ç†packed sequences
# - é€šè¿‡cu_seqlenså‚æ•°çŸ¥é“åºåˆ—è¾¹ç•Œ

# V2é€‚ç”¨äº: éFlash Attentionæ¨¡å‹ (å¦‚æ ‡å‡†Attention, SDPA)
# - éœ€è¦é€šè¿‡attention_maskåŒºåˆ†åºåˆ—
# - åœ¨mask_2d_to_4dä¸­æ„å»ºblock-diagonal mask
```

---

## 5. Attention Mask å¤„ç†

### 5.1 get_unpad_data()

**æ–‡ä»¶**: `src/axolotl/monkeypatch/utils.py:31-45`

```python
@torch.jit.script  # â† JITç¼–è¯‘ï¼ŒåŠ é€Ÿæ¨ç†
def get_unpad_data(attention_mask: torch.Tensor):
    """
    ä»packed attention_maskä¸­æå–æœ‰æ•ˆtokenä½ç½®å’Œåºåˆ—è¾¹ç•Œ

    è¾“å…¥: attention_mask with sequence IDs
          shape: [batch, total_tokens]
          ç¤ºä¾‹: [[1,1,1, 2,2,2,2, 3,3, 0,0]]

    è¾“å‡º: (indices, cu_seqlens, max_seqlen)
    """
    device = attention_mask.device

    # 1. è·å–æ¯ä¸ªåºåˆ—çš„é•¿åº¦
    seqlens_in_batch = get_max_seqlen_in_batch(attention_mask)
    # ç¤ºä¾‹: [3, 4, 2]  (3ä¸ªåºåˆ—ï¼Œé•¿åº¦åˆ†åˆ«ä¸º3,4,2)

    # 2. è·å–æ‰€æœ‰éé›¶tokençš„ä½ç½®
    indices = torch.nonzero(attention_mask.flatten()).flatten()
    # ç¤ºä¾‹: tensor([0,1,2, 3,4,5,6, 7,8])
    #              â†‘ ä½ç½®0-8æ˜¯æœ‰æ•ˆtokensï¼Œ9-10æ˜¯padding

    # 3. è®¡ç®—æœ€é•¿åºåˆ—é•¿åº¦
    max_seqlen_in_batch = seqlens_in_batch.max().item()
    # ç¤ºä¾‹: 4

    # 4. è®¡ç®—ç´¯ç§¯åºåˆ—é•¿åº¦ (cu_seqlens)
    cu_seqlens = F.pad(
        torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32),
        (1, 0)  # åœ¨å‰é¢padä¸€ä¸ª0
    ).to(device=device).detach()
    # cumsum([3,4,2]) = [3, 7, 9]
    # pad: [0, 3, 7, 9]  â† æ¯ä¸ªåºåˆ—çš„èµ·å§‹ä½ç½®

    return (
        indices,           # æœ‰æ•ˆtokenä½ç½®
        cu_seqlens,        # åºåˆ—è¾¹ç•Œ
        max_seqlen_in_batch,  # æœ€é•¿åºåˆ—é•¿åº¦
    )
```

**é€æ­¥è§£æ**:

```python
# è¾“å…¥:
attention_mask = torch.tensor([[1,1,1, 2,2,2,2, 3,3, 0,0]])

# æ­¥éª¤1: get_max_seqlen_in_batch
seqlens_in_batch = get_max_seqlen_in_batch(attention_mask)
# å†…éƒ¨é€»è¾‘:
max_num = 3  # æœ€å¤§åºåˆ—ID
counts = torch.zeros((1, 3))
# ç»Ÿè®¡æ¯ä¸ªIDçš„å‡ºç°æ¬¡æ•°:
# ID=1: 3æ¬¡ â†’ counts[0,0] = 3
# ID=2: 4æ¬¡ â†’ counts[0,1] = 4
# ID=3: 2æ¬¡ â†’ counts[0,2] = 2
# ç»“æœ: [3, 4, 2]

# æ­¥éª¤2: éé›¶ä½ç½®
attention_mask.flatten()  # [1,1,1,2,2,2,2,3,3,0,0]
indices = torch.nonzero(...)  # [[0],[1],[2],[3],[4],[5],[6],[7],[8]]
indices = indices.flatten()   # [0,1,2,3,4,5,6,7,8]

# æ­¥éª¤3: æœ€é•¿åºåˆ—
max_seqlen_in_batch = max([3,4,2]) = 4

# æ­¥éª¤4: ç´¯ç§¯é•¿åº¦
cumsum([3,4,2]) = [3, 7, 9]
pad (1, 0):     = [0, 3, 7, 9]
# å«ä¹‰:
# - åºåˆ—1: tokens 0-2   (cu_seqlens[0]=0 to cu_seqlens[1]=3)
# - åºåˆ—2: tokens 3-6   (cu_seqlens[1]=3 to cu_seqlens[2]=7)
# - åºåˆ—3: tokens 7-8   (cu_seqlens[2]=7 to cu_seqlens[3]=9)
```

### 5.2 get_max_seqlen_in_batch()

```python
@torch.jit.script
def get_max_seqlen_in_batch(attention_mask: torch.Tensor) -> torch.Tensor:
    """
    ä»attention_maskä¸­æå–æ¯ä¸ªåºåˆ—çš„é•¿åº¦

    è¾“å…¥: [batch, total_tokens] åŒ…å«åºåˆ—IDçš„mask
    è¾“å‡º: [num_sequences] æ¯ä¸ªåºåˆ—çš„é•¿åº¦
    """
    # æ‰¾åˆ°æœ€å¤§åºåˆ—ID
    max_num = int(torch.max(attention_mask).item())
    batch_size, _ = attention_mask.shape

    # ä¸ºæ¯ä¸ªIDç»Ÿè®¡å‡ºç°æ¬¡æ•°
    counts = torch.zeros((batch_size, max_num), dtype=torch.int32)

    for i in range(1, max_num + 1):
        # åˆ›å»ºmask: å½“å‰IDçš„ä½ç½®
        mask = (attention_mask == i)
        # ç»Ÿè®¡æ¯è¡Œè¯¥IDå‡ºç°çš„æ¬¡æ•°
        counts[:, i - 1] = torch.sum(mask, dim=-1).to(dtype=torch.int32)

    # å±•å¹³å¹¶å»é™¤0 (å¯èƒ½æœ‰ç©ºåºåˆ—)
    result = counts.flatten()
    nonzero_indices = torch.nonzero(result).squeeze(-1)
    return result[nonzero_indices]
```

**ç¤ºä¾‹**:

```python
# attention_maskåŒ…å«2ä¸ªbins:
attention_mask = torch.tensor([
    [1,1,1, 2,2,2,2, 0,0],     # Bin 1: seq1(3), seq2(4)
    [1,1, 2,2,2, 0,0,0,0],     # Bin 2: seq3(2), seq4(3)
])

max_num = 2  # æ¯ä¸ªbinå†…æœ€å¤š2ä¸ªåºåˆ—

counts = torch.zeros((2, 2), dtype=torch.int32)

# å¤„ç†ID=1:
mask = (attention_mask == 1)
# [[1,1,1, 0,0,0,0, 0,0],
#  [1,1, 0,0,0, 0,0,0,0]]
counts[:, 0] = torch.sum(mask, dim=-1)  # [3, 2]

# å¤„ç†ID=2:
mask = (attention_mask == 2)
# [[0,0,0, 1,1,1,1, 0,0],
#  [0,0, 1,1,1, 0,0,0,0]]
counts[:, 1] = torch.sum(mask, dim=-1)  # [4, 3]

# counts = [[3, 4],
#           [2, 3]]

result = counts.flatten()  # [3, 4, 2, 3]
# å³: bin1çš„seq1=3, seq2=4, bin2çš„seq3=2, seq4=3
```

### 5.3 åœ¨æ¨¡å‹forwardä¸­çš„ä½¿ç”¨

```python
# å…¸å‹çš„Attentionå±‚forwardå‡½æ•°
def forward(
    self,
    hidden_states,
    attention_mask=None,
    position_ids=None,
    ...
):
    bsz, q_len, _ = hidden_states.size()

    # è®¡ç®—QKV
    query_states = self.q_proj(hidden_states)
    key_states = self.k_proj(hidden_states)
    value_states = self.v_proj(hidden_states)

    # Reshape for multi-head
    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)
    key_states = key_states.view(bsz, q_len, self.num_kv_heads, self.head_dim)
    value_states = value_states.view(bsz, q_len, self.num_kv_heads, self.head_dim)

    # â­ Sample Packing: æå–æœ‰æ•ˆtokenså’Œåºåˆ—è¾¹ç•Œ
    if attention_mask is not None and torch.any(attention_mask > 1):
        # æ£€æµ‹åˆ°packed sequences (attention_maskåŒ…å«åºåˆ—ID)
        indices, cu_seqlens, max_seqlen = get_unpad_data(attention_mask)

        # å»é™¤padding
        query_states = query_states.flatten(0, 1)[indices]
        key_states = key_states.flatten(0, 1)[indices]
        value_states = value_states.flatten(0, 1)[indices]

        # Flash Attention with variable-length sequences
        attn_output = flash_attn_varlen_func(
            query_states,
            key_states,
            value_states,
            cu_seqlens_q=cu_seqlens,
            cu_seqlens_k=cu_seqlens,
            max_seqlen_q=max_seqlen,
            max_seqlen_k=max_seqlen,
            dropout_p=self.attention_dropout if self.training else 0.0,
            causal=True,
        )

        # Flash Attentionå†…éƒ¨ä¼šæ ¹æ®cu_seqlens:
        # - åªåœ¨åºåˆ—å†…éƒ¨è®¡ç®—attention
        # - è‡ªåŠ¨å±è”½è·¨åºåˆ—çš„attention
        # - å®Œå…¨è·³è¿‡paddingéƒ¨åˆ†

        # Reshapeå›åŸå§‹shape (åŒ…å«padding)
        attn_output_unpad = attn_output
        attn_output = torch.zeros(
            bsz * q_len, self.num_heads, self.head_dim,
            dtype=attn_output.dtype, device=attn_output.device
        )
        attn_output[indices] = attn_output_unpad

    else:
        # æ ‡å‡†attention (épacked)
        attn_output = self.standard_attention(
            query_states, key_states, value_states, attention_mask
        )

    return attn_output
```

---

## 6. Monkeypatch æœºåˆ¶

### 6.1 patch_for_multipack()

**æ–‡ä»¶**: `src/axolotl/monkeypatch/multipack.py:53-65`

```python
SUPPORTED_MULTIPACK_MODEL_TYPES = [
    "llama", "mistral", "mixtral", "qwen2", "gemma", "phi3",
    "deepseek_v2", "deepseek_v3", ...
]

def patch_for_multipack(model_type, model_name=None, has_remote_code=False):
    """
    ä¸ºæ¨¡å‹æ‰“patchä»¥æ”¯æŒSample Packing

    æ ¸å¿ƒæ€æƒ³:
    - æ›¿æ¢transformersåº“ä¸­çš„_get_unpad_dataå‡½æ•°
    - ä½¿å…¶èƒ½æ­£ç¡®å¤„ç†packed sequences
    """

    if has_remote_code:
        # è¿œç¨‹ä»£ç æ¨¡å‹éœ€è¦ç‰¹æ®Šå¤„ç†
        patch_remote(model_name)

    elif hasattr(transformers, "modeling_flash_attention_utils"):
        # Transformers >= 4.36ç‰ˆæœ¬
        # æ›¿æ¢å…¨å±€çš„_get_unpad_dataå‡½æ•°
        assert hasattr(
            transformers.modeling_flash_attention_utils,
            "_get_unpad_data"
        ), "transformers API changed!"

        # â­ æ ¸å¿ƒ: æ›¿æ¢ä¸ºæˆ‘ä»¬çš„å®ç°
        transformers.modeling_flash_attention_utils._get_unpad_data = get_unpad_data

    # Mixtral + DeepSpeed Zero3éœ€è¦é¢å¤–patch
    if model_type == "mixtral" and is_deepspeed_zero3_enabled():
        patch_mixtral_moe_forward_zero3()
```

**ä¸ºä»€ä¹ˆéœ€è¦monkeypatch?**

```python
# TransformersåŸç”Ÿçš„_get_unpad_data:
def _get_unpad_data(attention_mask):
    """
    åŸç”Ÿå®ç°å‡è®¾attention_maskæ˜¯binary (0/1)
    ä¸æ”¯æŒåºåˆ—ID (1,2,3,...)
    """
    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)
    # å¯¹äº [1,1,1, 2,2,2,2, 0,0]:
    # sum = 1+1+1+2+2+2+2 = 11  â† é”™è¯¯! åº”è¯¥æ˜¯9

    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()
    # è¿™éƒ¨åˆ†æ˜¯å¯¹çš„

    max_seqlen_in_batch = seqlens_in_batch.max().item()
    # é”™è¯¯çš„seqlenså¯¼è‡´é”™è¯¯çš„max_seqlen

    cu_seqlens = F.pad(
        torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)
    )
    # é”™è¯¯çš„seqlenså¯¼è‡´é”™è¯¯çš„cu_seqlens

    return (indices, cu_seqlens, max_seqlen_in_batch)

# Axolotlçš„å®ç°:
# æ­£ç¡®å¤„ç†åºåˆ—IDï¼Œæå–çœŸå®çš„åºåˆ—é•¿åº¦
```

### 6.2 patch_remote()

```python
def patch_remote(model_name):
    """
    ä¸ºremote codeæ¨¡å‹æ‰“patch

    æŒ‘æˆ˜:
    - Remote codeæ¨¡å‹çš„modelingæ–‡ä»¶åœ¨è¿è¡Œæ—¶åŠ¨æ€åŠ è½½
    - ä¸åœ¨transformersåº“ä¸­ï¼Œéœ€è¦æ‰¾åˆ°å®é™…æ¨¡å—
    """
    # 1. åŠ è½½æ¨¡å‹é…ç½®
    model_config = AutoConfig.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    # 2. åŠ è½½æ¨¡å‹ (è§¦å‘remote codeä¸‹è½½)
    with init_empty_weights():
        AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True
        )
    # ç°åœ¨modeling_xxx.pyå·²ç»è¢«import

    # 3. æ‰¾åˆ°modeling module
    parts = model_config.__class__.__module__.split(".")
    # ä¾‹: "transformers_modules.model_name.configuration_xxx"
    parts[-1] = parts[-1].replace("configuration_", "modeling_", 1)
    # â†’ "transformers_modules.model_name.modeling_xxx"

    module_name = ".".join(parts)
    modeling_arch = importlib.import_module(module_name)

    # 4. æ›¿æ¢_get_unpad_data
    if hasattr(modeling_arch, "_get_unpad_data"):
        modeling_arch._get_unpad_data = get_unpad_data
```

---

## 7. ä¸è®­ç»ƒæµç¨‹çš„é›†æˆ

### 7.1 åœ¨HFCausalTrainerBuilderä¸­çš„é…ç½®

**æ–‡ä»¶**: `src/axolotl/core/builders/causal.py:250-284`

```python
class HFCausalTrainerBuilder(TrainerBuilderBase):
    def build(self, total_num_steps):
        # ...

        # â­ Sample Packingé…ç½®ä¼ é€’ç»™TrainingArguments
        training_arguments_kwargs["sample_packing"] = bool(self.cfg.sample_packing)

        # æ˜¯å¦drop attention_mask (Flash Attentionå¯ä»¥drop)
        training_arguments_kwargs["sample_packing_drop_attention_mask"] = bool(
            self.cfg.flash_attention
            or self.cfg.xformers_attention
            or self.cfg.flex_attention
        )

        # æ˜¯å¦ä½¿ç”¨real batches (legacyè®¾ç½®)
        training_arguments_kwargs["multipack_real_batches"] = (
            self.cfg.multipack_real_batches
            if self.cfg.multipack_real_batches is not None
            else not (
                self.cfg.flash_attention
                or self.cfg.flex_attention
                or self.cfg.xformers_attention
            )
        )

        # Evalä¹Ÿå¯ç”¨packing
        training_arguments_kwargs["eval_sample_packing"] = bool(
            self.cfg.eval_sample_packing
        )

        # Packingæ¨¡å¼
        if self.cfg.sample_packing_sequentially is not None:
            training_arguments_kwargs["sample_packing_sequentially"] = (
                self.cfg.sample_packing_sequentially
            )

        # Biné…ç½®
        if self.cfg.sample_packing_bin_size is not None:
            training_arguments_kwargs["sample_packing_bin_size"] = (
                self.cfg.sample_packing_bin_size
            )

        if self.cfg.sample_packing_group_size is not None:
            training_arguments_kwargs["sample_packing_group_size"] = (
                self.cfg.sample_packing_group_size
            )

        # æ•ˆç‡ä¼°è®¡
        if self.cfg.sample_packing_eff_est:
            training_arguments_kwargs["sample_packing_efficiency"] = (
                self.cfg.sample_packing_eff_est
            )

        # ...

        training_args = AxolotlTrainingArguments(**training_arguments_kwargs)

        # ...
        return trainer
```

### 7.2 Collatoré€‰æ‹©é€»è¾‘

```python
def build_collator(self, training_args, is_eval=False, **kwargs):
    """é€‰æ‹©åˆé€‚çš„data collator"""

    # æ£€æŸ¥æ˜¯å¦éœ€è¦packing collator
    use_batch_sampler_collator = False
    if is_eval is False and training_args.sample_packing:
        use_batch_sampler_collator = True
    if is_eval and training_args.eval_sample_packing:
        use_batch_sampler_collator = True

    if use_batch_sampler_collator:
        # é€‰æ‹©V1 vs V2
        if (
            self.cfg.flex_attention
            or self.cfg.model_config_type in SUPPORTED_MULTIPACK_MODEL_TYPES
            or (
                self.cfg.model_config_type in ["llama"]
                and self.cfg.flash_attention is not True
            )
        ):
            # ä½¿ç”¨V2 (æ›´å¹¿æ³›å…¼å®¹)
            collator = V2BatchSamplerDataCollatorForSeq2Seq
        else:
            # ä½¿ç”¨V1 (Flash Attentionä¸“ç”¨)
            collator = BatchSamplerDataCollatorForSeq2Seq
    else:
        # æ ‡å‡†collator
        collator = DataCollatorForSeq2Seq

    return collator(self.tokenizer, **kwargs)
```

### 7.3 DeepSpeedç‰¹æ®Šå¤„ç†

```python
# åœ¨traineråˆ›å»ºå
if self.cfg.deepspeed and self.cfg.sample_packing:
    # DeepSpeedéœ€è¦çŸ¥é“çœŸå®çš„micro_batch_size
    # (å› ä¸ºSample Packingæ”¹å˜äº†batchç»“æ„)
    trainer.accelerator.state.deepspeed_plugin.deepspeed_config[
        "train_micro_batch_size_per_gpu"
    ] = self.cfg.micro_batch_size
```

---

## 8. åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ

### 8.1 DDPé›†æˆ

```python
class MultipackBatchSampler:
    def __init__(self, ...):
        # è‡ªåŠ¨æ£€æµ‹åˆ†å¸ƒå¼ç¯å¢ƒ
        if dist.is_available() and dist.is_initialized():
            self.rank = dist.get_rank()
            self.num_replicas = dist.get_world_size()
        else:
            self.rank = 0
            self.num_replicas = 1

    def generate_batches(self):
        # æ‰€æœ‰rankç”Ÿæˆç›¸åŒçš„batches (ä½¿ç”¨ç›¸åŒçš„seed)
        batches = pack_parallel(...)

        # ä½†æ¯ä¸ªrankåªå¤„ç†è‡ªå·±çš„åˆ†ç‰‡
        # é€šè¿‡samplerè‡ªåŠ¨å¤„ç† (DistributedSampler)
```

**DistributedSampler + MultipackBatchSampler**:

```python
# è®­ç»ƒä»£ç 
from torch.utils.data import DistributedSampler

# åˆ›å»ºsampler
base_sampler = DistributedSampler(
    dataset,
    num_replicas=world_size,
    rank=rank,
    shuffle=True,
    seed=42,
)

# åŒ…è£…ä¸ºMultipackBatchSampler
batch_sampler = MultipackBatchSampler(
    sampler=base_sampler,
    batch_size=4,
    batch_max_len=2048,
    lengths=precomputed_lengths,
    ...
)

# åˆ›å»ºDataLoader
train_loader = DataLoader(
    dataset,
    batch_sampler=batch_sampler,  # â† ä½¿ç”¨batch_sampler
    collate_fn=collator,
)

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    base_sampler.set_epoch(epoch)  # é‡è¦! è®¾ç½®epoch
    batch_sampler.set_epoch(epoch)  # ä¹Ÿè¦è®¾ç½®

    for batch in train_loader:
        # æ¯ä¸ªrankå¤„ç†ä¸åŒçš„bins
        ...
```

### 8.2 FSDP/TPé›†æˆ

```python
# FSDPå’ŒTPé€šè¿‡DeviceMeshåè°ƒ

# ä¾‹: 8 GPUs, TP=2, DP=4
from torch.distributed.device_mesh import init_device_mesh

device_mesh = init_device_mesh(
    "cuda",
    (4, 2),  # (DP, TP)
    mesh_dim_names=("dp", "tp"),
)

# MultipackBatchSamplerè‡ªåŠ¨å¤„ç†:
# - æ£€æµ‹å½“å‰rankåœ¨DPç»´åº¦çš„ä½ç½®
# - åªè¿”å›å±äºè¯¥DP rankçš„æ•°æ®

# ä¼ªä»£ç :
dp_rank = device_mesh.get_local_rank("dp")
dp_world_size = device_mesh.size("dp")

# åœ¨generate_batchesä¸­:
my_bins = all_bins[dp_rank::dp_world_size]
# DP rank 0: bins [0, 4, 8, 12, ...]
# DP rank 1: bins [1, 5, 9, 13, ...]
# DP rank 2: bins [2, 6, 10, 14, ...]
# DP rank 3: bins [3, 7, 11, 15, ...]
```

### 8.3 æ•ˆç‡åŒæ­¥

```python
class MultipackBatchSampler:
    def gather_efficiency(self) -> float:
        """æ”¶é›†æ‰€æœ‰ranksçš„æ•ˆç‡ç»Ÿè®¡"""
        if not dist.is_available() or not dist.is_initialized():
            return self._efficiency

        # åˆ›å»ºtensor
        efficiency_tensor = torch.tensor(
            [self._efficiency],
            dtype=torch.float32,
            device="cuda"
        )

        # AllGather: æ”¶é›†æ‰€æœ‰ranksçš„æ•ˆç‡
        gathered = [torch.zeros_like(efficiency_tensor) for _ in range(self.num_replicas)]
        dist.all_gather(gathered, efficiency_tensor)

        # è®¡ç®—å¹³å‡æ•ˆç‡
        efficiencies = [t.item() for t in gathered]
        avg_efficiency = sum(efficiencies) / len(efficiencies)

        return avg_efficiency

    def gather_len_batches(self) -> int:
        """æ”¶é›†æ‰€æœ‰ranksçš„batchæ•°é‡"""
        if not dist.is_available() or not dist.is_initialized():
            return len(self._batches) if self._batches else 0

        len_tensor = torch.tensor(
            [len(self._batches) if self._batches else 0],
            dtype=torch.int64,
            device="cuda"
        )

        # AllGather
        gathered = [torch.zeros_like(len_tensor) for _ in range(self.num_replicas)]
        dist.all_gather(gathered, len_tensor)

        # è¿”å›æœ€å°å€¼ (ç¡®ä¿æ‰€æœ‰ranksåŒæ­¥)
        lengths = [t.item() for t in gathered]
        return min(lengths)
```

**ä¸ºä»€ä¹ˆéœ€è¦gather_len_batches?**

```python
# é—®é¢˜: ä¸åŒrankså¯èƒ½ç”Ÿæˆä¸åŒæ•°é‡çš„batches
# Rank 0: 1000 batches
# Rank 1: 999 batches  â† æ•°æ®åˆ†ç‰‡å¯èƒ½ä¸å‡

# å¦‚æœä¸åŒæ­¥:
# - Rank 0ä¼šç­‰å¾…batch 1000
# - Rank 1å·²ç»ç»“æŸ
# - è®­ç»ƒhang!

# è§£å†³: å–æœ€å°å€¼ï¼Œç¡®ä¿æ‰€æœ‰ranksåœ¨ç›¸åŒæ­¥æ•°ç»“æŸ
min_batches = batch_sampler.gather_len_batches()
# æ‰€æœ‰rankséƒ½åªè¿è¡Œ999ä¸ªbatches
```

---

## æ€»ç»“

### æ ¸å¿ƒæºç ç»„ä»¶

1. **MultipackBatchSampler** (`multipack.py:244-474`)
   - è´Ÿè´£ç”Ÿæˆæ‰“åŒ…åçš„batchç´¢å¼•
   - æ”¯æŒFFDå¹¶è¡Œå’ŒSequentialä¸¤ç§æ¨¡å¼
   - è‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒ

2. **FFDç®—æ³•** (`multipack.py:61-190`)
   - `pack_parallel()`: å¹¶è¡ŒFFDï¼Œæœ€é«˜æ•ˆ
   - `pack_group()`: å•ç»„FFDï¼ŒNumbaåŠ é€Ÿ
   - `allocate_sequentially()`: é¡ºåºæ‰“åŒ…ï¼Œä¿æŒé¡ºåº

3. **Data Collator** (`batching.py:159-196`)
   - `V2BatchSamplerDataCollatorForSeq2Seq`: æ¨èï¼Œå¹¿æ³›å…¼å®¹
   - å…³é”®: attention_maskä½¿ç”¨åºåˆ—IDåŒºåˆ†åºåˆ—

4. **Attentionå¤„ç†** (`monkeypatch/utils.py:31-96`)
   - `get_unpad_data()`: æå–æœ‰æ•ˆtokenså’Œåºåˆ—è¾¹ç•Œ
   - `get_max_seqlen_in_batch()`: ä»maskæå–åºåˆ—é•¿åº¦
   - `get_cu_seqlens()`: è®¡ç®—ç´¯ç§¯åºåˆ—é•¿åº¦

5. **Monkeypatch** (`monkeypatch/multipack.py`)
   - æ›¿æ¢transformersçš„`_get_unpad_data`
   - ç¡®ä¿æ¨¡å‹æ­£ç¡®å¤„ç†packed sequences

### å…³é”®æŠ€æœ¯ç‚¹

- **Numba JIT**: FFDç®—æ³•åŠ é€Ÿ30-50å€
- **åºåˆ—IDæ ‡è®°**: attention_mask=[1,2,3,...] åŒºåˆ†ä¸åŒåºåˆ—
- **cu_seqlens**: Flash Attentionçš„åºåˆ—è¾¹ç•Œå‚æ•°
- **åˆ†å¸ƒå¼å…¼å®¹**: è‡ªåŠ¨æ£€æµ‹DDP/FSDP/TPç¯å¢ƒ

### æ•°æ®æµæ€»ç»“

```
Dataset
  â†“ __getitem__
{input_ids, attention_mask, labels}
  â†“ MultipackBatchSampler
[[idx1,idx2], [idx3,idx4,idx5], ...]  # bins
  â†“ V2Collator
{input_ids: [packed], attention_mask: [1,1,2,2,2,3,3], ...}
  â†“ get_unpad_data
(indices, cu_seqlens=[0,2,5,7], max_seqlen=3)
  â†“ Flash Attention
æ­£ç¡®éš”ç¦»åºåˆ—ï¼Œé«˜æ•ˆè®¡ç®—
  â†“ Loss
æ ‡å‡†äº¤å‰ç†µï¼Œä¸å—packingå½±å“
```

---

## ç›¸å…³æ–‡æ¡£

- [Sample Packing æ·±åº¦è§£æ](./sample_packing_deep_dive.md)
- [Sample Packing å¿«é€Ÿå‚è€ƒ](./sample_packing_quick_reference.md)
- [ä¸»ç´¢å¼•](./README.md)

---

*æ–‡æ¡£ç‰ˆæœ¬: v1.0 | æœ€åæ›´æ–°: 2025-11*
